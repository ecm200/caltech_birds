{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caltech UCSD Birds 200 2011 (CUB-200-2011)\n",
    "\n",
    "## Part 3 - Training a PyTorch ResNet152 convolutional neural network (CNN) using Torchvision pre-trained network on ImageNet database.\n",
    "\n",
    "This notebook shows how to prepare datasets for training a Convolutional Neural Network using the CUB-200-2011 database of birds.\n",
    "\n",
    "The previous notebooks sorted the data into an organised filing system for use with Torchvision data loaders and investigated the type of data and class sampling densities of the training and test sets.\n",
    "\n",
    "This is the third stage of our roadmap for building and understanding a birds image classifier:\n",
    "\n",
    "![RoadMapImage](../docs/birds_roadmap.png)\n",
    "\n",
    "This notebook is now going to show: \n",
    "\n",
    "    1. How to set up a dataloader using an augmentation workflow (as in Part 1) for use in providing images to our network for training.\n",
    "    \n",
    "    2. How to load a common network architecture, such as ResNet152, and load pretrained weights into the netowrk, using the models supplied in the PyTorch Torchvision module.\n",
    "    \n",
    "    3. How to manipulate the network structure to re-engineer the ouput classifier layer so that it suits the purposes of our problem.\n",
    "    \n",
    "    4. How to set up the required objects that are needed for training a neural network, in addition to the neural network model object itself. This includes:\n",
    "        a. Loss function.\n",
    "        b. Optimizer.\n",
    "        c. Learning rate scheduler.\n",
    "        d. training function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook setup\n",
    "\n",
    "### Modules and externals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets, models\n",
    "\n",
    "from imutils import paths\n",
    "from pathlib import Path\n",
    "import os\n",
    "import time\n",
    "import copy\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "\n",
    "# Local modules\n",
    "from cub_tools.train import train_model\n",
    "from cub_tools.visualize import imshow, visualize_model\n",
    "from cub_tools.utils import save_model_dict, save_model_full, unpickle\n",
    "from cub_tools.transforms import makeDefaultTransforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Runtime setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Script runtime options\n",
    "model_name = 'resnet152'\n",
    "model_func = models.resnet152\n",
    "root_dir = '../data'\n",
    "data_dir = os.path.join(root_dir,'images')\n",
    "working_dir = os.path.join('../models/classification', model_name)\n",
    "batch_size = 16\n",
    "num_workers = 4\n",
    "num_epochs = 40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataset loaders\n",
    "\n",
    "As with the visualisation section (001 Notebook), we need to create a loader for importing the images sets for both training and testing/validation purposes. As with all algorithm development and training, we must be careful to separate our data into separate training and test sets, such that we can gain a (hopefully) unbiased assessment of the model performance by testing the predictive power of the model with known images of birds. These images must not have been using the training process, as any images that will be presented to network during deployment will not have been seen by the network before. We want to assess what the performance of the network will be when it is presented with images of the same class, but not the same exact image, that it has been trained on. Only then can we get a fair assessment of the likely performance of the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(working_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the augmention transforms\n",
    "\n",
    "Data augmentation, a process we looked at in the previous notebook, is a process which applies random translations to the input images during each Epoch of training, such that every time the network sees each image, it is not the exact same representation of that image. These transforms are loaded from a preset function, which determines both the train and validation transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data transforms\n",
    "data_transforms = makeDefaultTransforms()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we create the dataset and dataloader objects by using the Torchvision dataset tools.\n",
    "\n",
    "As we have organised our image files into train and test directories already, we simply give the path to the dataset object function datasets.ImageFolder of Torchvision and it creates a Torch dataset object, with all the images. The images have also been arranged into sub-directories of image class, the function attributes the class label of the image from the directory name. The image dataset object also carries with the loading process and any augmentation transforms that are to be applied to the training and test data.\n",
    "\n",
    "The dataset objects can be used to be turned into batching image loaders, which can be used as the process which controls the inputs that are loaded into the neural network during training and testing. The **batch size** determines the number of images that are served up to the network in a **mini batch**, an iteration. The number of mini batches is determined by the number of images, and is divided by the batch size. A epoch, a complete iteraton, pertains to one complete training cycle over all images in the dataset. A network may take 50 to 100 epochs to successfully train to adequate enough accuracies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup data loaders with augmentation transforms\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x])\n",
    "                  for x in ['train', 'test']}\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size,\n",
    "                                             shuffle=True, num_workers=num_workers)\n",
    "              for x in ['train', 'test']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section reports the dataset sizes to terminal for user information, as well as showing the class labels (200) for this bird classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data\n",
      "========================================\n",
      "train  size::  5994  images\n",
      "test  size::  5794  images\n",
      "\n",
      "Number of classes::  200\n",
      "========================================\n",
      "0 ::  001.Black_footed_Albatross\n",
      "1 ::  002.Laysan_Albatross\n",
      "2 ::  003.Sooty_Albatross\n",
      "3 ::  004.Groove_billed_Ani\n",
      "4 ::  005.Crested_Auklet\n",
      "5 ::  006.Least_Auklet\n",
      "6 ::  007.Parakeet_Auklet\n",
      "7 ::  008.Rhinoceros_Auklet\n",
      "8 ::  009.Brewer_Blackbird\n",
      "9 ::  010.Red_winged_Blackbird\n",
      "10 ::  011.Rusty_Blackbird\n",
      "11 ::  012.Yellow_headed_Blackbird\n",
      "12 ::  013.Bobolink\n",
      "13 ::  014.Indigo_Bunting\n",
      "14 ::  015.Lazuli_Bunting\n",
      "15 ::  016.Painted_Bunting\n",
      "16 ::  017.Cardinal\n",
      "17 ::  018.Spotted_Catbird\n",
      "18 ::  019.Gray_Catbird\n",
      "19 ::  020.Yellow_breasted_Chat\n",
      "20 ::  021.Eastern_Towhee\n",
      "21 ::  022.Chuck_will_Widow\n",
      "22 ::  023.Brandt_Cormorant\n",
      "23 ::  024.Red_faced_Cormorant\n",
      "24 ::  025.Pelagic_Cormorant\n",
      "25 ::  026.Bronzed_Cowbird\n",
      "26 ::  027.Shiny_Cowbird\n",
      "27 ::  028.Brown_Creeper\n",
      "28 ::  029.American_Crow\n",
      "29 ::  030.Fish_Crow\n",
      "30 ::  031.Black_billed_Cuckoo\n",
      "31 ::  032.Mangrove_Cuckoo\n",
      "32 ::  033.Yellow_billed_Cuckoo\n",
      "33 ::  034.Gray_crowned_Rosy_Finch\n",
      "34 ::  035.Purple_Finch\n",
      "35 ::  036.Northern_Flicker\n",
      "36 ::  037.Acadian_Flycatcher\n",
      "37 ::  038.Great_Crested_Flycatcher\n",
      "38 ::  039.Least_Flycatcher\n",
      "39 ::  040.Olive_sided_Flycatcher\n",
      "40 ::  041.Scissor_tailed_Flycatcher\n",
      "41 ::  042.Vermilion_Flycatcher\n",
      "42 ::  043.Yellow_bellied_Flycatcher\n",
      "43 ::  044.Frigatebird\n",
      "44 ::  045.Northern_Fulmar\n",
      "45 ::  046.Gadwall\n",
      "46 ::  047.American_Goldfinch\n",
      "47 ::  048.European_Goldfinch\n",
      "48 ::  049.Boat_tailed_Grackle\n",
      "49 ::  050.Eared_Grebe\n",
      "50 ::  051.Horned_Grebe\n",
      "51 ::  052.Pied_billed_Grebe\n",
      "52 ::  053.Western_Grebe\n",
      "53 ::  054.Blue_Grosbeak\n",
      "54 ::  055.Evening_Grosbeak\n",
      "55 ::  056.Pine_Grosbeak\n",
      "56 ::  057.Rose_breasted_Grosbeak\n",
      "57 ::  058.Pigeon_Guillemot\n",
      "58 ::  059.California_Gull\n",
      "59 ::  060.Glaucous_winged_Gull\n",
      "60 ::  061.Heermann_Gull\n",
      "61 ::  062.Herring_Gull\n",
      "62 ::  063.Ivory_Gull\n",
      "63 ::  064.Ring_billed_Gull\n",
      "64 ::  065.Slaty_backed_Gull\n",
      "65 ::  066.Western_Gull\n",
      "66 ::  067.Anna_Hummingbird\n",
      "67 ::  068.Ruby_throated_Hummingbird\n",
      "68 ::  069.Rufous_Hummingbird\n",
      "69 ::  070.Green_Violetear\n",
      "70 ::  071.Long_tailed_Jaeger\n",
      "71 ::  072.Pomarine_Jaeger\n",
      "72 ::  073.Blue_Jay\n",
      "73 ::  074.Florida_Jay\n",
      "74 ::  075.Green_Jay\n",
      "75 ::  076.Dark_eyed_Junco\n",
      "76 ::  077.Tropical_Kingbird\n",
      "77 ::  078.Gray_Kingbird\n",
      "78 ::  079.Belted_Kingfisher\n",
      "79 ::  080.Green_Kingfisher\n",
      "80 ::  081.Pied_Kingfisher\n",
      "81 ::  082.Ringed_Kingfisher\n",
      "82 ::  083.White_breasted_Kingfisher\n",
      "83 ::  084.Red_legged_Kittiwake\n",
      "84 ::  085.Horned_Lark\n",
      "85 ::  086.Pacific_Loon\n",
      "86 ::  087.Mallard\n",
      "87 ::  088.Western_Meadowlark\n",
      "88 ::  089.Hooded_Merganser\n",
      "89 ::  090.Red_breasted_Merganser\n",
      "90 ::  091.Mockingbird\n",
      "91 ::  092.Nighthawk\n",
      "92 ::  093.Clark_Nutcracker\n",
      "93 ::  094.White_breasted_Nuthatch\n",
      "94 ::  095.Baltimore_Oriole\n",
      "95 ::  096.Hooded_Oriole\n",
      "96 ::  097.Orchard_Oriole\n",
      "97 ::  098.Scott_Oriole\n",
      "98 ::  099.Ovenbird\n",
      "99 ::  100.Brown_Pelican\n",
      "100 ::  101.White_Pelican\n",
      "101 ::  102.Western_Wood_Pewee\n",
      "102 ::  103.Sayornis\n",
      "103 ::  104.American_Pipit\n",
      "104 ::  105.Whip_poor_Will\n",
      "105 ::  106.Horned_Puffin\n",
      "106 ::  107.Common_Raven\n",
      "107 ::  108.White_necked_Raven\n",
      "108 ::  109.American_Redstart\n",
      "109 ::  110.Geococcyx\n",
      "110 ::  111.Loggerhead_Shrike\n",
      "111 ::  112.Great_Grey_Shrike\n",
      "112 ::  113.Baird_Sparrow\n",
      "113 ::  114.Black_throated_Sparrow\n",
      "114 ::  115.Brewer_Sparrow\n",
      "115 ::  116.Chipping_Sparrow\n",
      "116 ::  117.Clay_colored_Sparrow\n",
      "117 ::  118.House_Sparrow\n",
      "118 ::  119.Field_Sparrow\n",
      "119 ::  120.Fox_Sparrow\n",
      "120 ::  121.Grasshopper_Sparrow\n",
      "121 ::  122.Harris_Sparrow\n",
      "122 ::  123.Henslow_Sparrow\n",
      "123 ::  124.Le_Conte_Sparrow\n",
      "124 ::  125.Lincoln_Sparrow\n",
      "125 ::  126.Nelson_Sharp_tailed_Sparrow\n",
      "126 ::  127.Savannah_Sparrow\n",
      "127 ::  128.Seaside_Sparrow\n",
      "128 ::  129.Song_Sparrow\n",
      "129 ::  130.Tree_Sparrow\n",
      "130 ::  131.Vesper_Sparrow\n",
      "131 ::  132.White_crowned_Sparrow\n",
      "132 ::  133.White_throated_Sparrow\n",
      "133 ::  134.Cape_Glossy_Starling\n",
      "134 ::  135.Bank_Swallow\n",
      "135 ::  136.Barn_Swallow\n",
      "136 ::  137.Cliff_Swallow\n",
      "137 ::  138.Tree_Swallow\n",
      "138 ::  139.Scarlet_Tanager\n",
      "139 ::  140.Summer_Tanager\n",
      "140 ::  141.Artic_Tern\n",
      "141 ::  142.Black_Tern\n",
      "142 ::  143.Caspian_Tern\n",
      "143 ::  144.Common_Tern\n",
      "144 ::  145.Elegant_Tern\n",
      "145 ::  146.Forsters_Tern\n",
      "146 ::  147.Least_Tern\n",
      "147 ::  148.Green_tailed_Towhee\n",
      "148 ::  149.Brown_Thrasher\n",
      "149 ::  150.Sage_Thrasher\n",
      "150 ::  151.Black_capped_Vireo\n",
      "151 ::  152.Blue_headed_Vireo\n",
      "152 ::  153.Philadelphia_Vireo\n",
      "153 ::  154.Red_eyed_Vireo\n",
      "154 ::  155.Warbling_Vireo\n",
      "155 ::  156.White_eyed_Vireo\n",
      "156 ::  157.Yellow_throated_Vireo\n",
      "157 ::  158.Bay_breasted_Warbler\n",
      "158 ::  159.Black_and_white_Warbler\n",
      "159 ::  160.Black_throated_Blue_Warbler\n",
      "160 ::  161.Blue_winged_Warbler\n",
      "161 ::  162.Canada_Warbler\n",
      "162 ::  163.Cape_May_Warbler\n",
      "163 ::  164.Cerulean_Warbler\n",
      "164 ::  165.Chestnut_sided_Warbler\n",
      "165 ::  166.Golden_winged_Warbler\n",
      "166 ::  167.Hooded_Warbler\n",
      "167 ::  168.Kentucky_Warbler\n",
      "168 ::  169.Magnolia_Warbler\n",
      "169 ::  170.Mourning_Warbler\n",
      "170 ::  171.Myrtle_Warbler\n",
      "171 ::  172.Nashville_Warbler\n",
      "172 ::  173.Orange_crowned_Warbler\n",
      "173 ::  174.Palm_Warbler\n",
      "174 ::  175.Pine_Warbler\n",
      "175 ::  176.Prairie_Warbler\n",
      "176 ::  177.Prothonotary_Warbler\n",
      "177 ::  178.Swainson_Warbler\n",
      "178 ::  179.Tennessee_Warbler\n",
      "179 ::  180.Wilson_Warbler\n",
      "180 ::  181.Worm_eating_Warbler\n",
      "181 ::  182.Yellow_Warbler\n",
      "182 ::  183.Northern_Waterthrush\n",
      "183 ::  184.Louisiana_Waterthrush\n",
      "184 ::  185.Bohemian_Waxwing\n",
      "185 ::  186.Cedar_Waxwing\n",
      "186 ::  187.American_Three_toed_Woodpecker\n",
      "187 ::  188.Pileated_Woodpecker\n",
      "188 ::  189.Red_bellied_Woodpecker\n",
      "189 ::  190.Red_cockaded_Woodpecker\n",
      "190 ::  191.Red_headed_Woodpecker\n",
      "191 ::  192.Downy_Woodpecker\n",
      "192 ::  193.Bewick_Wren\n",
      "193 ::  194.Cactus_Wren\n",
      "194 ::  195.Carolina_Wren\n",
      "195 ::  196.House_Wren\n",
      "196 ::  197.Marsh_Wren\n",
      "197 ::  198.Rock_Wren\n",
      "198 ::  199.Winter_Wren\n",
      "199 ::  200.Common_Yellowthroat\n"
     ]
    }
   ],
   "source": [
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'test']}\n",
    "class_names = image_datasets['train'].classes\n",
    "\n",
    "print('Number of data')\n",
    "print('========================================')\n",
    "for dataset in dataset_sizes.keys():\n",
    "    print(dataset,' size:: ', dataset_sizes[dataset],' images')\n",
    "\n",
    "print('')\n",
    "print('Number of classes:: ', len(class_names))\n",
    "print('========================================')\n",
    "for i_class, class_name in enumerate(class_names):\n",
    "    print(i_class,':: ',class_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Device Setup\n",
    "\n",
    "PyTorch can be run a number of computational devices, including CPU, GPU, or even TPU if available.\n",
    "\n",
    "Generally speaking, most modern CNN require a considerable amout of TFLOPS of processing power to complete adequate training, and thus is generally not suitable for training on CPUs alone. GPU, with their ability to perform highly parallelized matrix operations due to their architecture of many cores are particularly well suited to this type of problem. A majority of the models in these examples require GPUs to perform training in a reasonable amount of time (30 minutes to 24 hours, depending on network size).\n",
    "\n",
    "Here we check for the availibility of GPU on the system and use that if available. If not, we set the device to CPU (but this is really not advisable for training purposes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Setup the device to run the computations\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Device::', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load a pre-trained model on ImageNet and train\n",
    "\n",
    "In this section we arrive at the point where we choose the model architecture and load pre-trained weights.\n",
    "\n",
    "What do we mean by pre-trained weights?\n",
    "\n",
    "![Image](https://miro.medium.com/max/764/1*BpnKbH4KiluEeC6PBSBb1w.png)\n",
    "\n",
    "When we build a neural network, we employ a series of chained functions, each that have their set of coefficients which need to trained optimally to produce the best accuracy in predicting the desired outcome. If we compare this to the more simple notion of trend line fitting using a straight line, the function y = ax + b dictates how we model a straight line. In this formulation, we have coefficients a and b, which are the gradient and the y-intercept value, which determines the behaviour of straight line function. During the process of fitting linear trend lines, we aim to train the coefficients, or weights, such that the straight optimally fits our observed data. However, a the beginning of the process we need to select values for those weights for the initial guess. In this simple example it could be simply setting the coefficients equal to 0, (a,b = 0), and then iterating to a solution from this point. In general, this topic of investigation is called \"initializing a neural network\" and simply relates the to values that the sometimes millions of coefficients (or weights) in a network take at the beginning of network training.\n",
    "\n",
    "If we wanted to start the model training from scratch then we might want to initialise the weights using random numbers from a certain distribution. This would be parting little or no prior knowledge of the expected features that our network might learn over the training process, giving it free reign to learn what it thinks is the most sensitive set of feature maps for the given problem.\n",
    "\n",
    "Another way of initialising a neural network is by way of using pre-trained weights. This is where the network in question has already been trained on a set of images (that are different to the problem at hand), and has produced a set of feature maps which perform adequately on that dataset. This type of approach falls under another subject heading called \"transfer learning\". This type of approach has a reasonable precident, given that the ultimate goal of training neural networks is produce feature maps that are abstract enough that they perform well on image classification problems with images that have not been seen before by the network. The more abstract these representations become, generally the better the network performs. Therefore, it follows that if we use a network that has been trained to a good performance on a general image set, then it will likely train better and perform more accurately than a network trained from random weights. It has been shown in the literature that pre-training even with highly general image sets such as ImageNet, results in better performing models, even for fine-grained classication problems like this one where there is minimal cross-over in class content. For example, the ImageNet may have a single category for bird, whereas the objective of this classification problem is classify 200 different types of bird species.\n",
    "\n",
    "More recent publications in the literature have shown that even further gains can be made by pre-training on generalized datasets that have more similarity to the problem at hand. For example, in the CUB-200-2011 dataset, improvements in classification performance have been found using domain specific transfer learning. For example [Cui et al 2018](https://arxiv.org/abs/1806.06193), found that using the [iNaturalist dataset](https://www.kaggle.com/c/inaturalist-challenge-at-fgvc-2017) [[iNaturalist ref]](https://arxiv.org/abs/1707.06642), a diverse naturist database of images of flauna and fora, including birds, resulted in improved classification performance when compared to networks using ImageNet or trained from scratch. This is all because the networks have been trained successfully enough to produce feature maps that generalize well to other, more fine grained problems like this. Starting training from these types of models allows the network to spend more time learning the fine-grained details between bird species.\n",
    "\n",
    "In this particular example, we will use the Torchvision ResNeXt 152 architecture, which has been pre-trained on the [ImageNet database](http://www.image-net.org/).\n",
    "\n",
    "The approach can be characterised in the following flow diagram:\n",
    "\n",
    "![Image](https://www.mathworks.com/help/examples/nnet/win64/TransferLearningUsingAlexNetExample_01.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly we load the most architecure from the Torchvision.models module, specifying the **pretrained=True** for downloading and populating the ImageNet trained weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the model and optimiser\n",
    "model_ft = models.resnet152(pretrained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Secondly, we have to modify the output layer of the network, as this has the wrong number of classes for our problem. Currently, it is designed for ImageNet classication which has 1000 classes in it's dataset, and thus the model produces predictions of each image for every of the 1000 classes. Our problem has only 200 classes, so we need to remove the final Linear Classifier layer, and replace it with the same Linear Classifier layer, but with an output of size of 200, not 1000. The network feature maps in the hidden layers are still initialized with the ImageNet based features, but we now have a classifier that is suitable for out purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_ftrs = model_ft.fc.in_features\n",
    "# Here the size of each output sample is set to 2.\n",
    "# Alternatively, it can be generalized to nn.Linear(num_ftrs, len(class_names)).\n",
    "model_ft.fc = nn.Linear(num_ftrs, len(class_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final step of preparing the model is to push the model to the device on which it will computed on, in this case the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft = model_ft.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function criterion, optimizer and learning rate scheduler\n",
    "\n",
    "Here we specify three important objects that relate to training our model.\n",
    "\n",
    "### Object Function\n",
    "\n",
    "The objective function is the function which measures the performance of a model which outputs a probability for each class as a prediction. In this particular problem, we have chosen to use the cross-entropy loss function, where the cross-entropy value increases as the predicted probability diverges from the true label. The loss (or objective function) helps us determine if our model is updating and iterating towards a solution, that is, it is getting better at predicting the true labels.\n",
    "\n",
    "Cross Entropy = −(ylog(p)+(1−y)log(1−p))\n",
    "\n",
    "![Image](https://ml-cheatsheet.readthedocs.io/en/latest/_images/cross_entropy.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Train the model\n",
    "\n",
    "Finally, we have arrived at the point where we train the model.\n",
    "\n",
    "The key parts of the model that we have built are:\n",
    "\n",
    "    1. The model itself - a ResNeXt 152 model with pre-trained weights from ImageNet training and a output layer of 200 classes.\n",
    "    \n",
    "    2. criterion - the objective function we use to help measure how well our model is performing.\n",
    "    \n",
    "    3. optimiser - the algorithm we use optimise the model coefficients by iterively exploring solution space. In this case, we use stochastic gradient descent.\n",
    "    \n",
    "![Image](https://miro.medium.com/max/699/1*mElyetzsTIJrNnKI8kTkCw.jpeg)\n",
    "\n",
    "![Image](https://ml-cheatsheet.readthedocs.io/en/latest/_images/gradient_descent_demystified.png)\n",
    "    \n",
    "    4. learning rate scheduler - the distance we step through model space at each iteration is important as controls how quickly we find a solution. This function controls how the distance we step after iteration varies as we progress through model training.\n",
    "    \n",
    "    5. device - the target device the training will be performed on e.g. GPU\n",
    "    \n",
    "    6. dataloaders - the iterative functions which control the loading and transforming of the input images of each mini-batch into the network during training  and validation steps.\n",
    "\n",
    "We pass the key parts of the model training process to the train_model function, which controls the epoch (iterations) of the model training and the batching of images into the model. It also instantuates the back-propagation of the gradient error through the model at the end of each epoch, and updates the model coefficients.\n",
    "Lastly, it provides an assessment of the performance of the model at the end of each epoch, using the held out validation image set, which has not been used to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/39\n",
      "----------\n",
      "train Loss: 3.2277 Acc: 0.3130\n",
      "test Loss: 1.9949 Acc: 0.5169\n",
      "\n",
      "Epoch 1/39\n",
      "----------\n",
      "train Loss: 2.3883 Acc: 0.4600\n",
      "test Loss: 1.4148 Acc: 0.6505\n",
      "\n",
      "Epoch 2/39\n",
      "----------\n",
      "train Loss: 1.8827 Acc: 0.5829\n",
      "test Loss: 1.1632 Acc: 0.6823\n",
      "\n",
      "Epoch 3/39\n",
      "----------\n",
      "train Loss: 1.6019 Acc: 0.6333\n",
      "test Loss: 1.0033 Acc: 0.7209\n",
      "\n",
      "Epoch 4/39\n",
      "----------\n",
      "train Loss: 1.3933 Acc: 0.6757\n",
      "test Loss: 0.8907 Acc: 0.7584\n",
      "\n",
      "Epoch 5/39\n",
      "----------\n",
      "train Loss: 1.2270 Acc: 0.7160\n",
      "test Loss: 0.8294 Acc: 0.7604\n",
      "\n",
      "Epoch 6/39\n",
      "----------\n",
      "train Loss: 1.0981 Acc: 0.7548\n",
      "test Loss: 0.7393 Acc: 0.7994\n",
      "\n",
      "Epoch 7/39\n",
      "----------\n",
      "train Loss: 1.0048 Acc: 0.7821\n",
      "test Loss: 0.7194 Acc: 0.8084\n",
      "\n",
      "Epoch 8/39\n",
      "----------\n",
      "train Loss: 0.9910 Acc: 0.7853\n",
      "test Loss: 0.7153 Acc: 0.8117\n",
      "\n",
      "Epoch 9/39\n",
      "----------\n",
      "train Loss: 0.9572 Acc: 0.7973\n",
      "test Loss: 0.7018 Acc: 0.8105\n",
      "\n",
      "Epoch 10/39\n",
      "----------\n",
      "train Loss: 0.9496 Acc: 0.7975\n",
      "test Loss: 0.7003 Acc: 0.8112\n",
      "\n",
      "Epoch 11/39\n",
      "----------\n",
      "train Loss: 0.9537 Acc: 0.7958\n",
      "test Loss: 0.6934 Acc: 0.8155\n",
      "\n",
      "Epoch 12/39\n",
      "----------\n",
      "train Loss: 0.9502 Acc: 0.8001\n",
      "test Loss: 0.6870 Acc: 0.8145\n",
      "\n",
      "Epoch 13/39\n",
      "----------\n",
      "train Loss: 0.9156 Acc: 0.8063\n",
      "test Loss: 0.7000 Acc: 0.8115\n",
      "\n",
      "Epoch 14/39\n",
      "----------\n",
      "train Loss: 0.9157 Acc: 0.8053\n",
      "test Loss: 0.6881 Acc: 0.8139\n",
      "\n",
      "Epoch 15/39\n",
      "----------\n",
      "train Loss: 0.9147 Acc: 0.8088\n",
      "test Loss: 0.6822 Acc: 0.8160\n",
      "\n",
      "Epoch 16/39\n",
      "----------\n",
      "train Loss: 0.9110 Acc: 0.8088\n",
      "test Loss: 0.6837 Acc: 0.8145\n",
      "\n",
      "Epoch 17/39\n",
      "----------\n",
      "train Loss: 0.8794 Acc: 0.8161\n",
      "test Loss: 0.6898 Acc: 0.8145\n",
      "\n",
      "Epoch 18/39\n",
      "----------\n",
      "train Loss: 0.9161 Acc: 0.8026\n",
      "test Loss: 0.6905 Acc: 0.8141\n",
      "\n",
      "Epoch 19/39\n",
      "----------\n",
      "train Loss: 0.8981 Acc: 0.8126\n",
      "test Loss: 0.6808 Acc: 0.8193\n",
      "\n",
      "Epoch 20/39\n",
      "----------\n",
      "train Loss: 0.8801 Acc: 0.8163\n",
      "test Loss: 0.6867 Acc: 0.8150\n",
      "\n",
      "Epoch 21/39\n",
      "----------\n",
      "train Loss: 0.9180 Acc: 0.8081\n",
      "test Loss: 0.6820 Acc: 0.8176\n",
      "\n",
      "Epoch 22/39\n",
      "----------\n",
      "train Loss: 0.9014 Acc: 0.8041\n",
      "test Loss: 0.6904 Acc: 0.8172\n",
      "\n",
      "Epoch 23/39\n",
      "----------\n",
      "train Loss: 0.9045 Acc: 0.8116\n",
      "test Loss: 0.7008 Acc: 0.8150\n",
      "\n",
      "Epoch 24/39\n",
      "----------\n",
      "train Loss: 0.8822 Acc: 0.8133\n",
      "test Loss: 0.6894 Acc: 0.8158\n",
      "\n",
      "Epoch 25/39\n",
      "----------\n",
      "train Loss: 0.8984 Acc: 0.8126\n",
      "test Loss: 0.6831 Acc: 0.8141\n",
      "\n",
      "Epoch 26/39\n",
      "----------\n",
      "train Loss: 0.8854 Acc: 0.8205\n",
      "test Loss: 0.6849 Acc: 0.8164\n",
      "\n",
      "Epoch 27/39\n",
      "----------\n",
      "train Loss: 0.8918 Acc: 0.8155\n",
      "test Loss: 0.6822 Acc: 0.8171\n",
      "\n",
      "Epoch 28/39\n",
      "----------\n",
      "train Loss: 0.8881 Acc: 0.8150\n",
      "test Loss: 0.6858 Acc: 0.8179\n",
      "\n",
      "Epoch 29/39\n",
      "----------\n",
      "train Loss: 0.8948 Acc: 0.8106\n",
      "test Loss: 0.6881 Acc: 0.8143\n",
      "\n",
      "Epoch 30/39\n",
      "----------\n",
      "train Loss: 0.8978 Acc: 0.8158\n",
      "test Loss: 0.6865 Acc: 0.8167\n",
      "\n",
      "Epoch 31/39\n",
      "----------\n",
      "train Loss: 0.8733 Acc: 0.8192\n",
      "test Loss: 0.6821 Acc: 0.8164\n",
      "\n",
      "Epoch 32/39\n",
      "----------\n",
      "train Loss: 0.9267 Acc: 0.8016\n",
      "test Loss: 0.6862 Acc: 0.8139\n",
      "\n",
      "Epoch 33/39\n",
      "----------\n",
      "train Loss: 0.8825 Acc: 0.8155\n",
      "test Loss: 0.6792 Acc: 0.8153\n",
      "\n",
      "Epoch 34/39\n",
      "----------\n",
      "train Loss: 0.9093 Acc: 0.8048\n",
      "test Loss: 0.6778 Acc: 0.8171\n",
      "\n",
      "Epoch 35/39\n",
      "----------\n",
      "train Loss: 0.9028 Acc: 0.8151\n",
      "test Loss: 0.6850 Acc: 0.8134\n",
      "\n",
      "Epoch 36/39\n",
      "----------\n",
      "train Loss: 0.8872 Acc: 0.8085\n",
      "test Loss: 0.6834 Acc: 0.8153\n",
      "\n",
      "Epoch 37/39\n",
      "----------\n",
      "train Loss: 0.9016 Acc: 0.8103\n",
      "test Loss: 0.6855 Acc: 0.8177\n",
      "\n",
      "Epoch 38/39\n",
      "----------\n",
      "train Loss: 0.8878 Acc: 0.8170\n",
      "test Loss: 0.6900 Acc: 0.8165\n",
      "\n",
      "Epoch 39/39\n",
      "----------\n",
      "train Loss: 0.9081 Acc: 0.8075\n",
      "test Loss: 0.6806 Acc: 0.8186\n",
      "\n",
      "Training complete in 88m 57s\n",
      "Best val Acc: 0.819296\n",
      "Returning object of best model.\n"
     ]
    }
   ],
   "source": [
    "model_ft = train_model(model=model_ft, criterion=criterion, optimizer=optimizer_ft, scheduler=exp_lr_scheduler, \n",
    "                       device=device, dataloaders=dataloaders, dataset_sizes=dataset_sizes, num_epochs=40,\n",
    "                       working_dir=working_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model training history\n",
    "model_history = '../models/classification/resnet152/model_history.pkl'\n",
    "history = unpickle(model_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "At the end of training process, we are passed back model of the epoch at which the model performed the best on. \n",
    "\n",
    "![Image](https://online.stat.psu.edu/onlinecourses/sites/stat508/files/lesson04/model_complexity.png)\n",
    "\n",
    "As model training progresses, it common to see both the training loss and validation loss decrease with iteration. This indicates that the model is learning, and at the end of each epoch, the model is better at predicting the class labels on the images in both the training and test set. At some point, the test/validation loss will start to increase, whilst the training loss continues to decrease. This the point at the which the model has stopped generalizing, that is, it has stopped learning features that will improve its ability to predict class labels on unseen data. Put another way, the model is starting to be over-trained, because the features it is learning are becoming specific to the training set. These features are not replicated in the validation set, they may even be representations of noise specific to the training set, and hence the model performs more poorly as training continues to overfit. Hence, the best model is the model that achieves the lowest validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-ecbb2b80bec9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'epoch'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train_acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'b-'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'epoch'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test_acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r-'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Test'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py38_pytorch171_cu102_cub200/lib/python3.8/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2838\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0m_copy_docstring_and_deprecators\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2839\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscalex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaley\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2840\u001b[0;31m     return gca().plot(\n\u001b[0m\u001b[1;32m   2841\u001b[0m         \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscalex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscalex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaley\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscaley\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2842\u001b[0m         **({\"data\": data} if data is not None else {}), **kwargs)\n",
      "\u001b[0;32m~/.conda/envs/py38_pytorch171_cu102_cub200/lib/python3.8/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1741\u001b[0m         \"\"\"\n\u001b[1;32m   1742\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1743\u001b[0;31m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1744\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py38_pytorch171_cu102_cub200/lib/python3.8/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m    271\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 273\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_next_color\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py38_pytorch171_cu102_cub200/lib/python3.8/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[0;34m(self, tup, kwargs)\u001b[0m\n\u001b[1;32m    387\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 389\u001b[0;31m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    390\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex_of\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py38_pytorch171_cu102_cub200/lib/python3.8/site-packages/matplotlib/cbook/__init__.py\u001b[0m in \u001b[0;36m_check_1d\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1293\u001b[0m     \u001b[0;34m\"\"\"Convert scalars to 1d arrays; pass-through arrays as is.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1294\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'shape'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1295\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matleast_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1296\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1297\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36matleast_1d\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py38_pytorch171_cu102_cub200/lib/python3.8/site-packages/numpy/core/shape_base.py\u001b[0m in \u001b[0;36matleast_1d\u001b[0;34m(*arys)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mary\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marys\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0mary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py38_pytorch171_cu102_cub200/lib/python3.8/site-packages/numpy/core/_asarray.py\u001b[0m in \u001b[0;36masanyarray\u001b[0;34m(a, dtype, order, like)\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_asanyarray_with_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlike\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlike\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py38_pytorch171_cu102_cub200/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    628\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__array__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrelevant_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 630\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    631\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(18,10))\n",
    "\n",
    "plt.subplot(2,1,1)\n",
    "plt.plot(np.arange(0, np.max(history['epoch'])+1,1), history['train_loss'], 'b-', label='Train')\n",
    "plt.plot(np.arange(0, np.max(history['epoch'])+1,1), history['test_loss'], 'r-', label='Test')\n",
    "plt.grid(True)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training / Validation Loss - Caltech Birds - {}'.format(model_name))\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2,1,2)\n",
    "plt.plot(np.arange(0, np.max(history['epoch'])+1,1), history['train_acc'], 'b-', label='Train')\n",
    "plt.plot(np.arange(0, np.max(history['epoch'])+1,1), history['test_acc'], 'r-', label='Test')\n",
    "plt.grid(True)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training / Validation Accuracy - Caltech Birds - {}'.format(model_name))\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model_full(model=model_ft, PATH='models/classification/caltech_birds_resnet152_full.pth')\n",
    "save_model_dict(model=model_ft, PATH='models/classification/caltech_birds_resnet152_dict.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'epoch': [0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  3,\n",
       "  3,\n",
       "  4,\n",
       "  4,\n",
       "  5,\n",
       "  5,\n",
       "  6,\n",
       "  6,\n",
       "  7,\n",
       "  7,\n",
       "  8,\n",
       "  8,\n",
       "  9,\n",
       "  9,\n",
       "  10,\n",
       "  10,\n",
       "  11,\n",
       "  11,\n",
       "  12,\n",
       "  12,\n",
       "  13,\n",
       "  13,\n",
       "  14,\n",
       "  14,\n",
       "  15,\n",
       "  15,\n",
       "  16,\n",
       "  16,\n",
       "  17,\n",
       "  17,\n",
       "  18,\n",
       "  18,\n",
       "  19,\n",
       "  19,\n",
       "  20,\n",
       "  20,\n",
       "  21,\n",
       "  21,\n",
       "  22,\n",
       "  22,\n",
       "  23,\n",
       "  23,\n",
       "  24,\n",
       "  24,\n",
       "  25,\n",
       "  25,\n",
       "  26,\n",
       "  26,\n",
       "  27,\n",
       "  27,\n",
       "  28,\n",
       "  28,\n",
       "  29,\n",
       "  29,\n",
       "  30,\n",
       "  30,\n",
       "  31,\n",
       "  31,\n",
       "  32,\n",
       "  32,\n",
       "  33,\n",
       "  33,\n",
       "  34,\n",
       "  34,\n",
       "  35,\n",
       "  35,\n",
       "  36,\n",
       "  36,\n",
       "  37,\n",
       "  37,\n",
       "  38,\n",
       "  38,\n",
       "  39,\n",
       "  39],\n",
       " 'train_loss': [3.2277253734536435,\n",
       "  2.388307583662204,\n",
       "  1.882707371646498,\n",
       "  1.6019210753776567,\n",
       "  1.3932783668105666,\n",
       "  1.2269834080337483,\n",
       "  1.0981375364052839,\n",
       "  1.0048343398231325,\n",
       "  0.9909793860442169,\n",
       "  0.9572278349090108,\n",
       "  0.9495910000196488,\n",
       "  0.9536937873364291,\n",
       "  0.9501587064973426,\n",
       "  0.9155672166818454,\n",
       "  0.9157184771310898,\n",
       "  0.9147107803228898,\n",
       "  0.9109939937198563,\n",
       "  0.8794236969144336,\n",
       "  0.9161299881157097,\n",
       "  0.8980909749432966,\n",
       "  0.8801103585713858,\n",
       "  0.9180340165490503,\n",
       "  0.9013834195531604,\n",
       "  0.9044563452482303,\n",
       "  0.8822127427703188,\n",
       "  0.8984043432706031,\n",
       "  0.8853947085303229,\n",
       "  0.8918011934310943,\n",
       "  0.8880637681679125,\n",
       "  0.8947845060983657,\n",
       "  0.8977795101063308,\n",
       "  0.8733063837190768,\n",
       "  0.9267313747673303,\n",
       "  0.8825237881274155,\n",
       "  0.9093131103593587,\n",
       "  0.9028179121287935,\n",
       "  0.8872015396475513,\n",
       "  0.9015681251987919,\n",
       "  0.8877903754248951,\n",
       "  0.9080934592354564],\n",
       " 'test_loss': [1.9949015807726895,\n",
       "  1.4148477887465372,\n",
       "  1.163200249991254,\n",
       "  1.0033029794775292,\n",
       "  0.8906540444874953,\n",
       "  0.8293639725144254,\n",
       "  0.739307855763516,\n",
       "  0.7194287533652259,\n",
       "  0.715304890078514,\n",
       "  0.7018469313650161,\n",
       "  0.7002518549844401,\n",
       "  0.6934363264686946,\n",
       "  0.6869559290905348,\n",
       "  0.6999569088529215,\n",
       "  0.6880838064271745,\n",
       "  0.6821628282059461,\n",
       "  0.6837306819498683,\n",
       "  0.6898356487226766,\n",
       "  0.6904870325999709,\n",
       "  0.6807657548959065,\n",
       "  0.6867303766791146,\n",
       "  0.6819932201030463,\n",
       "  0.6904144686153934,\n",
       "  0.7008453689857643,\n",
       "  0.6893804364258723,\n",
       "  0.6831230201884142,\n",
       "  0.6849268504492694,\n",
       "  0.6822473555391807,\n",
       "  0.6858384265935047,\n",
       "  0.6880598223369863,\n",
       "  0.686472986025937,\n",
       "  0.6820670922727555,\n",
       "  0.6861633060099463,\n",
       "  0.6791941043340547,\n",
       "  0.6778351106662606,\n",
       "  0.6850047512407503,\n",
       "  0.6833653783320887,\n",
       "  0.68552856201812,\n",
       "  0.6900091479369431,\n",
       "  0.6806342602084742],\n",
       " 'train_acc': [tensor(0.3130, device='cuda:0', dtype=torch.float64),\n",
       "  tensor(0.4600, device='cuda:0', dtype=torch.float64),\n",
       "  tensor(0.5829, device='cuda:0', dtype=torch.float64),\n",
       "  tensor(0.6333, device='cuda:0', dtype=torch.float64),\n",
       "  tensor(0.6757, device='cuda:0', dtype=torch.float64),\n",
       "  tensor(0.7160, device='cuda:0', dtype=torch.float64),\n",
       "  tensor(0.7548, device='cuda:0', dtype=torch.float64),\n",
       "  tensor(0.7821, device='cuda:0', dtype=torch.float64),\n",
       "  tensor(0.7853, device='cuda:0', dtype=torch.float64),\n",
       "  tensor(0.7973, device='cuda:0', dtype=torch.float64),\n",
       "  tensor(0.7975, device='cuda:0', dtype=torch.float64),\n",
       "  tensor(0.7958, device='cuda:0', dtype=torch.float64),\n",
       "  tensor(0.8001, device='cuda:0', dtype=torch.float64),\n",
       "  tensor(0.8063, device='cuda:0', dtype=torch.float64),\n",
       "  tensor(0.8053, device='cuda:0', dtype=torch.float64),\n",
       "  tensor(0.8088, device='cuda:0', dtype=torch.float64),\n",
       "  tensor(0.8088, device='cuda:0', dtype=torch.float64),\n",
       "  tensor(0.8161, device='cuda:0', dtype=torch.float64),\n",
       "  tensor(0.8026, device='cuda:0', dtype=torch.float64),\n",
       "  tensor(0.8126, device='cuda:0', dtype=torch.float64),\n",
       "  tensor(0.8163, device='cuda:0', dtype=torch.float64),\n",
       "  tensor(0.8081, device='cuda:0', dtype=torch.float64),\n",
       "  tensor(0.8041, device='cuda:0', dtype=torch.float64),\n",
       "  tensor(0.8116, device='cuda:0', dtype=torch.float64),\n",
       "  tensor(0.8133, device='cuda:0', dtype=torch.float64),\n",
       "  tensor(0.8126, device='cuda:0', dtype=torch.float64),\n",
       "  tensor(0.8205, device='cuda:0', dtype=torch.float64),\n",
       "  tensor(0.8155, device='cuda:0', dtype=torch.float64),\n",
       "  tensor(0.8150, device='cuda:0', dtype=torch.float64),\n",
       "  tensor(0.8106, device='cuda:0', dtype=torch.float64),\n",
       "  tensor(0.8158, device='cuda:0', dtype=torch.float64),\n",
       "  tensor(0.8192, device='cuda:0', dtype=torch.float64),\n",
       "  tensor(0.8016, device='cuda:0', dtype=torch.float64),\n",
       "  tensor(0.8155, device='cuda:0', dtype=torch.float64),\n",
       "  tensor(0.8048, device='cuda:0', dtype=torch.float64),\n",
       "  tensor(0.8151, device='cuda:0', dtype=torch.float64),\n",
       "  tensor(0.8085, device='cuda:0', dtype=torch.float64),\n",
       "  tensor(0.8103, device='cuda:0', dtype=torch.float64),\n",
       "  tensor(0.8170, device='cuda:0', dtype=torch.float64),\n",
       "  tensor(0.8075, device='cuda:0', dtype=torch.float64)],\n",
       " 'test_acc': [tensor(0.5169, device='cuda:0', dtype=torch.float64),\n",
       "  tensor(0.6505, device='cuda:0', dtype=torch.float64),\n",
       "  tensor(0.6823, device='cuda:0', dtype=torch.float64),\n",
       "  tensor(0.7209, device='cuda:0', dtype=torch.float64),\n",
       "  tensor(0.7584, device='cuda:0', dtype=torch.float64),\n",
       "  tensor(0.7604, device='cuda:0', dtype=torch.float64),\n",
       "  tensor(0.7994, device='cuda:0', dtype=torch.float64),\n",
       "  tensor(0.8084, device='cuda:0', dtype=torch.float64),\n",
       "  tensor(0.8117, device='cuda:0', dtype=torch.float64),\n",
       "  tensor(0.8105, device='cuda:0', dtype=torch.float64),\n",
       "  tensor(0.8112, device='cuda:0', dtype=torch.float64),\n",
       "  tensor(0.8155, device='cuda:0', dtype=torch.float64),\n",
       "  tensor(0.8145, device='cuda:0', dtype=torch.float64),\n",
       "  tensor(0.8115, device='cuda:0', dtype=torch.float64),\n",
       "  tensor(0.8139, device='cuda:0', dtype=torch.float64),\n",
       "  tensor(0.8160, device='cuda:0', dtype=torch.float64),\n",
       "  tensor(0.8145, device='cuda:0', dtype=torch.float64),\n",
       "  tensor(0.8145, device='cuda:0', dtype=torch.float64),\n",
       "  tensor(0.8141, device='cuda:0', dtype=torch.float64),\n",
       "  tensor(0.8193, device='cuda:0', dtype=torch.float64),\n",
       "  tensor(0.8150, device='cuda:0', dtype=torch.float64),\n",
       "  tensor(0.8176, device='cuda:0', dtype=torch.float64),\n",
       "  tensor(0.8172, device='cuda:0', dtype=torch.float64),\n",
       "  tensor(0.8150, device='cuda:0', dtype=torch.float64),\n",
       "  tensor(0.8158, device='cuda:0', dtype=torch.float64),\n",
       "  tensor(0.8141, device='cuda:0', dtype=torch.float64),\n",
       "  tensor(0.8164, device='cuda:0', dtype=torch.float64),\n",
       "  tensor(0.8171, device='cuda:0', dtype=torch.float64),\n",
       "  tensor(0.8179, device='cuda:0', dtype=torch.float64),\n",
       "  tensor(0.8143, device='cuda:0', dtype=torch.float64),\n",
       "  tensor(0.8167, device='cuda:0', dtype=torch.float64),\n",
       "  tensor(0.8164, device='cuda:0', dtype=torch.float64),\n",
       "  tensor(0.8139, device='cuda:0', dtype=torch.float64),\n",
       "  tensor(0.8153, device='cuda:0', dtype=torch.float64),\n",
       "  tensor(0.8171, device='cuda:0', dtype=torch.float64),\n",
       "  tensor(0.8134, device='cuda:0', dtype=torch.float64),\n",
       "  tensor(0.8153, device='cuda:0', dtype=torch.float64),\n",
       "  tensor(0.8177, device='cuda:0', dtype=torch.float64),\n",
       "  tensor(0.8165, device='cuda:0', dtype=torch.float64),\n",
       "  tensor(0.8186, device='cuda:0', dtype=torch.float64)]}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3.8 PyTorch 1.7.1 CUB200 [Cuda 10.2]",
   "language": "python",
   "name": "py38_pytorch171_cu102_cub200"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
