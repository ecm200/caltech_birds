{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets, models\n",
    "\n",
    "import skorch\n",
    "from skorch import NeuralNetClassifier\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import copy\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "import numpy as np\n",
    "from numpy import dstack\n",
    "\n",
    "# Local modules\n",
    "from cub_tools.train import train_model\n",
    "from cub_tools.visualize import imshow, visualize_model\n",
    "from cub_tools.utils import unpickle, save_pickle\n",
    "from cub_tools.transforms import makeDefaultTransforms\n",
    "#from cub_tools.ensembles import stackedEnsemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Script runtime options\n",
    "model_names = ['resnet152', 'resnext101_32x8d', 'inception_v3', 'googlenet']\n",
    "data_parallel = {'resnet152' : False,\n",
    "                 'inceptionv4' : True,\n",
    "                 'resnext101_64x4d' : True, \n",
    "                 'pnasnet5large' : True, \n",
    "                 'googlenet' : False,\n",
    "                 'inception_v3' : False, \n",
    "                 'resnext101_32x8d' : False}\n",
    "data_root_dir = '../data'\n",
    "model_root_dir = '../models'\n",
    "stages = ['train', 'test']\n",
    "\n",
    "\n",
    "# Paths setup\n",
    "data_dir = os.path.join(data_root_dir,'images')\n",
    "\n",
    "# Ensemble setup\n",
    "run_train_data_stack = False\n",
    "run_test_data_stack = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data transforms\n",
    "data_transforms = makeDefaultTransforms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup data loaders with augmentation transforms\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x])\n",
    "                  for x in stages}\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=16,\n",
    "                                             shuffle=True, num_workers=4)\n",
    "              for x in stages}\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in stages}\n",
    "class_names = image_datasets[stages[0]].classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Setup the device to run the computations\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Device::', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading model resnet152\n",
      "[INFO] Loading model resnext101_32x8d\n",
      "[INFO] Loading model inception_v3\n",
      "[INFO] Loading model googlenet\n"
     ]
    }
   ],
   "source": [
    "models = {}\n",
    "\n",
    "for model_name in model_names:\n",
    "    \n",
    "    print('[INFO] Loading model {}'.format(model_name))\n",
    "    \n",
    "    # Paths\n",
    "    output_dir = os.path.join(model_root_dir,'classification/{}'.format(model_name))\n",
    "    model_file = os.path.join(output_dir, 'caltech_birds_{}_full.pth'.format(model_name))\n",
    "    \n",
    "    # Load the best model from file\n",
    "    models[model_name] = torch.load(model_file)\n",
    "    if data_parallel[model_name]:\n",
    "        models[model_name] = torch.nn.DataParallel(model[model_name])\n",
    "    models[model_name] = models[model_name].to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Methods Part 1 - Stacking Torchvision models\n",
    "\n",
    "Different approaches for ensembling, first lets try stacking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking ensemble bespoke implementation for PyTorch\n",
    "\n",
    "Following the tutorial here: https://machinelearningmastery.com/stacking-ensemble-for-deep-learning-neural-networks/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class stackedEnsemble():\n",
    "    \n",
    "    def __init__(self, meta_learner=None, meta_learner_options=None, models=None, device=None):\n",
    "        \n",
    "        if meta_learner is None:\n",
    "            from sklearn.linear_model import LogisticRegression\n",
    "            self.meta_learner = LogisticRegression\n",
    "        else:\n",
    "            self.meta_learner = meta_learner\n",
    "        self.meta_learner_options = meta_learner_options\n",
    "        self.models = models\n",
    "        self.device = device\n",
    "        self.meta_learner_fit = False\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    def fit(self, dataloader=None, stackX=None, labels=None):\n",
    "        \n",
    "        # create dataset using ensemble\n",
    "        if (stackX is None) or (labels is None):\n",
    "            self.models = models\n",
    "            self.train_dataloader = dataloader\n",
    "            print('[INFO] Creating the meta learner inputs (probabilities from individual models) as none provided.')\n",
    "            self.stackX, self.labels = _stacked_dataset_from_dataloader(self.models, self.dataloader, self.device)\n",
    "        else:\n",
    "            self.train_dataloader = None\n",
    "            self.stackX = stackX\n",
    "            self.labels = labels\n",
    "            print('[INFO] Stacked input table and labels found, using these to train meta learner.')\n",
    "\n",
    "        # fit standalone model\n",
    "        print('[INFO] Training the meta learner...', end='')\n",
    "        if self.meta_learner_options is not None:\n",
    "            self.meta_model = self.meta_learner(**self.meta_learner_options)\n",
    "        else:\n",
    "            self.meta_model = self.meta_learner()\n",
    "        \n",
    "        self.meta_model.fit(self.stackX, self.labels)\n",
    "        \n",
    "        self.meta_learner_fit = True\n",
    "        print('..Complete')\n",
    "        \n",
    "    \n",
    "    \n",
    "    def predict(self, dataloader=None, stackX=None):\n",
    "        assert self.meta_learner_fit is True, 'Meta Leaner has not been fit. Please run the stackedEnsemble.fit() method to fit the meta learner before trying to predict'\n",
    "        self.test_dataloader=dataloader\n",
    "        self.test_stackX=stackX\n",
    "        # create dataset using ensemble\n",
    "        if (self.test_stackX is None) or (self.test_dataloader is not None):\n",
    "            print('[INFO] Creating the meta learner inputs (probabilities from individual models) as none provided.')\n",
    "            self.test_stackX, _ = _stacked_dataset_from_dataloader(self.models, self.test_dataloader, self.device)\n",
    "        else:\n",
    "            print('[INFO] Stacked input table and labels found, using these to train meta learner.')\n",
    "\n",
    "        # predict using the trained meta learner\n",
    "        print('[INFO] Predicting with the meta learner...', end='')\n",
    "        self.yhat = self.meta_model.predict(X=self.test_stackX)\n",
    "        \n",
    "        \n",
    "        print('..Complete')\n",
    "        \n",
    "        \n",
    "    def class_report(self, y_true):\n",
    "        from sklearn.metrics import classification_report\n",
    "        print(classification_report(y_pred=self.yhat, y_true=y_true))\n",
    "        \n",
    "        \n",
    "    def _stacked_dataset_from_dataloader(models, dataloader, device):\n",
    "        stackX = None\n",
    "        stacky = None\n",
    "        print('[INFO] Starting StackX', end='')\n",
    "        with torch.no_grad():\n",
    "            for i, (inputs, labels) in enumerate(dataloader):\n",
    "                if i < (len(dataloader)-1):\n",
    "                    temp_stack = None\n",
    "                    for model_name, model in models.items():\n",
    "                        # make prediction\n",
    "                        if isinstance(model, skorch.classifier.NeuralNetClassifier):\n",
    "                            yhat = model.predict_proba(inputs)\n",
    "                        else:\n",
    "                            model.eval()\n",
    "                            inputs = inputs.to(device)\n",
    "                            yhat = model(inputs)\n",
    "                            yhat = yhat.cpu().numpy()\n",
    "\n",
    "                        # Convert score to probability\n",
    "                        for ind in np.arange(0, yhat.shape[0], 1):\n",
    "                            yhat[ind, ::] = softmax(yhat[ind,::])\n",
    "\n",
    "                        # stack predictions into [rows, members, probabilities]\n",
    "                        if temp_stack is None:\n",
    "                            temp_stack = yhat\n",
    "                        else:\n",
    "                            temp_stack = dstack((temp_stack, yhat))\n",
    "\n",
    "                    # flatten predictions to [rows, members x probabilities]\n",
    "                    temp_stack = temp_stack.reshape((temp_stack.shape[0], temp_stack.shape[1]*temp_stack.shape[2]))\n",
    "                    # stack the batch of model probabilities onto the bottom of the results table\n",
    "                    if stackX is None:\n",
    "                        stackX = temp_stack\n",
    "                    else:\n",
    "                        stackX = np.vstack((stackX, temp_stack))\n",
    "\n",
    "                    # stack the output truth labels to bottom of truth labels table\n",
    "                    if stacky is None:\n",
    "                        stacky = labels.cpu().numpy().ravel()\n",
    "                    else:\n",
    "                        stacky = np.vstack((stacky, labels.cpu().numpy().ravel()))\n",
    "\n",
    "                    if i % 5 == 0:\n",
    "                        print('..{}'.format(i), end='')\n",
    "\n",
    "        print('..Complete')\n",
    "        return stackX, stacky"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the stacked datasets for train and test sets\n",
    "\n",
    "This process pushes the training and test images through the CNN models trained on the image classification problem, and stacks their predicted class probabilities into a single array. Assuming there N images in the set, and M models in the ensemble, the expected size of the array is as follows:\n",
    "\n",
    "nx (cols): number of classes by number of models e.g. 4 models and 200 classes, 800 columns.\n",
    "ny (rows): number of images\n",
    "\n",
    "The array is arrange as follows:\n",
    "\n",
    "    image 1: |---200 class probability columns model 1---|---200 class probability columns model 2---|---***---|---200 class probability columns model M---|\n",
    "    image 2: |---200 class probability columns model 1---|---200 class probability columns model 2---|---***---|---200 class probability columns model M---|\n",
    "    ***\n",
    "    ***\n",
    "    image N: |---200 class probability columns model 1---|---200 class probability columns model 2---|---***---|---200 class probability columns model M---| "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loaded stacked datasets...\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    if run_train_data_stack:\n",
    "        # Generate the stacked dataset from the ensemble models for the training data\n",
    "        stackX, stacky = stacked_dataset_from_dataloader(models, dataloaders['train'], device)\n",
    "\n",
    "        # Save to pikle file\n",
    "        save_pickle(stackX, os.path.join(model_root_dir,'classification/caltech_birds_ensemble_1_stackX.pkl'))\n",
    "        save_pickle(stacky, os.path.join(model_root_dir,'classification/caltech_birds_ensemble_1_stacky.pkl'))\n",
    "    else:\n",
    "        stackX = unpickle(os.path.join(model_root_dir,'classification/caltech_birds_ensemble_1_stackX.pkl'))\n",
    "        stacky = unpickle(os.path.join(model_root_dir,'classification/caltech_birds_ensemble_1_stacky.pkl'))\n",
    "\n",
    "    print('[INFO] Loaded stacked datasets...')\n",
    "except:\n",
    "    print('[INFO] Error trying to create or load datasets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loaded stacked datasets...\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    if run_test_data_stack:\n",
    "        # Generate the stacked dataset from the ensemble models for the training data\n",
    "        test_stackX, test_stacky = stacked_dataset_from_dataloader(models, dataloaders['test'], device)\n",
    "\n",
    "        # Save to pikle file\n",
    "        save_pickle(test_stackX, os.path.join(model_root_dir,'classification/caltech_birds_ensemble_1_test_stackX.pkl'))\n",
    "        save_pickle(test_stacky, os.path.join(model_root_dir,'classification/caltech_birds_ensemble_1_test_stacky.pkl'))\n",
    "    else:\n",
    "        test_stackX = unpickle(os.path.join(model_root_dir,'classification/caltech_birds_ensemble_1_test_stackX.pkl'))\n",
    "        test_stacky = unpickle(os.path.join(model_root_dir,'classification/caltech_birds_ensemble_1_test_stacky.pkl'))\n",
    "        print('[INFO] Loaded stacked datasets...')\n",
    "except:\n",
    "    print('[INFO] Error trying to create or load datasets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the ensemble of models\n",
    "\n",
    "The stacking ensemble takes either the set of PyTorch trained models and the PyTorch dataloader object and runs the dataset stacking prior to fitting the meta learner of the ensemble, or it takes the pre-stacked dataset of class predictions from the image set and simply fits the classifier object.\n",
    "\n",
    "The classifier object as default is a Logistic Regression. Any scikit-learn type classifier object can be passed to the fitting function, as long as it has a **fit** and **predict** method. Optional hyperparameter arguments can be provided to initialise the classifier object, and these can be passed in a dictionary with the *meta_learner_options* variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from xgboost import XGBClassifier, XGBRFClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default Logistic Regression\n",
    "\n",
    "Run predictions using the test set stacked dataset and run classification metrics to assess the model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Stacked input table and labels found, using these to train meta learner.\n",
      "[INFO] Training the meta learner.....Complete\n",
      "[INFO] Stacked input table and labels found, using these to train meta learner.\n",
      "[INFO] Predicting with the meta learner.....Complete\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.87      0.85        30\n",
      "           1       0.93      0.87      0.90        30\n",
      "           2       0.84      0.96      0.90        28\n",
      "           3       0.94      0.97      0.95        30\n",
      "           4       0.93      0.93      0.93        14\n",
      "           5       0.91      0.91      0.91        11\n",
      "           6       1.00      0.96      0.98        23\n",
      "           7       0.81      0.94      0.87        18\n",
      "           8       0.59      0.45      0.51        29\n",
      "           9       0.97      0.93      0.95        30\n",
      "          10       0.65      0.67      0.66        30\n",
      "          11       0.93      0.96      0.94        26\n",
      "          12       0.97      0.97      0.97        30\n",
      "          13       0.94      0.97      0.95        30\n",
      "          14       0.93      0.96      0.95        28\n",
      "          15       1.00      0.96      0.98        28\n",
      "          16       1.00      0.93      0.96        27\n",
      "          17       1.00      1.00      1.00        15\n",
      "          18       1.00      0.90      0.95        29\n",
      "          19       0.82      0.97      0.89        29\n",
      "          20       1.00      1.00      1.00        30\n",
      "          21       0.81      0.81      0.81        26\n",
      "          22       0.66      0.66      0.66        29\n",
      "          23       0.83      0.91      0.87        22\n",
      "          24       0.68      0.57      0.62        30\n",
      "          25       0.93      0.93      0.93        30\n",
      "          26       0.50      0.50      0.50        30\n",
      "          27       1.00      0.90      0.95        29\n",
      "          28       0.56      0.63      0.59        30\n",
      "          29       0.48      0.50      0.49        30\n",
      "          30       0.89      0.83      0.86        30\n",
      "          31       0.90      0.83      0.86        23\n",
      "          32       0.81      0.86      0.83        29\n",
      "          33       0.90      0.93      0.92        29\n",
      "          34       0.86      1.00      0.92        30\n",
      "          35       0.94      0.97      0.95        30\n",
      "          36       0.68      0.72      0.70        29\n",
      "          37       0.96      0.87      0.91        30\n",
      "          38       0.42      0.55      0.48        29\n",
      "          39       0.79      0.73      0.76        30\n",
      "          40       0.96      0.90      0.93        30\n",
      "          41       1.00      0.93      0.97        30\n",
      "          42       0.72      0.45      0.55        29\n",
      "          43       0.96      0.90      0.93        30\n",
      "          44       0.83      0.80      0.81        30\n",
      "          45       0.90      0.93      0.92        30\n",
      "          46       0.90      0.93      0.92        30\n",
      "          47       1.00      1.00      1.00        30\n",
      "          48       0.62      0.77      0.69        30\n",
      "          49       0.90      0.87      0.88        30\n",
      "          50       0.88      0.93      0.90        30\n",
      "          51       1.00      0.97      0.98        30\n",
      "          52       1.00      1.00      1.00        30\n",
      "          53       0.90      0.90      0.90        30\n",
      "          54       1.00      0.87      0.93        30\n",
      "          55       0.97      0.93      0.95        30\n",
      "          56       1.00      1.00      1.00        30\n",
      "          57       0.93      0.89      0.91        28\n",
      "          58       0.39      0.30      0.34        30\n",
      "          59       0.54      0.48      0.51        29\n",
      "          60       0.94      0.97      0.95        30\n",
      "          61       0.49      0.77      0.60        30\n",
      "          62       0.88      1.00      0.94        30\n",
      "          63       0.81      0.57      0.67        30\n",
      "          64       0.73      0.55      0.63        20\n",
      "          65       0.56      0.73      0.64        30\n",
      "          66       0.81      0.70      0.75        30\n",
      "          67       0.78      0.83      0.81        30\n",
      "          68       0.90      0.90      0.90        30\n",
      "          69       0.94      1.00      0.97        30\n",
      "          70       0.67      0.53      0.59        30\n",
      "          71       0.66      0.63      0.64        30\n",
      "          72       1.00      0.97      0.98        30\n",
      "          73       0.97      1.00      0.98        30\n",
      "          74       1.00      1.00      1.00        27\n",
      "          75       0.85      0.97      0.91        30\n",
      "          76       0.91      1.00      0.95        30\n",
      "          77       0.82      0.97      0.89        29\n",
      "          78       0.82      0.90      0.86        30\n",
      "          79       0.96      0.83      0.89        30\n",
      "          80       0.97      0.93      0.95        30\n",
      "          81       0.84      0.90      0.87        30\n",
      "          82       1.00      0.97      0.98        30\n",
      "          83       0.95      0.91      0.93        23\n",
      "          84       0.94      1.00      0.97        30\n",
      "          85       0.94      1.00      0.97        30\n",
      "          86       0.90      0.87      0.88        30\n",
      "          87       1.00      0.97      0.98        30\n",
      "          88       1.00      0.80      0.89        30\n",
      "          89       0.83      1.00      0.91        30\n",
      "          90       0.83      0.80      0.81        30\n",
      "          91       0.86      0.83      0.85        30\n",
      "          92       0.97      1.00      0.98        30\n",
      "          93       0.94      0.97      0.95        30\n",
      "          94       0.96      0.90      0.93        30\n",
      "          95       0.92      0.77      0.84        30\n",
      "          96       0.92      0.76      0.83        29\n",
      "          97       0.90      0.90      0.90        30\n",
      "          98       0.93      0.90      0.92        30\n",
      "          99       0.97      1.00      0.98        30\n",
      "         100       1.00      1.00      1.00        20\n",
      "         101       0.72      0.60      0.65        30\n",
      "         102       0.82      0.77      0.79        30\n",
      "         103       0.85      0.73      0.79        30\n",
      "         104       0.62      0.79      0.70        19\n",
      "         105       1.00      1.00      1.00        29\n",
      "         106       0.71      0.67      0.69        30\n",
      "         107       1.00      0.83      0.91        30\n",
      "         108       0.97      0.93      0.95        30\n",
      "         109       0.88      1.00      0.94        30\n",
      "         110       0.72      0.70      0.71        30\n",
      "         111       0.73      0.73      0.73        30\n",
      "         112       0.60      0.90      0.72        20\n",
      "         113       0.91      0.97      0.94        30\n",
      "         114       0.81      0.76      0.79        29\n",
      "         115       0.85      0.77      0.81        30\n",
      "         116       0.58      0.76      0.66        29\n",
      "         117       0.70      0.63      0.67        30\n",
      "         118       0.76      0.66      0.70        29\n",
      "         119       0.96      0.77      0.85        30\n",
      "         120       0.92      0.80      0.86        30\n",
      "         121       0.97      1.00      0.98        30\n",
      "         122       0.93      0.83      0.88        30\n",
      "         123       0.81      0.86      0.83        29\n",
      "         124       0.89      0.86      0.88        29\n",
      "         125       0.81      0.73      0.77        30\n",
      "         126       0.71      0.80      0.75        30\n",
      "         127       0.80      0.80      0.80        30\n",
      "         128       0.73      0.73      0.73        30\n",
      "         129       0.83      0.63      0.72        30\n",
      "         130       0.68      0.87      0.76        30\n",
      "         131       0.91      0.97      0.94        30\n",
      "         132       0.97      0.93      0.95        30\n",
      "         133       0.97      0.97      0.97        30\n",
      "         134       0.77      0.67      0.71        30\n",
      "         135       0.76      0.73      0.75        30\n",
      "         136       0.76      0.83      0.79        30\n",
      "         137       0.90      0.90      0.90        30\n",
      "         138       0.93      0.90      0.92        30\n",
      "         139       0.94      0.97      0.95        30\n",
      "         140       0.66      0.66      0.66        29\n",
      "         141       0.84      0.70      0.76        30\n",
      "         142       0.60      0.60      0.60        30\n",
      "         143       0.40      0.47      0.43        30\n",
      "         144       0.53      0.60      0.56        30\n",
      "         145       0.66      0.63      0.64        30\n",
      "         146       0.84      0.87      0.85        30\n",
      "         147       1.00      0.97      0.98        30\n",
      "         148       0.97      0.97      0.97        29\n",
      "         149       0.84      0.87      0.85        30\n",
      "         150       1.00      1.00      1.00        20\n",
      "         151       0.87      0.87      0.87        30\n",
      "         152       0.75      0.83      0.79        29\n",
      "         153       0.93      0.87      0.90        30\n",
      "         154       0.93      0.83      0.88        30\n",
      "         155       0.90      0.93      0.92        30\n",
      "         156       0.73      0.83      0.77        29\n",
      "         157       0.94      1.00      0.97        30\n",
      "         158       0.97      1.00      0.98        30\n",
      "         159       0.90      0.97      0.93        29\n",
      "         160       0.80      0.93      0.86        30\n",
      "         161       0.96      0.87      0.91        30\n",
      "         162       0.97      0.97      0.97        30\n",
      "         163       0.93      0.93      0.93        30\n",
      "         164       0.94      0.97      0.95        30\n",
      "         165       0.90      0.93      0.92        29\n",
      "         166       0.91      0.70      0.79        30\n",
      "         167       0.90      0.93      0.92        29\n",
      "         168       0.85      1.00      0.92        29\n",
      "         169       0.96      0.90      0.93        30\n",
      "         170       1.00      0.97      0.98        30\n",
      "         171       0.78      0.93      0.85        30\n",
      "         172       0.75      0.90      0.82        30\n",
      "         173       0.96      0.83      0.89        30\n",
      "         174       0.74      0.77      0.75        30\n",
      "         175       0.94      0.97      0.95        30\n",
      "         176       0.94      0.97      0.95        30\n",
      "         177       1.00      0.88      0.94        26\n",
      "         178       0.69      0.62      0.65        29\n",
      "         179       0.77      0.90      0.83        30\n",
      "         180       0.97      0.97      0.97        29\n",
      "         181       0.93      0.93      0.93        30\n",
      "         182       0.94      1.00      0.97        30\n",
      "         183       0.96      0.90      0.93        30\n",
      "         184       0.93      0.93      0.93        30\n",
      "         185       0.97      0.93      0.95        30\n",
      "         186       1.00      1.00      1.00        20\n",
      "         187       1.00      1.00      1.00        30\n",
      "         188       0.88      1.00      0.94        30\n",
      "         189       1.00      0.97      0.98        29\n",
      "         190       1.00      0.90      0.95        30\n",
      "         191       0.97      1.00      0.98        30\n",
      "         192       0.87      0.87      0.87        30\n",
      "         193       0.93      0.90      0.92        30\n",
      "         194       0.84      0.87      0.85        30\n",
      "         195       0.82      0.77      0.79        30\n",
      "         196       0.81      0.73      0.77        30\n",
      "         197       0.93      0.93      0.93        30\n",
      "         198       0.84      0.87      0.85        30\n",
      "         199       1.00      0.93      0.97        30\n",
      "\n",
      "    accuracy                           0.85      5792\n",
      "   macro avg       0.86      0.85      0.85      5792\n",
      "weighted avg       0.86      0.85      0.85      5792\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mtlearner1 = stackedEnsemble()\n",
    "mtlearner1.fit(stackX=stackX, labels=stacky.ravel())\n",
    "mtlearner1.predict(stackX=test_stackX)\n",
    "mtlearner1.class_report(y_true=test_stacky.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SKLearn Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtlearner2 = stackedEnsemble(meta_learner=RandomForestClassifier,\n",
    "                             meta_learner_options={'n_estimators' : 250, 'verbose' : 10, 'n_jobs' : -1})\n",
    "mtlearner2.fit(stackX=stackX, labels=stacky.ravel())\n",
    "mtlearner2.predict(stackX=test_stackX)\n",
    "mtlearner2.class_report(y_true=test_stacky.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost Classifiers\n",
    "\n",
    "Look at Random Forest and Gradient Boosting Machine classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Stacked input table and labels found, using these to train meta learner.\n",
      "[INFO] Training the meta learner.....Complete\n",
      "[INFO] Stacked input table and labels found, using these to train meta learner.\n",
      "[INFO] Predicting with the meta learner..."
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "predict() got an unexpected keyword argument 'X'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-5bf4bbccdd6a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m                                                    'tree_method' : 'gpu_hist'})\n\u001b[1;32m      8\u001b[0m \u001b[0mmtlearner2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstackX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstackX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacky\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mmtlearner2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstackX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_stackX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mmtlearner2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_stacky\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-8114720bcd38>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, dataloader, stackX)\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;31m# predict using the trained meta learner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'[INFO] Predicting with the meta learner...'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myhat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeta_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_stackX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: predict() got an unexpected keyword argument 'X'"
     ]
    }
   ],
   "source": [
    "mtlearner2 = stackedEnsemble(meta_learner=XGBClassifier,\n",
    "                             meta_learner_options={'n_estimators' : 250, \n",
    "                                                   'verbosity' : 1, \n",
    "                                                   'n_jobs' : 6, \n",
    "                                                   'objective' : 'multi:softmax', \n",
    "                                                   'num_class' : 200,\n",
    "                                                   'tree_method' : 'gpu_hist'})\n",
    "mtlearner2.fit(stackX=stackX, labels=stacky.ravel())\n",
    "mtlearner2.predict(stackX=test_stackX)\n",
    "mtlearner2.class_report(y_true=test_stacky.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "py37_pytorch_003",
   "language": "python",
   "name": "conda-env-py37_pytorch_003-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
