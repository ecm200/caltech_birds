{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Triton Inference Server Test\n",
    "\n",
    "Comparing model inference of an image classification problem using a local PyTorch executed model and a nVidia Triton Inference Server end-point inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "import sys\n",
    "from functools import partial\n",
    "import os\n",
    "from tritonclient import grpc\n",
    "import tritonclient.grpc.model_config_pb2 as mc\n",
    "from tritonclient import http\n",
    "from tritonclient.utils import triton_to_np_dtype\n",
    "from tritonclient.utils import InferenceServerException\n",
    "import torch\n",
    "from clearml import InputModel, Task\n",
    "import shutil\n",
    "import pathlib\n",
    "\n",
    "# Local modules\n",
    "from cub_tools.trainer import ClearML_Ignite_Trainer\n",
    "from cub_tools.args import get_parser\n",
    "from cub_tools.config import get_cfg_defaults, get_key_value_dict\n",
    "from cub_tools.triton import run_inference, get_model_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Models:**\n",
    "\n",
    "    1. PNASNet - d1a0a6c9a33f4d1da1bdf4f81f1595fa\n",
    "    \n",
    "    2. EfficientNetB - 0c8c57cf3c804c14a303375a83509477\n",
    "    \n",
    "    3. SwinT Base - 2be1bb58183c42e2aeb5f9af8c9510fc\n",
    "    \n",
    "    4. GoogLeNet - e72b8460dea742068cbb461107c6f725"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook run parameters\n",
    "model_name = 'SwinTBase' # PNASNet EfficientNetB0, SwinTBase, GoogLeNet\n",
    "n_tests = 15 # Number of test image batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook code starts from here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_name == 'PNASNet':\n",
    "    model = InputModel(model_id=\"d1a0a6c9a33f4d1da1bdf4f81f1595fa\")\n",
    "    model_config = '/home/edmorris/projects/image_classification/caltech_birds/scripts/configs/pytorchcv/pnasnetlarge_config.yaml'\n",
    "    endpoint_name = 'cub200_pnasnet'\n",
    "\n",
    "elif model_name == 'EfficientNetB0':\n",
    "    model = InputModel(model_id=\"0c8c57cf3c804c14a303375a83509477\")\n",
    "    model_config = '/home/edmorris/projects/image_classification/caltech_birds/scripts/configs/pytorchcv/efficientnet_b0_config.yaml'\n",
    "    endpoint_name = 'cub200_enetb0'\n",
    "\n",
    "elif model_name == 'SwinTBase':\n",
    "    model = InputModel(model_id=\"2be1bb58183c42e2aeb5f9af8c9510fc\")\n",
    "    model_config = '/home/edmorris/projects/image_classification/caltech_birds/scripts/configs/timm/swinbase_config.yaml'\n",
    "    endpoint_name = 'cub200_swinbase'\n",
    "\n",
    "elif model_name == 'GoogLeNet':\n",
    "    model = InputModel(model_id=\"e72b8460dea742068cbb461107c6f725\")\n",
    "    model_config = '/home/edmorris/projects/image_classification/caltech_birds/scripts/configs/torchvision/googlenet_config.yaml'\n",
    "    endpoint_name = 'cub200_googlenet'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the model weights locally.\n",
    "local_cache_path = model.get_local_copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Parameters Override:: ['DIRS.CLEAN_UP', False, 'MODEL.PRETRAINED', False]\n",
      "DATA:\n",
      "  DATA_DIR: /home/edmorris/projects/image_classification/caltech_birds/data/images\n",
      "  NUM_CLASSES: 200\n",
      "  TEST_DIR: test\n",
      "  TRAIN_DIR: train\n",
      "  TRANSFORMS:\n",
      "    PARAMS:\n",
      "      AGGRESIVE:\n",
      "        persp_distortion_scale: 0.25\n",
      "        rotation_range: (-10.0, 10.0)\n",
      "        type: all\n",
      "      DEFAULT:\n",
      "        img_crop_size: 384\n",
      "        img_resize: 512\n",
      "    TYPE: default\n",
      "DIRS:\n",
      "  CLEAN_UP: False\n",
      "  ROOT_DIR: /home/edmorris/projects/image_classification/caltech_birds\n",
      "  WORKING_DIR: /home/edmorris/projects/image_classification/caltech_birds/models/classification/ignite_swin_base_patch4_window12_384\n",
      "EARLY_STOPPING_PATIENCE: 5\n",
      "MODEL:\n",
      "  MODEL_LIBRARY: timm\n",
      "  MODEL_NAME: swin_base_patch4_window12_384\n",
      "  PRETRAINED: False\n",
      "  WITH_AMP: False\n",
      "  WITH_GRAD_SCALE: False\n",
      "SYSTEM:\n",
      "  LOG_HISTORY: True\n",
      "TRAIN:\n",
      "  BATCH_SIZE: 16\n",
      "  LOSS:\n",
      "    CRITERION: CrossEntropy\n",
      "  NUM_EPOCHS: 40\n",
      "  NUM_WORKERS: 4\n",
      "  OPTIMIZER:\n",
      "    PARAMS:\n",
      "      lr: 0.001\n",
      "      momentum: 0.9\n",
      "      nesterov: True\n",
      "    TYPE: SGD\n",
      "  SCHEDULER:\n",
      "    PARAMS: ['step_size', 7, 'gamma', 0.1]\n",
      "    TYPE: StepLR\n"
     ]
    }
   ],
   "source": [
    "cmd_args = [\n",
    "    'DIRS.CLEAN_UP', False,     # Don't do anything to the directory structure.\n",
    "    'MODEL.PRETRAINED', False,  # Don't load default weights, as we want to load our own.\n",
    "    ]  \n",
    "trainer = ClearML_Ignite_Trainer(task=None, config=model_config, cmd_args=cmd_args) # TODO Get config from clearml task at some point. From model.task to get task ID, and then pull the task config.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Transformer Properties\n",
      "DEFAULT :: img_crop_size: 384\n",
      "img_resize: 512\n",
      "AGGRESIVE :: persp_distortion_scale: 0.25\n",
      "rotation_range: (-10.0, 10.0)\n",
      "type: all\n"
     ]
    }
   ],
   "source": [
    "print('Image Transformer Properties')\n",
    "for key, value in trainer.config.DATA.TRANSFORMS.PARAMS.items():\n",
    "    print('{0} :: {1}'.format(key,value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***********************************************\n",
      "**            DATASET SUMMARY                **\n",
      "***********************************************\n",
      "train  size::  5994  images\n",
      "test  size::  5794  images\n",
      "Number of classes::  200\n",
      "***********************************************\n",
      "[INFO] Created data loaders.\n"
     ]
    }
   ],
   "source": [
    "# Get a sample dataset for running inference with\n",
    "trainer.create_datatransforms()\n",
    "trainer.create_dataloaders(shuffle={'train' : True, 'test' : True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Successfully created model but NOT pushed it to the device cuda:0\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [16, 128, 96, 96]           6,272\n",
      "         LayerNorm-2            [16, 9216, 128]             256\n",
      "        PatchEmbed-3            [16, 9216, 128]               0\n",
      "           Dropout-4            [16, 9216, 128]               0\n",
      "         LayerNorm-5            [16, 9216, 128]             256\n",
      "            Linear-6             [16, 144, 384]          49,536\n",
      "           Softmax-7          [16, 4, 144, 144]               0\n",
      "           Dropout-8          [16, 4, 144, 144]               0\n",
      "            Linear-9             [16, 144, 128]          16,512\n",
      "          Dropout-10             [16, 144, 128]               0\n",
      "  WindowAttention-11             [16, 144, 128]               0\n",
      "         Identity-12            [16, 9216, 128]               0\n",
      "        LayerNorm-13            [16, 9216, 128]             256\n",
      "           Linear-14            [16, 9216, 512]          66,048\n",
      "             GELU-15            [16, 9216, 512]               0\n",
      "          Dropout-16            [16, 9216, 512]               0\n",
      "           Linear-17            [16, 9216, 128]          65,664\n",
      "          Dropout-18            [16, 9216, 128]               0\n",
      "              Mlp-19            [16, 9216, 128]               0\n",
      "         Identity-20            [16, 9216, 128]               0\n",
      "SwinTransformerBlock-21            [16, 9216, 128]               0\n",
      "        LayerNorm-22            [16, 9216, 128]             256\n",
      "           Linear-23             [16, 144, 384]          49,536\n",
      "          Softmax-24          [16, 4, 144, 144]               0\n",
      "          Dropout-25          [16, 4, 144, 144]               0\n",
      "           Linear-26             [16, 144, 128]          16,512\n",
      "          Dropout-27             [16, 144, 128]               0\n",
      "  WindowAttention-28             [16, 144, 128]               0\n",
      "         DropPath-29            [16, 9216, 128]               0\n",
      "        LayerNorm-30            [16, 9216, 128]             256\n",
      "           Linear-31            [16, 9216, 512]          66,048\n",
      "             GELU-32            [16, 9216, 512]               0\n",
      "          Dropout-33            [16, 9216, 512]               0\n",
      "           Linear-34            [16, 9216, 128]          65,664\n",
      "          Dropout-35            [16, 9216, 128]               0\n",
      "              Mlp-36            [16, 9216, 128]               0\n",
      "         DropPath-37            [16, 9216, 128]               0\n",
      "SwinTransformerBlock-38            [16, 9216, 128]               0\n",
      "        LayerNorm-39            [16, 2304, 512]           1,024\n",
      "           Linear-40            [16, 2304, 256]         131,072\n",
      "     PatchMerging-41            [16, 2304, 256]               0\n",
      "       BasicLayer-42            [16, 2304, 256]               0\n",
      "        LayerNorm-43            [16, 2304, 256]             512\n",
      "           Linear-44             [16, 144, 768]         197,376\n",
      "          Softmax-45          [16, 8, 144, 144]               0\n",
      "          Dropout-46          [16, 8, 144, 144]               0\n",
      "           Linear-47             [16, 144, 256]          65,792\n",
      "          Dropout-48             [16, 144, 256]               0\n",
      "  WindowAttention-49             [16, 144, 256]               0\n",
      "         DropPath-50            [16, 2304, 256]               0\n",
      "        LayerNorm-51            [16, 2304, 256]             512\n",
      "           Linear-52           [16, 2304, 1024]         263,168\n",
      "             GELU-53           [16, 2304, 1024]               0\n",
      "          Dropout-54           [16, 2304, 1024]               0\n",
      "           Linear-55            [16, 2304, 256]         262,400\n",
      "          Dropout-56            [16, 2304, 256]               0\n",
      "              Mlp-57            [16, 2304, 256]               0\n",
      "         DropPath-58            [16, 2304, 256]               0\n",
      "SwinTransformerBlock-59            [16, 2304, 256]               0\n",
      "        LayerNorm-60            [16, 2304, 256]             512\n",
      "           Linear-61             [16, 144, 768]         197,376\n",
      "          Softmax-62          [16, 8, 144, 144]               0\n",
      "          Dropout-63          [16, 8, 144, 144]               0\n",
      "           Linear-64             [16, 144, 256]          65,792\n",
      "          Dropout-65             [16, 144, 256]               0\n",
      "  WindowAttention-66             [16, 144, 256]               0\n",
      "         DropPath-67            [16, 2304, 256]               0\n",
      "        LayerNorm-68            [16, 2304, 256]             512\n",
      "           Linear-69           [16, 2304, 1024]         263,168\n",
      "             GELU-70           [16, 2304, 1024]               0\n",
      "          Dropout-71           [16, 2304, 1024]               0\n",
      "           Linear-72            [16, 2304, 256]         262,400\n",
      "          Dropout-73            [16, 2304, 256]               0\n",
      "              Mlp-74            [16, 2304, 256]               0\n",
      "         DropPath-75            [16, 2304, 256]               0\n",
      "SwinTransformerBlock-76            [16, 2304, 256]               0\n",
      "        LayerNorm-77            [16, 576, 1024]           2,048\n",
      "           Linear-78             [16, 576, 512]         524,288\n",
      "     PatchMerging-79             [16, 576, 512]               0\n",
      "       BasicLayer-80             [16, 576, 512]               0\n",
      "        LayerNorm-81             [16, 576, 512]           1,024\n",
      "           Linear-82            [16, 144, 1536]         787,968\n",
      "          Softmax-83         [16, 16, 144, 144]               0\n",
      "          Dropout-84         [16, 16, 144, 144]               0\n",
      "           Linear-85             [16, 144, 512]         262,656\n",
      "          Dropout-86             [16, 144, 512]               0\n",
      "  WindowAttention-87             [16, 144, 512]               0\n",
      "         DropPath-88             [16, 576, 512]               0\n",
      "        LayerNorm-89             [16, 576, 512]           1,024\n",
      "           Linear-90            [16, 576, 2048]       1,050,624\n",
      "             GELU-91            [16, 576, 2048]               0\n",
      "          Dropout-92            [16, 576, 2048]               0\n",
      "           Linear-93             [16, 576, 512]       1,049,088\n",
      "          Dropout-94             [16, 576, 512]               0\n",
      "              Mlp-95             [16, 576, 512]               0\n",
      "         DropPath-96             [16, 576, 512]               0\n",
      "SwinTransformerBlock-97             [16, 576, 512]               0\n",
      "        LayerNorm-98             [16, 576, 512]           1,024\n",
      "           Linear-99            [16, 144, 1536]         787,968\n",
      "         Softmax-100         [16, 16, 144, 144]               0\n",
      "         Dropout-101         [16, 16, 144, 144]               0\n",
      "          Linear-102             [16, 144, 512]         262,656\n",
      "         Dropout-103             [16, 144, 512]               0\n",
      " WindowAttention-104             [16, 144, 512]               0\n",
      "        DropPath-105             [16, 576, 512]               0\n",
      "       LayerNorm-106             [16, 576, 512]           1,024\n",
      "          Linear-107            [16, 576, 2048]       1,050,624\n",
      "            GELU-108            [16, 576, 2048]               0\n",
      "         Dropout-109            [16, 576, 2048]               0\n",
      "          Linear-110             [16, 576, 512]       1,049,088\n",
      "         Dropout-111             [16, 576, 512]               0\n",
      "             Mlp-112             [16, 576, 512]               0\n",
      "        DropPath-113             [16, 576, 512]               0\n",
      "SwinTransformerBlock-114             [16, 576, 512]               0\n",
      "       LayerNorm-115             [16, 576, 512]           1,024\n",
      "          Linear-116            [16, 144, 1536]         787,968\n",
      "         Softmax-117         [16, 16, 144, 144]               0\n",
      "         Dropout-118         [16, 16, 144, 144]               0\n",
      "          Linear-119             [16, 144, 512]         262,656\n",
      "         Dropout-120             [16, 144, 512]               0\n",
      " WindowAttention-121             [16, 144, 512]               0\n",
      "        DropPath-122             [16, 576, 512]               0\n",
      "       LayerNorm-123             [16, 576, 512]           1,024\n",
      "          Linear-124            [16, 576, 2048]       1,050,624\n",
      "            GELU-125            [16, 576, 2048]               0\n",
      "         Dropout-126            [16, 576, 2048]               0\n",
      "          Linear-127             [16, 576, 512]       1,049,088\n",
      "         Dropout-128             [16, 576, 512]               0\n",
      "             Mlp-129             [16, 576, 512]               0\n",
      "        DropPath-130             [16, 576, 512]               0\n",
      "SwinTransformerBlock-131             [16, 576, 512]               0\n",
      "       LayerNorm-132             [16, 576, 512]           1,024\n",
      "          Linear-133            [16, 144, 1536]         787,968\n",
      "         Softmax-134         [16, 16, 144, 144]               0\n",
      "         Dropout-135         [16, 16, 144, 144]               0\n",
      "          Linear-136             [16, 144, 512]         262,656\n",
      "         Dropout-137             [16, 144, 512]               0\n",
      " WindowAttention-138             [16, 144, 512]               0\n",
      "        DropPath-139             [16, 576, 512]               0\n",
      "       LayerNorm-140             [16, 576, 512]           1,024\n",
      "          Linear-141            [16, 576, 2048]       1,050,624\n",
      "            GELU-142            [16, 576, 2048]               0\n",
      "         Dropout-143            [16, 576, 2048]               0\n",
      "          Linear-144             [16, 576, 512]       1,049,088\n",
      "         Dropout-145             [16, 576, 512]               0\n",
      "             Mlp-146             [16, 576, 512]               0\n",
      "        DropPath-147             [16, 576, 512]               0\n",
      "SwinTransformerBlock-148             [16, 576, 512]               0\n",
      "       LayerNorm-149             [16, 576, 512]           1,024\n",
      "          Linear-150            [16, 144, 1536]         787,968\n",
      "         Softmax-151         [16, 16, 144, 144]               0\n",
      "         Dropout-152         [16, 16, 144, 144]               0\n",
      "          Linear-153             [16, 144, 512]         262,656\n",
      "         Dropout-154             [16, 144, 512]               0\n",
      " WindowAttention-155             [16, 144, 512]               0\n",
      "        DropPath-156             [16, 576, 512]               0\n",
      "       LayerNorm-157             [16, 576, 512]           1,024\n",
      "          Linear-158            [16, 576, 2048]       1,050,624\n",
      "            GELU-159            [16, 576, 2048]               0\n",
      "         Dropout-160            [16, 576, 2048]               0\n",
      "          Linear-161             [16, 576, 512]       1,049,088\n",
      "         Dropout-162             [16, 576, 512]               0\n",
      "             Mlp-163             [16, 576, 512]               0\n",
      "        DropPath-164             [16, 576, 512]               0\n",
      "SwinTransformerBlock-165             [16, 576, 512]               0\n",
      "       LayerNorm-166             [16, 576, 512]           1,024\n",
      "          Linear-167            [16, 144, 1536]         787,968\n",
      "         Softmax-168         [16, 16, 144, 144]               0\n",
      "         Dropout-169         [16, 16, 144, 144]               0\n",
      "          Linear-170             [16, 144, 512]         262,656\n",
      "         Dropout-171             [16, 144, 512]               0\n",
      " WindowAttention-172             [16, 144, 512]               0\n",
      "        DropPath-173             [16, 576, 512]               0\n",
      "       LayerNorm-174             [16, 576, 512]           1,024\n",
      "          Linear-175            [16, 576, 2048]       1,050,624\n",
      "            GELU-176            [16, 576, 2048]               0\n",
      "         Dropout-177            [16, 576, 2048]               0\n",
      "          Linear-178             [16, 576, 512]       1,049,088\n",
      "         Dropout-179             [16, 576, 512]               0\n",
      "             Mlp-180             [16, 576, 512]               0\n",
      "        DropPath-181             [16, 576, 512]               0\n",
      "SwinTransformerBlock-182             [16, 576, 512]               0\n",
      "       LayerNorm-183             [16, 576, 512]           1,024\n",
      "          Linear-184            [16, 144, 1536]         787,968\n",
      "         Softmax-185         [16, 16, 144, 144]               0\n",
      "         Dropout-186         [16, 16, 144, 144]               0\n",
      "          Linear-187             [16, 144, 512]         262,656\n",
      "         Dropout-188             [16, 144, 512]               0\n",
      " WindowAttention-189             [16, 144, 512]               0\n",
      "        DropPath-190             [16, 576, 512]               0\n",
      "       LayerNorm-191             [16, 576, 512]           1,024\n",
      "          Linear-192            [16, 576, 2048]       1,050,624\n",
      "            GELU-193            [16, 576, 2048]               0\n",
      "         Dropout-194            [16, 576, 2048]               0\n",
      "          Linear-195             [16, 576, 512]       1,049,088\n",
      "         Dropout-196             [16, 576, 512]               0\n",
      "             Mlp-197             [16, 576, 512]               0\n",
      "        DropPath-198             [16, 576, 512]               0\n",
      "SwinTransformerBlock-199             [16, 576, 512]               0\n",
      "       LayerNorm-200             [16, 576, 512]           1,024\n",
      "          Linear-201            [16, 144, 1536]         787,968\n",
      "         Softmax-202         [16, 16, 144, 144]               0\n",
      "         Dropout-203         [16, 16, 144, 144]               0\n",
      "          Linear-204             [16, 144, 512]         262,656\n",
      "         Dropout-205             [16, 144, 512]               0\n",
      " WindowAttention-206             [16, 144, 512]               0\n",
      "        DropPath-207             [16, 576, 512]               0\n",
      "       LayerNorm-208             [16, 576, 512]           1,024\n",
      "          Linear-209            [16, 576, 2048]       1,050,624\n",
      "            GELU-210            [16, 576, 2048]               0\n",
      "         Dropout-211            [16, 576, 2048]               0\n",
      "          Linear-212             [16, 576, 512]       1,049,088\n",
      "         Dropout-213             [16, 576, 512]               0\n",
      "             Mlp-214             [16, 576, 512]               0\n",
      "        DropPath-215             [16, 576, 512]               0\n",
      "SwinTransformerBlock-216             [16, 576, 512]               0\n",
      "       LayerNorm-217             [16, 576, 512]           1,024\n",
      "          Linear-218            [16, 144, 1536]         787,968\n",
      "         Softmax-219         [16, 16, 144, 144]               0\n",
      "         Dropout-220         [16, 16, 144, 144]               0\n",
      "          Linear-221             [16, 144, 512]         262,656\n",
      "         Dropout-222             [16, 144, 512]               0\n",
      " WindowAttention-223             [16, 144, 512]               0\n",
      "        DropPath-224             [16, 576, 512]               0\n",
      "       LayerNorm-225             [16, 576, 512]           1,024\n",
      "          Linear-226            [16, 576, 2048]       1,050,624\n",
      "            GELU-227            [16, 576, 2048]               0\n",
      "         Dropout-228            [16, 576, 2048]               0\n",
      "          Linear-229             [16, 576, 512]       1,049,088\n",
      "         Dropout-230             [16, 576, 512]               0\n",
      "             Mlp-231             [16, 576, 512]               0\n",
      "        DropPath-232             [16, 576, 512]               0\n",
      "SwinTransformerBlock-233             [16, 576, 512]               0\n",
      "       LayerNorm-234             [16, 576, 512]           1,024\n",
      "          Linear-235            [16, 144, 1536]         787,968\n",
      "         Softmax-236         [16, 16, 144, 144]               0\n",
      "         Dropout-237         [16, 16, 144, 144]               0\n",
      "          Linear-238             [16, 144, 512]         262,656\n",
      "         Dropout-239             [16, 144, 512]               0\n",
      " WindowAttention-240             [16, 144, 512]               0\n",
      "        DropPath-241             [16, 576, 512]               0\n",
      "       LayerNorm-242             [16, 576, 512]           1,024\n",
      "          Linear-243            [16, 576, 2048]       1,050,624\n",
      "            GELU-244            [16, 576, 2048]               0\n",
      "         Dropout-245            [16, 576, 2048]               0\n",
      "          Linear-246             [16, 576, 512]       1,049,088\n",
      "         Dropout-247             [16, 576, 512]               0\n",
      "             Mlp-248             [16, 576, 512]               0\n",
      "        DropPath-249             [16, 576, 512]               0\n",
      "SwinTransformerBlock-250             [16, 576, 512]               0\n",
      "       LayerNorm-251             [16, 576, 512]           1,024\n",
      "          Linear-252            [16, 144, 1536]         787,968\n",
      "         Softmax-253         [16, 16, 144, 144]               0\n",
      "         Dropout-254         [16, 16, 144, 144]               0\n",
      "          Linear-255             [16, 144, 512]         262,656\n",
      "         Dropout-256             [16, 144, 512]               0\n",
      " WindowAttention-257             [16, 144, 512]               0\n",
      "        DropPath-258             [16, 576, 512]               0\n",
      "       LayerNorm-259             [16, 576, 512]           1,024\n",
      "          Linear-260            [16, 576, 2048]       1,050,624\n",
      "            GELU-261            [16, 576, 2048]               0\n",
      "         Dropout-262            [16, 576, 2048]               0\n",
      "          Linear-263             [16, 576, 512]       1,049,088\n",
      "         Dropout-264             [16, 576, 512]               0\n",
      "             Mlp-265             [16, 576, 512]               0\n",
      "        DropPath-266             [16, 576, 512]               0\n",
      "SwinTransformerBlock-267             [16, 576, 512]               0\n",
      "       LayerNorm-268             [16, 576, 512]           1,024\n",
      "          Linear-269            [16, 144, 1536]         787,968\n",
      "         Softmax-270         [16, 16, 144, 144]               0\n",
      "         Dropout-271         [16, 16, 144, 144]               0\n",
      "          Linear-272             [16, 144, 512]         262,656\n",
      "         Dropout-273             [16, 144, 512]               0\n",
      " WindowAttention-274             [16, 144, 512]               0\n",
      "        DropPath-275             [16, 576, 512]               0\n",
      "       LayerNorm-276             [16, 576, 512]           1,024\n",
      "          Linear-277            [16, 576, 2048]       1,050,624\n",
      "            GELU-278            [16, 576, 2048]               0\n",
      "         Dropout-279            [16, 576, 2048]               0\n",
      "          Linear-280             [16, 576, 512]       1,049,088\n",
      "         Dropout-281             [16, 576, 512]               0\n",
      "             Mlp-282             [16, 576, 512]               0\n",
      "        DropPath-283             [16, 576, 512]               0\n",
      "SwinTransformerBlock-284             [16, 576, 512]               0\n",
      "       LayerNorm-285             [16, 576, 512]           1,024\n",
      "          Linear-286            [16, 144, 1536]         787,968\n",
      "         Softmax-287         [16, 16, 144, 144]               0\n",
      "         Dropout-288         [16, 16, 144, 144]               0\n",
      "          Linear-289             [16, 144, 512]         262,656\n",
      "         Dropout-290             [16, 144, 512]               0\n",
      " WindowAttention-291             [16, 144, 512]               0\n",
      "        DropPath-292             [16, 576, 512]               0\n",
      "       LayerNorm-293             [16, 576, 512]           1,024\n",
      "          Linear-294            [16, 576, 2048]       1,050,624\n",
      "            GELU-295            [16, 576, 2048]               0\n",
      "         Dropout-296            [16, 576, 2048]               0\n",
      "          Linear-297             [16, 576, 512]       1,049,088\n",
      "         Dropout-298             [16, 576, 512]               0\n",
      "             Mlp-299             [16, 576, 512]               0\n",
      "        DropPath-300             [16, 576, 512]               0\n",
      "SwinTransformerBlock-301             [16, 576, 512]               0\n",
      "       LayerNorm-302             [16, 576, 512]           1,024\n",
      "          Linear-303            [16, 144, 1536]         787,968\n",
      "         Softmax-304         [16, 16, 144, 144]               0\n",
      "         Dropout-305         [16, 16, 144, 144]               0\n",
      "          Linear-306             [16, 144, 512]         262,656\n",
      "         Dropout-307             [16, 144, 512]               0\n",
      " WindowAttention-308             [16, 144, 512]               0\n",
      "        DropPath-309             [16, 576, 512]               0\n",
      "       LayerNorm-310             [16, 576, 512]           1,024\n",
      "          Linear-311            [16, 576, 2048]       1,050,624\n",
      "            GELU-312            [16, 576, 2048]               0\n",
      "         Dropout-313            [16, 576, 2048]               0\n",
      "          Linear-314             [16, 576, 512]       1,049,088\n",
      "         Dropout-315             [16, 576, 512]               0\n",
      "             Mlp-316             [16, 576, 512]               0\n",
      "        DropPath-317             [16, 576, 512]               0\n",
      "SwinTransformerBlock-318             [16, 576, 512]               0\n",
      "       LayerNorm-319             [16, 576, 512]           1,024\n",
      "          Linear-320            [16, 144, 1536]         787,968\n",
      "         Softmax-321         [16, 16, 144, 144]               0\n",
      "         Dropout-322         [16, 16, 144, 144]               0\n",
      "          Linear-323             [16, 144, 512]         262,656\n",
      "         Dropout-324             [16, 144, 512]               0\n",
      " WindowAttention-325             [16, 144, 512]               0\n",
      "        DropPath-326             [16, 576, 512]               0\n",
      "       LayerNorm-327             [16, 576, 512]           1,024\n",
      "          Linear-328            [16, 576, 2048]       1,050,624\n",
      "            GELU-329            [16, 576, 2048]               0\n",
      "         Dropout-330            [16, 576, 2048]               0\n",
      "          Linear-331             [16, 576, 512]       1,049,088\n",
      "         Dropout-332             [16, 576, 512]               0\n",
      "             Mlp-333             [16, 576, 512]               0\n",
      "        DropPath-334             [16, 576, 512]               0\n",
      "SwinTransformerBlock-335             [16, 576, 512]               0\n",
      "       LayerNorm-336             [16, 576, 512]           1,024\n",
      "          Linear-337            [16, 144, 1536]         787,968\n",
      "         Softmax-338         [16, 16, 144, 144]               0\n",
      "         Dropout-339         [16, 16, 144, 144]               0\n",
      "          Linear-340             [16, 144, 512]         262,656\n",
      "         Dropout-341             [16, 144, 512]               0\n",
      " WindowAttention-342             [16, 144, 512]               0\n",
      "        DropPath-343             [16, 576, 512]               0\n",
      "       LayerNorm-344             [16, 576, 512]           1,024\n",
      "          Linear-345            [16, 576, 2048]       1,050,624\n",
      "            GELU-346            [16, 576, 2048]               0\n",
      "         Dropout-347            [16, 576, 2048]               0\n",
      "          Linear-348             [16, 576, 512]       1,049,088\n",
      "         Dropout-349             [16, 576, 512]               0\n",
      "             Mlp-350             [16, 576, 512]               0\n",
      "        DropPath-351             [16, 576, 512]               0\n",
      "SwinTransformerBlock-352             [16, 576, 512]               0\n",
      "       LayerNorm-353             [16, 576, 512]           1,024\n",
      "          Linear-354            [16, 144, 1536]         787,968\n",
      "         Softmax-355         [16, 16, 144, 144]               0\n",
      "         Dropout-356         [16, 16, 144, 144]               0\n",
      "          Linear-357             [16, 144, 512]         262,656\n",
      "         Dropout-358             [16, 144, 512]               0\n",
      " WindowAttention-359             [16, 144, 512]               0\n",
      "        DropPath-360             [16, 576, 512]               0\n",
      "       LayerNorm-361             [16, 576, 512]           1,024\n",
      "          Linear-362            [16, 576, 2048]       1,050,624\n",
      "            GELU-363            [16, 576, 2048]               0\n",
      "         Dropout-364            [16, 576, 2048]               0\n",
      "          Linear-365             [16, 576, 512]       1,049,088\n",
      "         Dropout-366             [16, 576, 512]               0\n",
      "             Mlp-367             [16, 576, 512]               0\n",
      "        DropPath-368             [16, 576, 512]               0\n",
      "SwinTransformerBlock-369             [16, 576, 512]               0\n",
      "       LayerNorm-370             [16, 576, 512]           1,024\n",
      "          Linear-371            [16, 144, 1536]         787,968\n",
      "         Softmax-372         [16, 16, 144, 144]               0\n",
      "         Dropout-373         [16, 16, 144, 144]               0\n",
      "          Linear-374             [16, 144, 512]         262,656\n",
      "         Dropout-375             [16, 144, 512]               0\n",
      " WindowAttention-376             [16, 144, 512]               0\n",
      "        DropPath-377             [16, 576, 512]               0\n",
      "       LayerNorm-378             [16, 576, 512]           1,024\n",
      "          Linear-379            [16, 576, 2048]       1,050,624\n",
      "            GELU-380            [16, 576, 2048]               0\n",
      "         Dropout-381            [16, 576, 2048]               0\n",
      "          Linear-382             [16, 576, 512]       1,049,088\n",
      "         Dropout-383             [16, 576, 512]               0\n",
      "             Mlp-384             [16, 576, 512]               0\n",
      "        DropPath-385             [16, 576, 512]               0\n",
      "SwinTransformerBlock-386             [16, 576, 512]               0\n",
      "       LayerNorm-387            [16, 144, 2048]           4,096\n",
      "          Linear-388            [16, 144, 1024]       2,097,152\n",
      "    PatchMerging-389            [16, 144, 1024]               0\n",
      "      BasicLayer-390            [16, 144, 1024]               0\n",
      "       LayerNorm-391            [16, 144, 1024]           2,048\n",
      "          Linear-392            [16, 144, 3072]       3,148,800\n",
      "         Softmax-393         [16, 32, 144, 144]               0\n",
      "         Dropout-394         [16, 32, 144, 144]               0\n",
      "          Linear-395            [16, 144, 1024]       1,049,600\n",
      "         Dropout-396            [16, 144, 1024]               0\n",
      " WindowAttention-397            [16, 144, 1024]               0\n",
      "        DropPath-398            [16, 144, 1024]               0\n",
      "       LayerNorm-399            [16, 144, 1024]           2,048\n",
      "          Linear-400            [16, 144, 4096]       4,198,400\n",
      "            GELU-401            [16, 144, 4096]               0\n",
      "         Dropout-402            [16, 144, 4096]               0\n",
      "          Linear-403            [16, 144, 1024]       4,195,328\n",
      "         Dropout-404            [16, 144, 1024]               0\n",
      "             Mlp-405            [16, 144, 1024]               0\n",
      "        DropPath-406            [16, 144, 1024]               0\n",
      "SwinTransformerBlock-407            [16, 144, 1024]               0\n",
      "       LayerNorm-408            [16, 144, 1024]           2,048\n",
      "          Linear-409            [16, 144, 3072]       3,148,800\n",
      "         Softmax-410         [16, 32, 144, 144]               0\n",
      "         Dropout-411         [16, 32, 144, 144]               0\n",
      "          Linear-412            [16, 144, 1024]       1,049,600\n",
      "         Dropout-413            [16, 144, 1024]               0\n",
      " WindowAttention-414            [16, 144, 1024]               0\n",
      "        DropPath-415            [16, 144, 1024]               0\n",
      "       LayerNorm-416            [16, 144, 1024]           2,048\n",
      "          Linear-417            [16, 144, 4096]       4,198,400\n",
      "            GELU-418            [16, 144, 4096]               0\n",
      "         Dropout-419            [16, 144, 4096]               0\n",
      "          Linear-420            [16, 144, 1024]       4,195,328\n",
      "         Dropout-421            [16, 144, 1024]               0\n",
      "             Mlp-422            [16, 144, 1024]               0\n",
      "        DropPath-423            [16, 144, 1024]               0\n",
      "SwinTransformerBlock-424            [16, 144, 1024]               0\n",
      "      BasicLayer-425            [16, 144, 1024]               0\n",
      "       LayerNorm-426            [16, 144, 1024]           2,048\n",
      "AdaptiveAvgPool1d-427              [16, 1024, 1]               0\n",
      "          Linear-428                  [16, 200]         205,000\n",
      "================================================================\n",
      "Total params: 86,884,680\n",
      "Trainable params: 86,884,680\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 27.00\n",
      "Forward/backward pass size (MB): 26734.65\n",
      "Params size (MB): 331.44\n",
      "Estimated Total Size (MB): 27093.09\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "trainer.create_model(load_to_device=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Successfully loaded weights into the model from weights file:: /home/edmorris/.clearml/cache/storage_manager/global/20e3119d38b61ad083794837894030cf.cub200_swin_base_patch4_window12_384_ignite_best_model_0.pt\n",
      "[INFO] Successfully updated model and pushed it to the device cuda:0\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [16, 128, 96, 96]           6,272\n",
      "         LayerNorm-2            [16, 9216, 128]             256\n",
      "        PatchEmbed-3            [16, 9216, 128]               0\n",
      "           Dropout-4            [16, 9216, 128]               0\n",
      "         LayerNorm-5            [16, 9216, 128]             256\n",
      "            Linear-6             [16, 144, 384]          49,536\n",
      "           Softmax-7          [16, 4, 144, 144]               0\n",
      "           Dropout-8          [16, 4, 144, 144]               0\n",
      "            Linear-9             [16, 144, 128]          16,512\n",
      "          Dropout-10             [16, 144, 128]               0\n",
      "  WindowAttention-11             [16, 144, 128]               0\n",
      "         Identity-12            [16, 9216, 128]               0\n",
      "        LayerNorm-13            [16, 9216, 128]             256\n",
      "           Linear-14            [16, 9216, 512]          66,048\n",
      "             GELU-15            [16, 9216, 512]               0\n",
      "          Dropout-16            [16, 9216, 512]               0\n",
      "           Linear-17            [16, 9216, 128]          65,664\n",
      "          Dropout-18            [16, 9216, 128]               0\n",
      "              Mlp-19            [16, 9216, 128]               0\n",
      "         Identity-20            [16, 9216, 128]               0\n",
      "SwinTransformerBlock-21            [16, 9216, 128]               0\n",
      "        LayerNorm-22            [16, 9216, 128]             256\n",
      "           Linear-23             [16, 144, 384]          49,536\n",
      "          Softmax-24          [16, 4, 144, 144]               0\n",
      "          Dropout-25          [16, 4, 144, 144]               0\n",
      "           Linear-26             [16, 144, 128]          16,512\n",
      "          Dropout-27             [16, 144, 128]               0\n",
      "  WindowAttention-28             [16, 144, 128]               0\n",
      "         DropPath-29            [16, 9216, 128]               0\n",
      "        LayerNorm-30            [16, 9216, 128]             256\n",
      "           Linear-31            [16, 9216, 512]          66,048\n",
      "             GELU-32            [16, 9216, 512]               0\n",
      "          Dropout-33            [16, 9216, 512]               0\n",
      "           Linear-34            [16, 9216, 128]          65,664\n",
      "          Dropout-35            [16, 9216, 128]               0\n",
      "              Mlp-36            [16, 9216, 128]               0\n",
      "         DropPath-37            [16, 9216, 128]               0\n",
      "SwinTransformerBlock-38            [16, 9216, 128]               0\n",
      "        LayerNorm-39            [16, 2304, 512]           1,024\n",
      "           Linear-40            [16, 2304, 256]         131,072\n",
      "     PatchMerging-41            [16, 2304, 256]               0\n",
      "       BasicLayer-42            [16, 2304, 256]               0\n",
      "        LayerNorm-43            [16, 2304, 256]             512\n",
      "           Linear-44             [16, 144, 768]         197,376\n",
      "          Softmax-45          [16, 8, 144, 144]               0\n",
      "          Dropout-46          [16, 8, 144, 144]               0\n",
      "           Linear-47             [16, 144, 256]          65,792\n",
      "          Dropout-48             [16, 144, 256]               0\n",
      "  WindowAttention-49             [16, 144, 256]               0\n",
      "         DropPath-50            [16, 2304, 256]               0\n",
      "        LayerNorm-51            [16, 2304, 256]             512\n",
      "           Linear-52           [16, 2304, 1024]         263,168\n",
      "             GELU-53           [16, 2304, 1024]               0\n",
      "          Dropout-54           [16, 2304, 1024]               0\n",
      "           Linear-55            [16, 2304, 256]         262,400\n",
      "          Dropout-56            [16, 2304, 256]               0\n",
      "              Mlp-57            [16, 2304, 256]               0\n",
      "         DropPath-58            [16, 2304, 256]               0\n",
      "SwinTransformerBlock-59            [16, 2304, 256]               0\n",
      "        LayerNorm-60            [16, 2304, 256]             512\n",
      "           Linear-61             [16, 144, 768]         197,376\n",
      "          Softmax-62          [16, 8, 144, 144]               0\n",
      "          Dropout-63          [16, 8, 144, 144]               0\n",
      "           Linear-64             [16, 144, 256]          65,792\n",
      "          Dropout-65             [16, 144, 256]               0\n",
      "  WindowAttention-66             [16, 144, 256]               0\n",
      "         DropPath-67            [16, 2304, 256]               0\n",
      "        LayerNorm-68            [16, 2304, 256]             512\n",
      "           Linear-69           [16, 2304, 1024]         263,168\n",
      "             GELU-70           [16, 2304, 1024]               0\n",
      "          Dropout-71           [16, 2304, 1024]               0\n",
      "           Linear-72            [16, 2304, 256]         262,400\n",
      "          Dropout-73            [16, 2304, 256]               0\n",
      "              Mlp-74            [16, 2304, 256]               0\n",
      "         DropPath-75            [16, 2304, 256]               0\n",
      "SwinTransformerBlock-76            [16, 2304, 256]               0\n",
      "        LayerNorm-77            [16, 576, 1024]           2,048\n",
      "           Linear-78             [16, 576, 512]         524,288\n",
      "     PatchMerging-79             [16, 576, 512]               0\n",
      "       BasicLayer-80             [16, 576, 512]               0\n",
      "        LayerNorm-81             [16, 576, 512]           1,024\n",
      "           Linear-82            [16, 144, 1536]         787,968\n",
      "          Softmax-83         [16, 16, 144, 144]               0\n",
      "          Dropout-84         [16, 16, 144, 144]               0\n",
      "           Linear-85             [16, 144, 512]         262,656\n",
      "          Dropout-86             [16, 144, 512]               0\n",
      "  WindowAttention-87             [16, 144, 512]               0\n",
      "         DropPath-88             [16, 576, 512]               0\n",
      "        LayerNorm-89             [16, 576, 512]           1,024\n",
      "           Linear-90            [16, 576, 2048]       1,050,624\n",
      "             GELU-91            [16, 576, 2048]               0\n",
      "          Dropout-92            [16, 576, 2048]               0\n",
      "           Linear-93             [16, 576, 512]       1,049,088\n",
      "          Dropout-94             [16, 576, 512]               0\n",
      "              Mlp-95             [16, 576, 512]               0\n",
      "         DropPath-96             [16, 576, 512]               0\n",
      "SwinTransformerBlock-97             [16, 576, 512]               0\n",
      "        LayerNorm-98             [16, 576, 512]           1,024\n",
      "           Linear-99            [16, 144, 1536]         787,968\n",
      "         Softmax-100         [16, 16, 144, 144]               0\n",
      "         Dropout-101         [16, 16, 144, 144]               0\n",
      "          Linear-102             [16, 144, 512]         262,656\n",
      "         Dropout-103             [16, 144, 512]               0\n",
      " WindowAttention-104             [16, 144, 512]               0\n",
      "        DropPath-105             [16, 576, 512]               0\n",
      "       LayerNorm-106             [16, 576, 512]           1,024\n",
      "          Linear-107            [16, 576, 2048]       1,050,624\n",
      "            GELU-108            [16, 576, 2048]               0\n",
      "         Dropout-109            [16, 576, 2048]               0\n",
      "          Linear-110             [16, 576, 512]       1,049,088\n",
      "         Dropout-111             [16, 576, 512]               0\n",
      "             Mlp-112             [16, 576, 512]               0\n",
      "        DropPath-113             [16, 576, 512]               0\n",
      "SwinTransformerBlock-114             [16, 576, 512]               0\n",
      "       LayerNorm-115             [16, 576, 512]           1,024\n",
      "          Linear-116            [16, 144, 1536]         787,968\n",
      "         Softmax-117         [16, 16, 144, 144]               0\n",
      "         Dropout-118         [16, 16, 144, 144]               0\n",
      "          Linear-119             [16, 144, 512]         262,656\n",
      "         Dropout-120             [16, 144, 512]               0\n",
      " WindowAttention-121             [16, 144, 512]               0\n",
      "        DropPath-122             [16, 576, 512]               0\n",
      "       LayerNorm-123             [16, 576, 512]           1,024\n",
      "          Linear-124            [16, 576, 2048]       1,050,624\n",
      "            GELU-125            [16, 576, 2048]               0\n",
      "         Dropout-126            [16, 576, 2048]               0\n",
      "          Linear-127             [16, 576, 512]       1,049,088\n",
      "         Dropout-128             [16, 576, 512]               0\n",
      "             Mlp-129             [16, 576, 512]               0\n",
      "        DropPath-130             [16, 576, 512]               0\n",
      "SwinTransformerBlock-131             [16, 576, 512]               0\n",
      "       LayerNorm-132             [16, 576, 512]           1,024\n",
      "          Linear-133            [16, 144, 1536]         787,968\n",
      "         Softmax-134         [16, 16, 144, 144]               0\n",
      "         Dropout-135         [16, 16, 144, 144]               0\n",
      "          Linear-136             [16, 144, 512]         262,656\n",
      "         Dropout-137             [16, 144, 512]               0\n",
      " WindowAttention-138             [16, 144, 512]               0\n",
      "        DropPath-139             [16, 576, 512]               0\n",
      "       LayerNorm-140             [16, 576, 512]           1,024\n",
      "          Linear-141            [16, 576, 2048]       1,050,624\n",
      "            GELU-142            [16, 576, 2048]               0\n",
      "         Dropout-143            [16, 576, 2048]               0\n",
      "          Linear-144             [16, 576, 512]       1,049,088\n",
      "         Dropout-145             [16, 576, 512]               0\n",
      "             Mlp-146             [16, 576, 512]               0\n",
      "        DropPath-147             [16, 576, 512]               0\n",
      "SwinTransformerBlock-148             [16, 576, 512]               0\n",
      "       LayerNorm-149             [16, 576, 512]           1,024\n",
      "          Linear-150            [16, 144, 1536]         787,968\n",
      "         Softmax-151         [16, 16, 144, 144]               0\n",
      "         Dropout-152         [16, 16, 144, 144]               0\n",
      "          Linear-153             [16, 144, 512]         262,656\n",
      "         Dropout-154             [16, 144, 512]               0\n",
      " WindowAttention-155             [16, 144, 512]               0\n",
      "        DropPath-156             [16, 576, 512]               0\n",
      "       LayerNorm-157             [16, 576, 512]           1,024\n",
      "          Linear-158            [16, 576, 2048]       1,050,624\n",
      "            GELU-159            [16, 576, 2048]               0\n",
      "         Dropout-160            [16, 576, 2048]               0\n",
      "          Linear-161             [16, 576, 512]       1,049,088\n",
      "         Dropout-162             [16, 576, 512]               0\n",
      "             Mlp-163             [16, 576, 512]               0\n",
      "        DropPath-164             [16, 576, 512]               0\n",
      "SwinTransformerBlock-165             [16, 576, 512]               0\n",
      "       LayerNorm-166             [16, 576, 512]           1,024\n",
      "          Linear-167            [16, 144, 1536]         787,968\n",
      "         Softmax-168         [16, 16, 144, 144]               0\n",
      "         Dropout-169         [16, 16, 144, 144]               0\n",
      "          Linear-170             [16, 144, 512]         262,656\n",
      "         Dropout-171             [16, 144, 512]               0\n",
      " WindowAttention-172             [16, 144, 512]               0\n",
      "        DropPath-173             [16, 576, 512]               0\n",
      "       LayerNorm-174             [16, 576, 512]           1,024\n",
      "          Linear-175            [16, 576, 2048]       1,050,624\n",
      "            GELU-176            [16, 576, 2048]               0\n",
      "         Dropout-177            [16, 576, 2048]               0\n",
      "          Linear-178             [16, 576, 512]       1,049,088\n",
      "         Dropout-179             [16, 576, 512]               0\n",
      "             Mlp-180             [16, 576, 512]               0\n",
      "        DropPath-181             [16, 576, 512]               0\n",
      "SwinTransformerBlock-182             [16, 576, 512]               0\n",
      "       LayerNorm-183             [16, 576, 512]           1,024\n",
      "          Linear-184            [16, 144, 1536]         787,968\n",
      "         Softmax-185         [16, 16, 144, 144]               0\n",
      "         Dropout-186         [16, 16, 144, 144]               0\n",
      "          Linear-187             [16, 144, 512]         262,656\n",
      "         Dropout-188             [16, 144, 512]               0\n",
      " WindowAttention-189             [16, 144, 512]               0\n",
      "        DropPath-190             [16, 576, 512]               0\n",
      "       LayerNorm-191             [16, 576, 512]           1,024\n",
      "          Linear-192            [16, 576, 2048]       1,050,624\n",
      "            GELU-193            [16, 576, 2048]               0\n",
      "         Dropout-194            [16, 576, 2048]               0\n",
      "          Linear-195             [16, 576, 512]       1,049,088\n",
      "         Dropout-196             [16, 576, 512]               0\n",
      "             Mlp-197             [16, 576, 512]               0\n",
      "        DropPath-198             [16, 576, 512]               0\n",
      "SwinTransformerBlock-199             [16, 576, 512]               0\n",
      "       LayerNorm-200             [16, 576, 512]           1,024\n",
      "          Linear-201            [16, 144, 1536]         787,968\n",
      "         Softmax-202         [16, 16, 144, 144]               0\n",
      "         Dropout-203         [16, 16, 144, 144]               0\n",
      "          Linear-204             [16, 144, 512]         262,656\n",
      "         Dropout-205             [16, 144, 512]               0\n",
      " WindowAttention-206             [16, 144, 512]               0\n",
      "        DropPath-207             [16, 576, 512]               0\n",
      "       LayerNorm-208             [16, 576, 512]           1,024\n",
      "          Linear-209            [16, 576, 2048]       1,050,624\n",
      "            GELU-210            [16, 576, 2048]               0\n",
      "         Dropout-211            [16, 576, 2048]               0\n",
      "          Linear-212             [16, 576, 512]       1,049,088\n",
      "         Dropout-213             [16, 576, 512]               0\n",
      "             Mlp-214             [16, 576, 512]               0\n",
      "        DropPath-215             [16, 576, 512]               0\n",
      "SwinTransformerBlock-216             [16, 576, 512]               0\n",
      "       LayerNorm-217             [16, 576, 512]           1,024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Linear-218            [16, 144, 1536]         787,968\n",
      "         Softmax-219         [16, 16, 144, 144]               0\n",
      "         Dropout-220         [16, 16, 144, 144]               0\n",
      "          Linear-221             [16, 144, 512]         262,656\n",
      "         Dropout-222             [16, 144, 512]               0\n",
      " WindowAttention-223             [16, 144, 512]               0\n",
      "        DropPath-224             [16, 576, 512]               0\n",
      "       LayerNorm-225             [16, 576, 512]           1,024\n",
      "          Linear-226            [16, 576, 2048]       1,050,624\n",
      "            GELU-227            [16, 576, 2048]               0\n",
      "         Dropout-228            [16, 576, 2048]               0\n",
      "          Linear-229             [16, 576, 512]       1,049,088\n",
      "         Dropout-230             [16, 576, 512]               0\n",
      "             Mlp-231             [16, 576, 512]               0\n",
      "        DropPath-232             [16, 576, 512]               0\n",
      "SwinTransformerBlock-233             [16, 576, 512]               0\n",
      "       LayerNorm-234             [16, 576, 512]           1,024\n",
      "          Linear-235            [16, 144, 1536]         787,968\n",
      "         Softmax-236         [16, 16, 144, 144]               0\n",
      "         Dropout-237         [16, 16, 144, 144]               0\n",
      "          Linear-238             [16, 144, 512]         262,656\n",
      "         Dropout-239             [16, 144, 512]               0\n",
      " WindowAttention-240             [16, 144, 512]               0\n",
      "        DropPath-241             [16, 576, 512]               0\n",
      "       LayerNorm-242             [16, 576, 512]           1,024\n",
      "          Linear-243            [16, 576, 2048]       1,050,624\n",
      "            GELU-244            [16, 576, 2048]               0\n",
      "         Dropout-245            [16, 576, 2048]               0\n",
      "          Linear-246             [16, 576, 512]       1,049,088\n",
      "         Dropout-247             [16, 576, 512]               0\n",
      "             Mlp-248             [16, 576, 512]               0\n",
      "        DropPath-249             [16, 576, 512]               0\n",
      "SwinTransformerBlock-250             [16, 576, 512]               0\n",
      "       LayerNorm-251             [16, 576, 512]           1,024\n",
      "          Linear-252            [16, 144, 1536]         787,968\n",
      "         Softmax-253         [16, 16, 144, 144]               0\n",
      "         Dropout-254         [16, 16, 144, 144]               0\n",
      "          Linear-255             [16, 144, 512]         262,656\n",
      "         Dropout-256             [16, 144, 512]               0\n",
      " WindowAttention-257             [16, 144, 512]               0\n",
      "        DropPath-258             [16, 576, 512]               0\n",
      "       LayerNorm-259             [16, 576, 512]           1,024\n",
      "          Linear-260            [16, 576, 2048]       1,050,624\n",
      "            GELU-261            [16, 576, 2048]               0\n",
      "         Dropout-262            [16, 576, 2048]               0\n",
      "          Linear-263             [16, 576, 512]       1,049,088\n",
      "         Dropout-264             [16, 576, 512]               0\n",
      "             Mlp-265             [16, 576, 512]               0\n",
      "        DropPath-266             [16, 576, 512]               0\n",
      "SwinTransformerBlock-267             [16, 576, 512]               0\n",
      "       LayerNorm-268             [16, 576, 512]           1,024\n",
      "          Linear-269            [16, 144, 1536]         787,968\n",
      "         Softmax-270         [16, 16, 144, 144]               0\n",
      "         Dropout-271         [16, 16, 144, 144]               0\n",
      "          Linear-272             [16, 144, 512]         262,656\n",
      "         Dropout-273             [16, 144, 512]               0\n",
      " WindowAttention-274             [16, 144, 512]               0\n",
      "        DropPath-275             [16, 576, 512]               0\n",
      "       LayerNorm-276             [16, 576, 512]           1,024\n",
      "          Linear-277            [16, 576, 2048]       1,050,624\n",
      "            GELU-278            [16, 576, 2048]               0\n",
      "         Dropout-279            [16, 576, 2048]               0\n",
      "          Linear-280             [16, 576, 512]       1,049,088\n",
      "         Dropout-281             [16, 576, 512]               0\n",
      "             Mlp-282             [16, 576, 512]               0\n",
      "        DropPath-283             [16, 576, 512]               0\n",
      "SwinTransformerBlock-284             [16, 576, 512]               0\n",
      "       LayerNorm-285             [16, 576, 512]           1,024\n",
      "          Linear-286            [16, 144, 1536]         787,968\n",
      "         Softmax-287         [16, 16, 144, 144]               0\n",
      "         Dropout-288         [16, 16, 144, 144]               0\n",
      "          Linear-289             [16, 144, 512]         262,656\n",
      "         Dropout-290             [16, 144, 512]               0\n",
      " WindowAttention-291             [16, 144, 512]               0\n",
      "        DropPath-292             [16, 576, 512]               0\n",
      "       LayerNorm-293             [16, 576, 512]           1,024\n",
      "          Linear-294            [16, 576, 2048]       1,050,624\n",
      "            GELU-295            [16, 576, 2048]               0\n",
      "         Dropout-296            [16, 576, 2048]               0\n",
      "          Linear-297             [16, 576, 512]       1,049,088\n",
      "         Dropout-298             [16, 576, 512]               0\n",
      "             Mlp-299             [16, 576, 512]               0\n",
      "        DropPath-300             [16, 576, 512]               0\n",
      "SwinTransformerBlock-301             [16, 576, 512]               0\n",
      "       LayerNorm-302             [16, 576, 512]           1,024\n",
      "          Linear-303            [16, 144, 1536]         787,968\n",
      "         Softmax-304         [16, 16, 144, 144]               0\n",
      "         Dropout-305         [16, 16, 144, 144]               0\n",
      "          Linear-306             [16, 144, 512]         262,656\n",
      "         Dropout-307             [16, 144, 512]               0\n",
      " WindowAttention-308             [16, 144, 512]               0\n",
      "        DropPath-309             [16, 576, 512]               0\n",
      "       LayerNorm-310             [16, 576, 512]           1,024\n",
      "          Linear-311            [16, 576, 2048]       1,050,624\n",
      "            GELU-312            [16, 576, 2048]               0\n",
      "         Dropout-313            [16, 576, 2048]               0\n",
      "          Linear-314             [16, 576, 512]       1,049,088\n",
      "         Dropout-315             [16, 576, 512]               0\n",
      "             Mlp-316             [16, 576, 512]               0\n",
      "        DropPath-317             [16, 576, 512]               0\n",
      "SwinTransformerBlock-318             [16, 576, 512]               0\n",
      "       LayerNorm-319             [16, 576, 512]           1,024\n",
      "          Linear-320            [16, 144, 1536]         787,968\n",
      "         Softmax-321         [16, 16, 144, 144]               0\n",
      "         Dropout-322         [16, 16, 144, 144]               0\n",
      "          Linear-323             [16, 144, 512]         262,656\n",
      "         Dropout-324             [16, 144, 512]               0\n",
      " WindowAttention-325             [16, 144, 512]               0\n",
      "        DropPath-326             [16, 576, 512]               0\n",
      "       LayerNorm-327             [16, 576, 512]           1,024\n",
      "          Linear-328            [16, 576, 2048]       1,050,624\n",
      "            GELU-329            [16, 576, 2048]               0\n",
      "         Dropout-330            [16, 576, 2048]               0\n",
      "          Linear-331             [16, 576, 512]       1,049,088\n",
      "         Dropout-332             [16, 576, 512]               0\n",
      "             Mlp-333             [16, 576, 512]               0\n",
      "        DropPath-334             [16, 576, 512]               0\n",
      "SwinTransformerBlock-335             [16, 576, 512]               0\n",
      "       LayerNorm-336             [16, 576, 512]           1,024\n",
      "          Linear-337            [16, 144, 1536]         787,968\n",
      "         Softmax-338         [16, 16, 144, 144]               0\n",
      "         Dropout-339         [16, 16, 144, 144]               0\n",
      "          Linear-340             [16, 144, 512]         262,656\n",
      "         Dropout-341             [16, 144, 512]               0\n",
      " WindowAttention-342             [16, 144, 512]               0\n",
      "        DropPath-343             [16, 576, 512]               0\n",
      "       LayerNorm-344             [16, 576, 512]           1,024\n",
      "          Linear-345            [16, 576, 2048]       1,050,624\n",
      "            GELU-346            [16, 576, 2048]               0\n",
      "         Dropout-347            [16, 576, 2048]               0\n",
      "          Linear-348             [16, 576, 512]       1,049,088\n",
      "         Dropout-349             [16, 576, 512]               0\n",
      "             Mlp-350             [16, 576, 512]               0\n",
      "        DropPath-351             [16, 576, 512]               0\n",
      "SwinTransformerBlock-352             [16, 576, 512]               0\n",
      "       LayerNorm-353             [16, 576, 512]           1,024\n",
      "          Linear-354            [16, 144, 1536]         787,968\n",
      "         Softmax-355         [16, 16, 144, 144]               0\n",
      "         Dropout-356         [16, 16, 144, 144]               0\n",
      "          Linear-357             [16, 144, 512]         262,656\n",
      "         Dropout-358             [16, 144, 512]               0\n",
      " WindowAttention-359             [16, 144, 512]               0\n",
      "        DropPath-360             [16, 576, 512]               0\n",
      "       LayerNorm-361             [16, 576, 512]           1,024\n",
      "          Linear-362            [16, 576, 2048]       1,050,624\n",
      "            GELU-363            [16, 576, 2048]               0\n",
      "         Dropout-364            [16, 576, 2048]               0\n",
      "          Linear-365             [16, 576, 512]       1,049,088\n",
      "         Dropout-366             [16, 576, 512]               0\n",
      "             Mlp-367             [16, 576, 512]               0\n",
      "        DropPath-368             [16, 576, 512]               0\n",
      "SwinTransformerBlock-369             [16, 576, 512]               0\n",
      "       LayerNorm-370             [16, 576, 512]           1,024\n",
      "          Linear-371            [16, 144, 1536]         787,968\n",
      "         Softmax-372         [16, 16, 144, 144]               0\n",
      "         Dropout-373         [16, 16, 144, 144]               0\n",
      "          Linear-374             [16, 144, 512]         262,656\n",
      "         Dropout-375             [16, 144, 512]               0\n",
      " WindowAttention-376             [16, 144, 512]               0\n",
      "        DropPath-377             [16, 576, 512]               0\n",
      "       LayerNorm-378             [16, 576, 512]           1,024\n",
      "          Linear-379            [16, 576, 2048]       1,050,624\n",
      "            GELU-380            [16, 576, 2048]               0\n",
      "         Dropout-381            [16, 576, 2048]               0\n",
      "          Linear-382             [16, 576, 512]       1,049,088\n",
      "         Dropout-383             [16, 576, 512]               0\n",
      "             Mlp-384             [16, 576, 512]               0\n",
      "        DropPath-385             [16, 576, 512]               0\n",
      "SwinTransformerBlock-386             [16, 576, 512]               0\n",
      "       LayerNorm-387            [16, 144, 2048]           4,096\n",
      "          Linear-388            [16, 144, 1024]       2,097,152\n",
      "    PatchMerging-389            [16, 144, 1024]               0\n",
      "      BasicLayer-390            [16, 144, 1024]               0\n",
      "       LayerNorm-391            [16, 144, 1024]           2,048\n",
      "          Linear-392            [16, 144, 3072]       3,148,800\n",
      "         Softmax-393         [16, 32, 144, 144]               0\n",
      "         Dropout-394         [16, 32, 144, 144]               0\n",
      "          Linear-395            [16, 144, 1024]       1,049,600\n",
      "         Dropout-396            [16, 144, 1024]               0\n",
      " WindowAttention-397            [16, 144, 1024]               0\n",
      "        DropPath-398            [16, 144, 1024]               0\n",
      "       LayerNorm-399            [16, 144, 1024]           2,048\n",
      "          Linear-400            [16, 144, 4096]       4,198,400\n",
      "            GELU-401            [16, 144, 4096]               0\n",
      "         Dropout-402            [16, 144, 4096]               0\n",
      "          Linear-403            [16, 144, 1024]       4,195,328\n",
      "         Dropout-404            [16, 144, 1024]               0\n",
      "             Mlp-405            [16, 144, 1024]               0\n",
      "        DropPath-406            [16, 144, 1024]               0\n",
      "SwinTransformerBlock-407            [16, 144, 1024]               0\n",
      "       LayerNorm-408            [16, 144, 1024]           2,048\n",
      "          Linear-409            [16, 144, 3072]       3,148,800\n",
      "         Softmax-410         [16, 32, 144, 144]               0\n",
      "         Dropout-411         [16, 32, 144, 144]               0\n",
      "          Linear-412            [16, 144, 1024]       1,049,600\n",
      "         Dropout-413            [16, 144, 1024]               0\n",
      " WindowAttention-414            [16, 144, 1024]               0\n",
      "        DropPath-415            [16, 144, 1024]               0\n",
      "       LayerNorm-416            [16, 144, 1024]           2,048\n",
      "          Linear-417            [16, 144, 4096]       4,198,400\n",
      "            GELU-418            [16, 144, 4096]               0\n",
      "         Dropout-419            [16, 144, 4096]               0\n",
      "          Linear-420            [16, 144, 1024]       4,195,328\n",
      "         Dropout-421            [16, 144, 1024]               0\n",
      "             Mlp-422            [16, 144, 1024]               0\n",
      "        DropPath-423            [16, 144, 1024]               0\n",
      "SwinTransformerBlock-424            [16, 144, 1024]               0\n",
      "      BasicLayer-425            [16, 144, 1024]               0\n",
      "       LayerNorm-426            [16, 144, 1024]           2,048\n",
      "AdaptiveAvgPool1d-427              [16, 1024, 1]               0\n",
      "          Linear-428                  [16, 200]         205,000\n",
      "================================================================\n",
      "Total params: 86,884,680\n",
      "Trainable params: 86,884,680\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 27.00\n",
      "Forward/backward pass size (MB): 26734.65\n",
      "Params size (MB): 331.44\n",
      "Estimated Total Size (MB): 27093.09\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "trainer.update_model_from_checkpoint(checkpoint_file=local_cache_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------\n",
      "Running a comparison of PyTorch and nVidia Triton Inference Server inference results.\n",
      "Model Name:: SwinTBase \n",
      "Model Endpoint:: cub200_swinbase\n",
      "------------------------------------------------------------------------------------------\n",
      "Result:: \ty\t\t:: [144  57] \n",
      "\t \ty_pred[local]\t:: [146  57] \n",
      "\t \ty_pred[triton]\t:: [146  57] \n",
      "\n",
      "Result:: \ty\t\t:: [11 89] \n",
      "\t \ty_pred[local]\t:: [11 89] \n",
      "\t \ty_pred[triton]\t:: [11 89] \n",
      "\n",
      "Result:: \ty\t\t:: [166  45] \n",
      "\t \ty_pred[local]\t:: [166  45] \n",
      "\t \ty_pred[triton]\t:: [166  45] \n",
      "\n",
      "Result:: \ty\t\t:: [ 90 107] \n",
      "\t \ty_pred[local]\t:: [ 90 107] \n",
      "\t \ty_pred[triton]\t:: [ 90 107] \n",
      "\n",
      "Result:: \ty\t\t:: [116  89] \n",
      "\t \ty_pred[local]\t:: [131  89] \n",
      "\t \ty_pred[triton]\t:: [131  89] \n",
      "\n",
      "Result:: \ty\t\t:: [144  53] \n",
      "\t \ty_pred[local]\t:: [144  53] \n",
      "\t \ty_pred[triton]\t:: [144  53] \n",
      "\n",
      "Result:: \ty\t\t:: [114 130] \n",
      "\t \ty_pred[local]\t:: [114 120] \n",
      "\t \ty_pred[triton]\t:: [114 120] \n",
      "\n",
      "Result:: \ty\t\t:: [30 38] \n",
      "\t \ty_pred[local]\t:: [30 36] \n",
      "\t \ty_pred[triton]\t:: [30 36] \n",
      "\n",
      "Result:: \ty\t\t:: [176 106] \n",
      "\t \ty_pred[local]\t:: [176 106] \n",
      "\t \ty_pred[triton]\t:: [176 106] \n",
      "\n",
      "Result:: \ty\t\t:: [ 4 26] \n",
      "\t \ty_pred[local]\t:: [ 4 26] \n",
      "\t \ty_pred[triton]\t:: [ 4 26] \n",
      "\n",
      "Result:: \ty\t\t:: [68  1] \n",
      "\t \ty_pred[local]\t:: [68  1] \n",
      "\t \ty_pred[triton]\t:: [68  1] \n",
      "\n",
      "Result:: \ty\t\t:: [184 161] \n",
      "\t \ty_pred[local]\t:: [184 171] \n",
      "\t \ty_pred[triton]\t:: [184 171] \n",
      "\n",
      "Result:: \ty\t\t:: [37 84] \n",
      "\t \ty_pred[local]\t:: [37 84] \n",
      "\t \ty_pred[triton]\t:: [37 84] \n",
      "\n",
      "Result:: \ty\t\t:: [93 57] \n",
      "\t \ty_pred[local]\t:: [93 57] \n",
      "\t \ty_pred[triton]\t:: [93 57] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('-'*90)\n",
    "print('Running a comparison of PyTorch and nVidia Triton Inference Server inference results.')\n",
    "print('Model Name:: {} \\nModel Endpoint:: {}'.format(model_name, endpoint_name))\n",
    "print('-'*90)\n",
    "for i in np.arange(1,n_tests,1):\n",
    "\n",
    "    # Get a validation batch\n",
    "    X, y = next(iter(trainer.val_loader))\n",
    "    # Set the model into eval mode\n",
    "    trainer.model.eval()\n",
    "    # Push input images to gpu\n",
    "    X_gpu = X.to(trainer.device)\n",
    "    # Run inference on validatgion batch image\n",
    "    y_prob_pred = trainer.model(X_gpu)\n",
    "    # Get predicted classes\n",
    "    _, y_pred = torch.max(y_prob_pred, 1)\n",
    "\n",
    "    # Get Triton served predicted classes\n",
    "    y_pred_proba_remote, y_pred_remote = run_inference(X.numpy(), X.shape, model_name=endpoint_name, VERBOSE=False)\n",
    "    \n",
    "    \n",
    "    print('Result:: \\ty\\t\\t:: {} \\n\\t \\ty_pred[local]\\t:: {} \\n\\t \\ty_pred[triton]\\t:: {} '.format(y.numpy(),y_pred.cpu().numpy(),y_pred_remote))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 Triton Client for PyTorch Ignite ClearML",
   "language": "python",
   "name": "py38_pytorch181_cu111_timm_triton"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
