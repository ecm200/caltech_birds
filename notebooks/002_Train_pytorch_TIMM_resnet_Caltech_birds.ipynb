{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caltech UCSD Birds 200 2011 (CUB-200-2011)\n",
    "\n",
    "## Part 3 - Training a PyTorch ResNet152 convolutional neural network (CNN) using Torchvision pre-trained network on ImageNet database.\n",
    "\n",
    "This notebook shows how to prepare datasets for training a Convolutional Neural Network using the CUB-200-2011 database of birds.\n",
    "\n",
    "The previous notebooks sorted the data into an organised filing system for use with Torchvision data loaders and investigated the type of data and class sampling densities of the training and test sets.\n",
    "\n",
    "This is the third stage of our roadmap for building and understanding a birds image classifier:\n",
    "\n",
    "![RoadMapImage](../docs/birds_roadmap.png)\n",
    "\n",
    "This notebook is now going to show: \n",
    "\n",
    "    1. How to set up a dataloader using an augmentation workflow (as in Part 1) for use in providing images to our network for training.\n",
    "    \n",
    "    2. How to load a common network architecture, such as ResNet152, and load pretrained weights into the netowrk, using the models supplied in the PyTorch Torchvision module.\n",
    "    \n",
    "    3. How to manipulate the network structure to re-engineer the ouput classifier layer so that it suits the purposes of our problem.\n",
    "    \n",
    "    4. How to set up the required objects that are needed for training a neural network, in addition to the neural network model object itself. This includes:\n",
    "        a. Loss function.\n",
    "        b. Optimizer.\n",
    "        c. Learning rate scheduler.\n",
    "        d. training function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook setup\n",
    "\n",
    "### Modules and externals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets, models\n",
    "\n",
    "import timm\n",
    "\n",
    "from imutils import paths\n",
    "from pathlib import Path\n",
    "import os\n",
    "import time\n",
    "import copy\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "\n",
    "# Local modules\n",
    "from cub_tools.train import train_model\n",
    "from cub_tools.visualize import imshow, visualize_model\n",
    "from cub_tools.utils import save_model_dict, save_model_full, unpickle\n",
    "from cub_tools.transforms import makeDefaultTransforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Runtime setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Script runtime options\n",
    "model_name = 'vit_base'\n",
    "model_func = models.resnet152\n",
    "root_dir = '../data'\n",
    "data_dir = os.path.join(root_dir,'images')\n",
    "working_dir = os.path.join('../models/classification', model_name)\n",
    "batch_size = 16\n",
    "num_workers = 4\n",
    "num_epochs = 40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataset loaders\n",
    "\n",
    "As with the visualisation section (001 Notebook), we need to create a loader for importing the images sets for both training and testing/validation purposes. As with all algorithm development and training, we must be careful to separate our data into separate training and test sets, such that we can gain a (hopefully) unbiased assessment of the model performance by testing the predictive power of the model with known images of birds. These images must not have been using the training process, as any images that will be presented to network during deployment will not have been seen by the network before. We want to assess what the performance of the network will be when it is presented with images of the same class, but not the same exact image, that it has been trained on. Only then can we get a fair assessment of the likely performance of the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(working_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the augmention transforms\n",
    "\n",
    "Data augmentation, a process we looked at in the previous notebook, is a process which applies random translations to the input images during each Epoch of training, such that every time the network sees each image, it is not the exact same representation of that image. These transforms are loaded from a preset function, which determines both the train and validation transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data transforms\n",
    "data_transforms = makeDefaultTransforms()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we create the dataset and dataloader objects by using the Torchvision dataset tools.\n",
    "\n",
    "As we have organised our image files into train and test directories already, we simply give the path to the dataset object function datasets.ImageFolder of Torchvision and it creates a Torch dataset object, with all the images. The images have also been arranged into sub-directories of image class, the function attributes the class label of the image from the directory name. The image dataset object also carries with the loading process and any augmentation transforms that are to be applied to the training and test data.\n",
    "\n",
    "The dataset objects can be used to be turned into batching image loaders, which can be used as the process which controls the inputs that are loaded into the neural network during training and testing. The **batch size** determines the number of images that are served up to the network in a **mini batch**, an iteration. The number of mini batches is determined by the number of images, and is divided by the batch size. A epoch, a complete iteraton, pertains to one complete training cycle over all images in the dataset. A network may take 50 to 100 epochs to successfully train to adequate enough accuracies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup data loaders with augmentation transforms\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x])\n",
    "                  for x in ['train', 'test']}\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size,\n",
    "                                             shuffle=True, num_workers=num_workers)\n",
    "              for x in ['train', 'test']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(dataloaders['train']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, labels = batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-1.0390, -1.1075, -1.1075,  ..., -0.1314, -0.1143, -0.1143],\n",
       "          [-0.3027, -0.7308, -1.0904,  ..., -0.1314, -0.0972, -0.1143],\n",
       "          [-0.0458, -0.5424, -1.1247,  ..., -0.1999, -0.1657, -0.1657],\n",
       "          ...,\n",
       "          [ 0.7762,  0.7933,  0.7933,  ..., -0.8335, -0.5938, -0.3198],\n",
       "          [ 0.7933,  0.7762,  0.7591,  ..., -0.9192, -0.7137, -0.4054],\n",
       "          [ 0.7591,  0.7591,  0.7419,  ..., -0.9877, -0.7993, -0.5596]],\n",
       "\n",
       "         [[-1.1078, -1.2304, -1.1954,  ..., -0.0749, -0.0749, -0.0399],\n",
       "          [-0.4251, -0.9678, -1.2654,  ..., -0.1099, -0.0749, -0.0749],\n",
       "          [-0.1450, -0.6176, -1.1954,  ..., -0.0924, -0.0749, -0.0574],\n",
       "          ...,\n",
       "          [ 0.8179,  0.8179,  0.8354,  ..., -0.8452, -0.5651, -0.0749],\n",
       "          [ 0.8179,  0.8179,  0.8004,  ..., -0.9153, -0.7402, -0.2150],\n",
       "          [ 0.8004,  0.7829,  0.7829,  ..., -0.9153, -0.7402, -0.3375]],\n",
       "\n",
       "         [[-1.0724, -1.0898, -0.9853,  ..., -0.2881, -0.2358, -0.2358],\n",
       "          [-0.7064, -0.9504, -1.1247,  ..., -0.2707, -0.2358, -0.2358],\n",
       "          [-0.4101, -0.6715, -1.1073,  ..., -0.2707, -0.2532, -0.2358],\n",
       "          ...,\n",
       "          [ 0.6879,  0.6879,  0.6879,  ..., -0.6367, -0.3055,  0.1999],\n",
       "          [ 0.6705,  0.6531,  0.6356,  ..., -0.6890, -0.4275,  0.0605],\n",
       "          [ 0.6356,  0.6531,  0.6531,  ..., -0.7936, -0.5670, -0.2010]]],\n",
       "\n",
       "\n",
       "        [[[ 0.3652,  0.3481,  0.3652,  ...,  1.6153,  1.6153,  1.5982],\n",
       "          [ 0.3481,  0.3309,  0.3481,  ...,  1.6495,  1.6495,  1.6495],\n",
       "          [ 0.3309,  0.3481,  0.3481,  ...,  1.6324,  1.6495,  1.6667],\n",
       "          ...,\n",
       "          [-0.5767, -0.5596, -0.5596,  ...,  0.5878,  0.5707,  0.5022],\n",
       "          [-0.5596, -0.5424, -0.5253,  ...,  0.4851,  0.5193,  0.5364],\n",
       "          [-0.5596, -0.5424, -0.5082,  ...,  0.3994,  0.4508,  0.4679]],\n",
       "\n",
       "         [[ 0.3102,  0.2927,  0.3627,  ...,  1.4132,  1.4132,  1.4307],\n",
       "          [ 0.2927,  0.2752,  0.3452,  ...,  1.4482,  1.4482,  1.4657],\n",
       "          [ 0.2752,  0.2927,  0.3452,  ...,  1.4307,  1.4482,  1.4657],\n",
       "          ...,\n",
       "          [-0.7052, -0.7052, -0.6877,  ...,  0.7479,  0.7129,  0.6429],\n",
       "          [-0.7227, -0.7227, -0.6877,  ...,  0.7129,  0.7479,  0.7654],\n",
       "          [-0.7402, -0.7227, -0.6877,  ...,  0.6779,  0.7304,  0.7479]],\n",
       "\n",
       "         [[ 0.4962,  0.4788,  0.5311,  ...,  1.2980,  1.2980,  1.2980],\n",
       "          [ 0.4788,  0.4614,  0.5136,  ...,  1.3328,  1.3328,  1.3328],\n",
       "          [ 0.4614,  0.4788,  0.5136,  ...,  1.3154,  1.3328,  1.3502],\n",
       "          ...,\n",
       "          [-0.6715, -0.6541, -0.6541,  ...,  0.4091,  0.4265,  0.3916],\n",
       "          [-0.6715, -0.6715, -0.6367,  ...,  0.2348,  0.3045,  0.3568],\n",
       "          [-0.6890, -0.6715, -0.6715,  ...,  0.1128,  0.1651,  0.1999]]],\n",
       "\n",
       "\n",
       "        [[[-1.2103, -1.1418, -1.1075,  ..., -0.2684, -0.2856, -0.3198],\n",
       "          [-1.2445, -1.2103, -1.1075,  ..., -0.2684, -0.2684, -0.3027],\n",
       "          [-1.2103, -1.1760, -1.1075,  ..., -0.2684, -0.2684, -0.3198],\n",
       "          ...,\n",
       "          [-1.0562, -1.0904, -1.1075,  ..., -1.3644, -1.3473, -1.3130],\n",
       "          [-1.0904, -1.1075, -1.1418,  ..., -1.3815, -1.3644, -1.3130],\n",
       "          [-1.0904, -1.1247, -1.1589,  ..., -1.3815, -1.3815, -1.3302]],\n",
       "\n",
       "         [[-0.4776, -0.4601, -0.4601,  ..., -0.4251, -0.4426, -0.4426],\n",
       "          [-0.4426, -0.4251, -0.4426,  ..., -0.4251, -0.4601, -0.4251],\n",
       "          [-0.4076, -0.3901, -0.4076,  ..., -0.4426, -0.4426, -0.4426],\n",
       "          ...,\n",
       "          [-0.7577, -0.7752, -0.7927,  ..., -0.7752, -0.7752, -0.8102],\n",
       "          [-0.7752, -0.8102, -0.8452,  ..., -0.7752, -0.7927, -0.7752],\n",
       "          [-0.7927, -0.8277, -0.8627,  ..., -0.7752, -0.7752, -0.7577]],\n",
       "\n",
       "         [[-1.0550, -1.0201, -1.0027,  ..., -0.6541, -0.6541, -0.6890],\n",
       "          [-1.0201, -0.9678, -0.9504,  ..., -0.6367, -0.6367, -0.6715],\n",
       "          [-0.9853, -0.9853, -0.9678,  ..., -0.6715, -0.6890, -0.6890],\n",
       "          ...,\n",
       "          [-0.7064, -0.7238, -0.7413,  ..., -0.8807, -0.8284, -0.8284],\n",
       "          [-0.7238, -0.7413, -0.7761,  ..., -0.8981, -0.8633, -0.8110],\n",
       "          [-0.7064, -0.7413, -0.7761,  ..., -0.8981, -0.8633, -0.7936]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[ 0.1768,  0.1768,  0.1768,  ...,  2.0092,  2.0777,  2.1119],\n",
       "          [ 0.1768,  0.2111,  0.2111,  ...,  2.0777,  2.1119,  2.1462],\n",
       "          [ 0.2282,  0.2282,  0.2282,  ...,  2.1290,  2.1633,  2.1633],\n",
       "          ...,\n",
       "          [ 2.2147,  2.1633,  2.1290,  ...,  0.7591,  0.8618,  0.8789],\n",
       "          [ 2.2489,  2.2147,  2.1462,  ...,  0.7077,  0.4337,  0.8961],\n",
       "          [ 2.1119,  2.1975,  2.1975,  ...,  0.7933,  0.4508,  0.7419]],\n",
       "\n",
       "         [[ 0.2927,  0.3277,  0.3277,  ...,  2.1835,  2.2535,  2.2885],\n",
       "          [ 0.2927,  0.3277,  0.3277,  ...,  2.2360,  2.2710,  2.3060],\n",
       "          [ 0.3277,  0.3277,  0.3102,  ...,  2.2885,  2.3235,  2.3235],\n",
       "          ...,\n",
       "          [ 2.3410,  2.3235,  2.2885,  ...,  0.8004,  0.8880,  0.8880],\n",
       "          [ 2.3761,  2.3761,  2.3060,  ...,  0.7304,  0.4328,  0.8880],\n",
       "          [ 2.2535,  2.3761,  2.3585,  ...,  0.8354,  0.4678,  0.7479]],\n",
       "\n",
       "         [[ 0.4265,  0.4439,  0.4439,  ...,  2.2566,  2.3263,  2.3611],\n",
       "          [ 0.4265,  0.4614,  0.4614,  ...,  2.3437,  2.3960,  2.4134],\n",
       "          [ 0.4614,  0.4614,  0.4439,  ...,  2.4134,  2.4483,  2.4657],\n",
       "          ...,\n",
       "          [ 2.5006,  2.4483,  2.4134,  ...,  0.9145,  0.9842,  0.9494],\n",
       "          [ 2.5006,  2.5006,  2.4483,  ...,  0.8622,  0.5311,  0.9319],\n",
       "          [ 2.3786,  2.5006,  2.5006,  ...,  0.9842,  0.5834,  0.8099]]],\n",
       "\n",
       "\n",
       "        [[[-1.3815, -1.4500, -1.4500,  ..., -1.0048, -1.0048, -1.0219],\n",
       "          [-1.3815, -1.4329, -1.4329,  ..., -1.0219, -1.0219, -1.0390],\n",
       "          [-1.3987, -1.4158, -1.4158,  ..., -1.0219, -1.0390, -1.0390],\n",
       "          ...,\n",
       "          [ 1.1187,  1.0844,  1.1529,  ..., -0.2513, -0.4397, -0.5424],\n",
       "          [ 1.0844,  1.1529,  1.1187,  ..., -0.4397, -0.4739, -0.6452],\n",
       "          [ 1.0331,  1.1529,  1.1358,  ..., -0.6109, -0.5938, -0.5082]],\n",
       "\n",
       "         [[-1.3354, -1.3529, -1.3529,  ..., -0.8277, -0.8277, -0.8452],\n",
       "          [-1.3354, -1.3354, -1.3354,  ..., -0.8452, -0.8452, -0.8627],\n",
       "          [-1.3529, -1.3179, -1.3179,  ..., -0.8452, -0.8627, -0.8627],\n",
       "          ...,\n",
       "          [ 0.2402,  0.2052,  0.2752,  ...,  0.0301, -0.1975, -0.3025],\n",
       "          [ 0.1702,  0.2752,  0.2402,  ..., -0.1450, -0.1975, -0.3550],\n",
       "          [ 0.0826,  0.2577,  0.2402,  ..., -0.3375, -0.3025, -0.2150]],\n",
       "\n",
       "         [[-1.2293, -1.2641, -1.2641,  ..., -0.8981, -0.8981, -0.9156],\n",
       "          [-1.2293, -1.2467, -1.2467,  ..., -0.9156, -0.9156, -0.9330],\n",
       "          [-1.2467, -1.2293, -1.2293,  ..., -0.9156, -0.9330, -0.9330],\n",
       "          ...,\n",
       "          [-0.1487, -0.2707, -0.2010,  ..., -0.8633, -1.1247, -1.2990],\n",
       "          [-0.2184, -0.1138, -0.1487,  ..., -1.1247, -1.2816, -1.4907],\n",
       "          [-0.3055, -0.1138, -0.1312,  ..., -1.2816, -1.3164, -1.2293]]],\n",
       "\n",
       "\n",
       "        [[[-0.3369, -0.3369, -0.3883,  ..., -0.3712, -0.3541, -0.3369],\n",
       "          [-0.3198, -0.3369, -0.3369,  ..., -0.3712, -0.3541, -0.3541],\n",
       "          [-0.3027, -0.3369, -0.2856,  ..., -0.3712, -0.3541, -0.3369],\n",
       "          ...,\n",
       "          [-0.2684, -0.2171, -0.2856,  ...,  0.7762,  0.7762,  0.6563],\n",
       "          [-0.2856, -0.2513, -0.2513,  ..., -0.2342, -0.2684, -0.3712],\n",
       "          [-0.3198, -0.3198, -0.2171,  ..., -0.6794, -0.5082, -0.5082]],\n",
       "\n",
       "         [[ 0.5903,  0.5903,  0.5378,  ...,  0.5553,  0.5553,  0.5903],\n",
       "          [ 0.6078,  0.5903,  0.5378,  ...,  0.5553,  0.5553,  0.5728],\n",
       "          [ 0.5903,  0.5903,  0.5903,  ...,  0.5553,  0.5553,  0.5903],\n",
       "          ...,\n",
       "          [ 0.5728,  0.6254,  0.5903,  ...,  0.8529,  0.8529,  0.7829],\n",
       "          [ 0.5903,  0.6078,  0.6254,  ..., -0.1800, -0.1975, -0.2675],\n",
       "          [ 0.6078,  0.6078,  0.6254,  ..., -0.5826, -0.4076, -0.4076]],\n",
       "\n",
       "         [[ 1.7163,  1.7163,  1.6640,  ...,  1.6814,  1.7337,  1.7163],\n",
       "          [ 1.7337,  1.7163,  1.6814,  ...,  1.6814,  1.7337,  1.6988],\n",
       "          [ 1.7163,  1.7163,  1.7337,  ...,  1.6814,  1.7337,  1.7163],\n",
       "          ...,\n",
       "          [ 1.7337,  1.7860,  1.7337,  ...,  1.0365,  1.0539,  0.9494],\n",
       "          [ 1.7337,  1.7685,  1.7685,  ...,  0.0256,  0.0082, -0.1312],\n",
       "          [ 1.7337,  1.7337,  1.7860,  ..., -0.3927, -0.2184, -0.2707]]]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section reports the dataset sizes to terminal for user information, as well as showing the class labels (200) for this bird classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data\n",
      "========================================\n",
      "train  size::  5994  images\n",
      "test  size::  5794  images\n",
      "\n",
      "Number of classes::  200\n",
      "========================================\n",
      "0 ::  001.Black_footed_Albatross\n",
      "1 ::  002.Laysan_Albatross\n",
      "2 ::  003.Sooty_Albatross\n",
      "3 ::  004.Groove_billed_Ani\n",
      "4 ::  005.Crested_Auklet\n",
      "5 ::  006.Least_Auklet\n",
      "6 ::  007.Parakeet_Auklet\n",
      "7 ::  008.Rhinoceros_Auklet\n",
      "8 ::  009.Brewer_Blackbird\n",
      "9 ::  010.Red_winged_Blackbird\n",
      "10 ::  011.Rusty_Blackbird\n",
      "11 ::  012.Yellow_headed_Blackbird\n",
      "12 ::  013.Bobolink\n",
      "13 ::  014.Indigo_Bunting\n",
      "14 ::  015.Lazuli_Bunting\n",
      "15 ::  016.Painted_Bunting\n",
      "16 ::  017.Cardinal\n",
      "17 ::  018.Spotted_Catbird\n",
      "18 ::  019.Gray_Catbird\n",
      "19 ::  020.Yellow_breasted_Chat\n",
      "20 ::  021.Eastern_Towhee\n",
      "21 ::  022.Chuck_will_Widow\n",
      "22 ::  023.Brandt_Cormorant\n",
      "23 ::  024.Red_faced_Cormorant\n",
      "24 ::  025.Pelagic_Cormorant\n",
      "25 ::  026.Bronzed_Cowbird\n",
      "26 ::  027.Shiny_Cowbird\n",
      "27 ::  028.Brown_Creeper\n",
      "28 ::  029.American_Crow\n",
      "29 ::  030.Fish_Crow\n",
      "30 ::  031.Black_billed_Cuckoo\n",
      "31 ::  032.Mangrove_Cuckoo\n",
      "32 ::  033.Yellow_billed_Cuckoo\n",
      "33 ::  034.Gray_crowned_Rosy_Finch\n",
      "34 ::  035.Purple_Finch\n",
      "35 ::  036.Northern_Flicker\n",
      "36 ::  037.Acadian_Flycatcher\n",
      "37 ::  038.Great_Crested_Flycatcher\n",
      "38 ::  039.Least_Flycatcher\n",
      "39 ::  040.Olive_sided_Flycatcher\n",
      "40 ::  041.Scissor_tailed_Flycatcher\n",
      "41 ::  042.Vermilion_Flycatcher\n",
      "42 ::  043.Yellow_bellied_Flycatcher\n",
      "43 ::  044.Frigatebird\n",
      "44 ::  045.Northern_Fulmar\n",
      "45 ::  046.Gadwall\n",
      "46 ::  047.American_Goldfinch\n",
      "47 ::  048.European_Goldfinch\n",
      "48 ::  049.Boat_tailed_Grackle\n",
      "49 ::  050.Eared_Grebe\n",
      "50 ::  051.Horned_Grebe\n",
      "51 ::  052.Pied_billed_Grebe\n",
      "52 ::  053.Western_Grebe\n",
      "53 ::  054.Blue_Grosbeak\n",
      "54 ::  055.Evening_Grosbeak\n",
      "55 ::  056.Pine_Grosbeak\n",
      "56 ::  057.Rose_breasted_Grosbeak\n",
      "57 ::  058.Pigeon_Guillemot\n",
      "58 ::  059.California_Gull\n",
      "59 ::  060.Glaucous_winged_Gull\n",
      "60 ::  061.Heermann_Gull\n",
      "61 ::  062.Herring_Gull\n",
      "62 ::  063.Ivory_Gull\n",
      "63 ::  064.Ring_billed_Gull\n",
      "64 ::  065.Slaty_backed_Gull\n",
      "65 ::  066.Western_Gull\n",
      "66 ::  067.Anna_Hummingbird\n",
      "67 ::  068.Ruby_throated_Hummingbird\n",
      "68 ::  069.Rufous_Hummingbird\n",
      "69 ::  070.Green_Violetear\n",
      "70 ::  071.Long_tailed_Jaeger\n",
      "71 ::  072.Pomarine_Jaeger\n",
      "72 ::  073.Blue_Jay\n",
      "73 ::  074.Florida_Jay\n",
      "74 ::  075.Green_Jay\n",
      "75 ::  076.Dark_eyed_Junco\n",
      "76 ::  077.Tropical_Kingbird\n",
      "77 ::  078.Gray_Kingbird\n",
      "78 ::  079.Belted_Kingfisher\n",
      "79 ::  080.Green_Kingfisher\n",
      "80 ::  081.Pied_Kingfisher\n",
      "81 ::  082.Ringed_Kingfisher\n",
      "82 ::  083.White_breasted_Kingfisher\n",
      "83 ::  084.Red_legged_Kittiwake\n",
      "84 ::  085.Horned_Lark\n",
      "85 ::  086.Pacific_Loon\n",
      "86 ::  087.Mallard\n",
      "87 ::  088.Western_Meadowlark\n",
      "88 ::  089.Hooded_Merganser\n",
      "89 ::  090.Red_breasted_Merganser\n",
      "90 ::  091.Mockingbird\n",
      "91 ::  092.Nighthawk\n",
      "92 ::  093.Clark_Nutcracker\n",
      "93 ::  094.White_breasted_Nuthatch\n",
      "94 ::  095.Baltimore_Oriole\n",
      "95 ::  096.Hooded_Oriole\n",
      "96 ::  097.Orchard_Oriole\n",
      "97 ::  098.Scott_Oriole\n",
      "98 ::  099.Ovenbird\n",
      "99 ::  100.Brown_Pelican\n",
      "100 ::  101.White_Pelican\n",
      "101 ::  102.Western_Wood_Pewee\n",
      "102 ::  103.Sayornis\n",
      "103 ::  104.American_Pipit\n",
      "104 ::  105.Whip_poor_Will\n",
      "105 ::  106.Horned_Puffin\n",
      "106 ::  107.Common_Raven\n",
      "107 ::  108.White_necked_Raven\n",
      "108 ::  109.American_Redstart\n",
      "109 ::  110.Geococcyx\n",
      "110 ::  111.Loggerhead_Shrike\n",
      "111 ::  112.Great_Grey_Shrike\n",
      "112 ::  113.Baird_Sparrow\n",
      "113 ::  114.Black_throated_Sparrow\n",
      "114 ::  115.Brewer_Sparrow\n",
      "115 ::  116.Chipping_Sparrow\n",
      "116 ::  117.Clay_colored_Sparrow\n",
      "117 ::  118.House_Sparrow\n",
      "118 ::  119.Field_Sparrow\n",
      "119 ::  120.Fox_Sparrow\n",
      "120 ::  121.Grasshopper_Sparrow\n",
      "121 ::  122.Harris_Sparrow\n",
      "122 ::  123.Henslow_Sparrow\n",
      "123 ::  124.Le_Conte_Sparrow\n",
      "124 ::  125.Lincoln_Sparrow\n",
      "125 ::  126.Nelson_Sharp_tailed_Sparrow\n",
      "126 ::  127.Savannah_Sparrow\n",
      "127 ::  128.Seaside_Sparrow\n",
      "128 ::  129.Song_Sparrow\n",
      "129 ::  130.Tree_Sparrow\n",
      "130 ::  131.Vesper_Sparrow\n",
      "131 ::  132.White_crowned_Sparrow\n",
      "132 ::  133.White_throated_Sparrow\n",
      "133 ::  134.Cape_Glossy_Starling\n",
      "134 ::  135.Bank_Swallow\n",
      "135 ::  136.Barn_Swallow\n",
      "136 ::  137.Cliff_Swallow\n",
      "137 ::  138.Tree_Swallow\n",
      "138 ::  139.Scarlet_Tanager\n",
      "139 ::  140.Summer_Tanager\n",
      "140 ::  141.Artic_Tern\n",
      "141 ::  142.Black_Tern\n",
      "142 ::  143.Caspian_Tern\n",
      "143 ::  144.Common_Tern\n",
      "144 ::  145.Elegant_Tern\n",
      "145 ::  146.Forsters_Tern\n",
      "146 ::  147.Least_Tern\n",
      "147 ::  148.Green_tailed_Towhee\n",
      "148 ::  149.Brown_Thrasher\n",
      "149 ::  150.Sage_Thrasher\n",
      "150 ::  151.Black_capped_Vireo\n",
      "151 ::  152.Blue_headed_Vireo\n",
      "152 ::  153.Philadelphia_Vireo\n",
      "153 ::  154.Red_eyed_Vireo\n",
      "154 ::  155.Warbling_Vireo\n",
      "155 ::  156.White_eyed_Vireo\n",
      "156 ::  157.Yellow_throated_Vireo\n",
      "157 ::  158.Bay_breasted_Warbler\n",
      "158 ::  159.Black_and_white_Warbler\n",
      "159 ::  160.Black_throated_Blue_Warbler\n",
      "160 ::  161.Blue_winged_Warbler\n",
      "161 ::  162.Canada_Warbler\n",
      "162 ::  163.Cape_May_Warbler\n",
      "163 ::  164.Cerulean_Warbler\n",
      "164 ::  165.Chestnut_sided_Warbler\n",
      "165 ::  166.Golden_winged_Warbler\n",
      "166 ::  167.Hooded_Warbler\n",
      "167 ::  168.Kentucky_Warbler\n",
      "168 ::  169.Magnolia_Warbler\n",
      "169 ::  170.Mourning_Warbler\n",
      "170 ::  171.Myrtle_Warbler\n",
      "171 ::  172.Nashville_Warbler\n",
      "172 ::  173.Orange_crowned_Warbler\n",
      "173 ::  174.Palm_Warbler\n",
      "174 ::  175.Pine_Warbler\n",
      "175 ::  176.Prairie_Warbler\n",
      "176 ::  177.Prothonotary_Warbler\n",
      "177 ::  178.Swainson_Warbler\n",
      "178 ::  179.Tennessee_Warbler\n",
      "179 ::  180.Wilson_Warbler\n",
      "180 ::  181.Worm_eating_Warbler\n",
      "181 ::  182.Yellow_Warbler\n",
      "182 ::  183.Northern_Waterthrush\n",
      "183 ::  184.Louisiana_Waterthrush\n",
      "184 ::  185.Bohemian_Waxwing\n",
      "185 ::  186.Cedar_Waxwing\n",
      "186 ::  187.American_Three_toed_Woodpecker\n",
      "187 ::  188.Pileated_Woodpecker\n",
      "188 ::  189.Red_bellied_Woodpecker\n",
      "189 ::  190.Red_cockaded_Woodpecker\n",
      "190 ::  191.Red_headed_Woodpecker\n",
      "191 ::  192.Downy_Woodpecker\n",
      "192 ::  193.Bewick_Wren\n",
      "193 ::  194.Cactus_Wren\n",
      "194 ::  195.Carolina_Wren\n",
      "195 ::  196.House_Wren\n",
      "196 ::  197.Marsh_Wren\n",
      "197 ::  198.Rock_Wren\n",
      "198 ::  199.Winter_Wren\n",
      "199 ::  200.Common_Yellowthroat\n"
     ]
    }
   ],
   "source": [
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'test']}\n",
    "class_names = image_datasets['train'].classes\n",
    "\n",
    "print('Number of data')\n",
    "print('========================================')\n",
    "for dataset in dataset_sizes.keys():\n",
    "    print(dataset,' size:: ', dataset_sizes[dataset],' images')\n",
    "\n",
    "print('')\n",
    "print('Number of classes:: ', len(class_names))\n",
    "print('========================================')\n",
    "for i_class, class_name in enumerate(class_names):\n",
    "    print(i_class,':: ',class_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Device Setup\n",
    "\n",
    "PyTorch can be run a number of computational devices, including CPU, GPU, or even TPU if available.\n",
    "\n",
    "Generally speaking, most modern CNN require a considerable amout of TFLOPS of processing power to complete adequate training, and thus is generally not suitable for training on CPUs alone. GPU, with their ability to perform highly parallelized matrix operations due to their architecture of many cores are particularly well suited to this type of problem. A majority of the models in these examples require GPUs to perform training in a reasonable amount of time (30 minutes to 24 hours, depending on network size).\n",
    "\n",
    "Here we check for the availibility of GPU on the system and use that if available. If not, we set the device to CPU (but this is really not advisable for training purposes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Setup the device to run the computations\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Device::', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load a pre-trained model on ImageNet and train\n",
    "\n",
    "In this section we arrive at the point where we choose the model architecture and load pre-trained weights.\n",
    "\n",
    "What do we mean by pre-trained weights?\n",
    "\n",
    "![Image](https://miro.medium.com/max/764/1*BpnKbH4KiluEeC6PBSBb1w.png)\n",
    "\n",
    "When we build a neural network, we employ a series of chained functions, each that have their set of coefficients which need to trained optimally to produce the best accuracy in predicting the desired outcome. If we compare this to the more simple notion of trend line fitting using a straight line, the function y = ax + b dictates how we model a straight line. In this formulation, we have coefficients a and b, which are the gradient and the y-intercept value, which determines the behaviour of straight line function. During the process of fitting linear trend lines, we aim to train the coefficients, or weights, such that the straight optimally fits our observed data. However, a the beginning of the process we need to select values for those weights for the initial guess. In this simple example it could be simply setting the coefficients equal to 0, (a,b = 0), and then iterating to a solution from this point. In general, this topic of investigation is called \"initializing a neural network\" and simply relates the to values that the sometimes millions of coefficients (or weights) in a network take at the beginning of network training.\n",
    "\n",
    "If we wanted to start the model training from scratch then we might want to initialise the weights using random numbers from a certain distribution. This would be parting little or no prior knowledge of the expected features that our network might learn over the training process, giving it free reign to learn what it thinks is the most sensitive set of feature maps for the given problem.\n",
    "\n",
    "Another way of initialising a neural network is by way of using pre-trained weights. This is where the network in question has already been trained on a set of images (that are different to the problem at hand), and has produced a set of feature maps which perform adequately on that dataset. This type of approach falls under another subject heading called \"transfer learning\". This type of approach has a reasonable precident, given that the ultimate goal of training neural networks is produce feature maps that are abstract enough that they perform well on image classification problems with images that have not been seen before by the network. The more abstract these representations become, generally the better the network performs. Therefore, it follows that if we use a network that has been trained to a good performance on a general image set, then it will likely train better and perform more accurately than a network trained from random weights. It has been shown in the literature that pre-training even with highly general image sets such as ImageNet, results in better performing models, even for fine-grained classication problems like this one where there is minimal cross-over in class content. For example, the ImageNet may have a single category for bird, whereas the objective of this classification problem is classify 200 different types of bird species.\n",
    "\n",
    "More recent publications in the literature have shown that even further gains can be made by pre-training on generalized datasets that have more similarity to the problem at hand. For example, in the CUB-200-2011 dataset, improvements in classification performance have been found using domain specific transfer learning. For example [Cui et al 2018](https://arxiv.org/abs/1806.06193), found that using the [iNaturalist dataset](https://www.kaggle.com/c/inaturalist-challenge-at-fgvc-2017) [[iNaturalist ref]](https://arxiv.org/abs/1707.06642), a diverse naturist database of images of flauna and fora, including birds, resulted in improved classification performance when compared to networks using ImageNet or trained from scratch. This is all because the networks have been trained successfully enough to produce feature maps that generalize well to other, more fine grained problems like this. Starting training from these types of models allows the network to spend more time learning the fine-grained details between bird species.\n",
    "\n",
    "In this particular example, we will use the Torchvision ResNeXt 152 architecture, which has been pre-trained on the [ImageNet database](http://www.image-net.org/).\n",
    "\n",
    "The approach can be characterised in the following flow diagram:\n",
    "\n",
    "![Image](https://www.mathworks.com/help/examples/nnet/win64/TransferLearningUsingAlexNetExample_01.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly we load the most architecure from the Torchvision.models module, specifying the **pretrained=True** for downloading and populating the ImageNet trained weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the model and optimiser\n",
    "model_ft = timm.create_model('vit_base_patch16_224', pretrained=True, num_classes=len(class_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Secondly, we have to modify the output layer of the network, as this has the wrong number of classes for our problem. Currently, it is designed for ImageNet classication which has 1000 classes in it's dataset, and thus the model produces predictions of each image for every of the 1000 classes. Our problem has only 200 classes, so we need to remove the final Linear Classifier layer, and replace it with the same Linear Classifier layer, but with an output of size of 200, not 1000. The network feature maps in the hidden layers are still initialized with the ImageNet based features, but we now have a classifier that is suitable for out purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOT NEEDED AS THE PREVIOUS STEP CREATES THE OUTPUT LAYER WITH THE RIGHT NUMBER OF CLASSES\n",
    "#num_ftrs = model_ft.fc.in_features\n",
    "## Here the size of each output sample is set to 2.\n",
    "## Alternatively, it can be generalized to nn.Linear(num_ftrs, len(class_names)).\n",
    "#model_ft.fc = nn.Linear(num_ftrs, len(class_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final step of preparing the model is to push the model to the device on which it will computed on, in this case the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft = model_ft.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function criterion, optimizer and learning rate scheduler\n",
    "\n",
    "Here we specify three important objects that relate to training our model.\n",
    "\n",
    "### Object Function\n",
    "\n",
    "The objective function is the function which measures the performance of a model which outputs a probability for each class as a prediction. In this particular problem, we have chosen to use the cross-entropy loss function, where the cross-entropy value increases as the predicted probability diverges from the true label. The loss (or objective function) helps us determine if our model is updating and iterating towards a solution, that is, it is getting better at predicting the true labels.\n",
    "\n",
    "Cross Entropy = −(ylog(p)+(1−y)log(1−p))\n",
    "\n",
    "![Image](https://ml-cheatsheet.readthedocs.io/en/latest/_images/cross_entropy.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Train the model\n",
    "\n",
    "Finally, we have arrived at the point where we train the model.\n",
    "\n",
    "The key parts of the model that we have built are:\n",
    "\n",
    "    1. The model itself - a ResNeXt 152 model with pre-trained weights from ImageNet training and a output layer of 200 classes.\n",
    "    \n",
    "    2. criterion - the objective function we use to help measure how well our model is performing.\n",
    "    \n",
    "    3. optimiser - the algorithm we use optimise the model coefficients by iterively exploring solution space. In this case, we use stochastic gradient descent.\n",
    "    \n",
    "![Image](https://miro.medium.com/max/699/1*mElyetzsTIJrNnKI8kTkCw.jpeg)\n",
    "\n",
    "![Image](https://ml-cheatsheet.readthedocs.io/en/latest/_images/gradient_descent_demystified.png)\n",
    "    \n",
    "    4. learning rate scheduler - the distance we step through model space at each iteration is important as controls how quickly we find a solution. This function controls how the distance we step after iteration varies as we progress through model training.\n",
    "    \n",
    "    5. device - the target device the training will be performed on e.g. GPU\n",
    "    \n",
    "    6. dataloaders - the iterative functions which control the loading and transforming of the input images of each mini-batch into the network during training  and validation steps.\n",
    "\n",
    "We pass the key parts of the model training process to the train_model function, which controls the epoch (iterations) of the model training and the batching of images into the model. It also instantuates the back-propagation of the gradient error through the model at the end of each epoch, and updates the model coefficients.\n",
    "Lastly, it provides an assessment of the performance of the model at the end of each epoch, using the held out validation image set, which has not been used to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/39\n",
      "----------\n",
      "train Loss: 3.3361 Acc: 0.3874\n",
      "test Loss: 1.3870 Acc: 0.7423\n",
      "\n",
      "Epoch 1/39\n",
      "----------\n",
      "train Loss: 1.3120 Acc: 0.7049\n",
      "test Loss: 0.7874 Acc: 0.8114\n",
      "\n",
      "Epoch 2/39\n",
      "----------\n",
      "train Loss: 0.9372 Acc: 0.7743\n",
      "test Loss: 0.6116 Acc: 0.8459\n",
      "\n",
      "Epoch 3/39\n",
      "----------\n",
      "train Loss: 0.7884 Acc: 0.8028\n",
      "test Loss: 0.5393 Acc: 0.8595\n",
      "\n",
      "Epoch 4/39\n",
      "----------\n",
      "train Loss: 0.7110 Acc: 0.8215\n",
      "test Loss: 0.4948 Acc: 0.8714\n",
      "\n",
      "Epoch 5/39\n",
      "----------\n",
      "train Loss: 0.6383 Acc: 0.8430\n",
      "test Loss: 0.4712 Acc: 0.8707\n",
      "\n",
      "Epoch 6/39\n",
      "----------\n",
      "train Loss: 0.6184 Acc: 0.8480\n",
      "test Loss: 0.4499 Acc: 0.8794\n",
      "\n",
      "Epoch 7/39\n",
      "----------\n",
      "train Loss: 0.5563 Acc: 0.8644\n",
      "test Loss: 0.4338 Acc: 0.8838\n",
      "\n",
      "Epoch 8/39\n",
      "----------\n",
      "train Loss: 0.5195 Acc: 0.8772\n",
      "test Loss: 0.4281 Acc: 0.8870\n",
      "\n",
      "Epoch 9/39\n",
      "----------\n",
      "train Loss: 0.5255 Acc: 0.8770\n",
      "test Loss: 0.4256 Acc: 0.8866\n",
      "\n",
      "Epoch 10/39\n",
      "----------\n",
      "train Loss: 0.5069 Acc: 0.8815\n",
      "test Loss: 0.4231 Acc: 0.8878\n",
      "\n",
      "Epoch 11/39\n",
      "----------\n",
      "train Loss: 0.5216 Acc: 0.8754\n",
      "test Loss: 0.4217 Acc: 0.8885\n",
      "\n",
      "Epoch 12/39\n",
      "----------\n",
      "train Loss: 0.5007 Acc: 0.8820\n",
      "test Loss: 0.4193 Acc: 0.8889\n",
      "\n",
      "Epoch 13/39\n",
      "----------\n",
      "train Loss: 0.4915 Acc: 0.8837\n",
      "test Loss: 0.4177 Acc: 0.8895\n",
      "\n",
      "Epoch 14/39\n",
      "----------\n",
      "train Loss: 0.5086 Acc: 0.8779\n",
      "test Loss: 0.4176 Acc: 0.8899\n",
      "\n",
      "Epoch 15/39\n",
      "----------\n",
      "train Loss: 0.5007 Acc: 0.8819\n",
      "test Loss: 0.4174 Acc: 0.8906\n",
      "\n",
      "Epoch 16/39\n",
      "----------\n",
      "train Loss: 0.4952 Acc: 0.8834\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-3575f74a1e32>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m model_ft = train_model(model=model_ft, criterion=criterion, optimizer=optimizer_ft, scheduler=exp_lr_scheduler, \n\u001b[0m\u001b[1;32m      2\u001b[0m                        \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_sizes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                        working_dir=working_dir)\n",
      "\u001b[0;32m/datadrive/drive0/projects/image_classification/caltech_birds/cub_tools/cub_tools/train.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, criterion, optimizer, scheduler, device, dataloaders, dataset_sizes, num_epochs, return_history, log_history, working_dir)\u001b[0m\n\u001b[1;32m     56\u001b[0m                 \u001b[0;31m# track history if only in train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m                     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m                     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/timm/models/vision_transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 374\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    375\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/timm/models/vision_transformer.py\u001b[0m in \u001b[0;36mforward_features\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mblk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 367\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/timm/models/vision_transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/timm/models/vision_transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    932\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_full_backward_hook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 934\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Module'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    935\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'_parameters'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    936\u001b[0m             \u001b[0m_parameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'_parameters'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_ft = train_model(model=model_ft, criterion=criterion, optimizer=optimizer_ft, scheduler=exp_lr_scheduler, \n",
    "                       device=device, dataloaders=dataloaders, dataset_sizes=dataset_sizes, num_epochs=40,\n",
    "                       working_dir=working_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model training history\n",
    "model_history = '../models/classification/resnet152/model_history.pkl'\n",
    "history = unpickle(model_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "At the end of training process, we are passed back model of the epoch at which the model performed the best on. \n",
    "\n",
    "![Image](https://online.stat.psu.edu/onlinecourses/sites/stat508/files/lesson04/model_complexity.png)\n",
    "\n",
    "As model training progresses, it common to see both the training loss and validation loss decrease with iteration. This indicates that the model is learning, and at the end of each epoch, the model is better at predicting the class labels on the images in both the training and test set. At some point, the test/validation loss will start to increase, whilst the training loss continues to decrease. This the point at the which the model has stopped generalizing, that is, it has stopped learning features that will improve its ability to predict class labels on unseen data. Put another way, the model is starting to be over-trained, because the features it is learning are becoming specific to the training set. These features are not replicated in the validation set, they may even be representations of noise specific to the training set, and hence the model performs more poorly as training continues to overfit. Hence, the best model is the model that achieves the lowest validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,10))\n",
    "\n",
    "plt.subplot(2,1,1)\n",
    "plt.plot(np.arange(0, np.max(history['epoch'])+1,1), history['train_loss'], 'b-', label='Train')\n",
    "plt.plot(np.arange(0, np.max(history['epoch'])+1,1), history['test_loss'], 'r-', label='Test')\n",
    "plt.grid(True)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training / Validation Loss - Caltech Birds - {}'.format(model_name))\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2,1,2)\n",
    "plt.plot(np.arange(0, np.max(history['epoch'])+1,1), history['train_acc'], 'b-', label='Train')\n",
    "plt.plot(np.arange(0, np.max(history['epoch'])+1,1), history['test_acc'], 'r-', label='Test')\n",
    "plt.grid(True)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training / Validation Accuracy - Caltech Birds - {}'.format(model_name))\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model_full(model=model_ft, PATH='models/classification/caltech_birds_resnet152_full.pth')\n",
    "save_model_dict(model=model_ft, PATH='models/classification/caltech_birds_resnet152_dict.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3.8 TIMM [PyTorch 1.8.1 - CUDA 11.1]",
   "language": "python",
   "name": "py38_pytorch181_cu111_timm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
