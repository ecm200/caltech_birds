{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets, models\n",
    "\n",
    "import skorch\n",
    "from skorch import NeuralNetClassifier\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import copy\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "\n",
    "# Local modules\n",
    "from cub_tools.train import train_model\n",
    "from cub_tools.visualize import imshow, visualize_model\n",
    "from cub_tools.utils import unpickle, save_pickle\n",
    "from cub_tools.transforms import makeDefaultTransforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Script runtime options\n",
    "model_names = ['resnet152', 'resnext101_32x8d', 'inception_v3', 'googlenet']\n",
    "data_parallel = {'resnet152' : False,\n",
    "                 'inceptionv4' : True,\n",
    "                 'resnext101_64x4d' : True, \n",
    "                 'pnasnet5large' : True, \n",
    "                 'googlenet' : False,\n",
    "                 'inception_v3' : False, \n",
    "                 'resnext101_32x8d' : False}\n",
    "data_root_dir = '../data'\n",
    "model_root_dir = '../models'\n",
    "stages = ['train', 'test']\n",
    "\n",
    "\n",
    "# Paths setup\n",
    "data_dir = os.path.join(data_root_dir,'images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data transforms\n",
    "data_transforms = makeDefaultTransforms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup data loaders with augmentation transforms\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x])\n",
    "                  for x in stages}\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=16,\n",
    "                                             shuffle=True, num_workers=4)\n",
    "              for x in stages}\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in stages}\n",
    "class_names = image_datasets[stages[0]].classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Setup the device to run the computations\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Device::', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading model resnet152\n",
      "[INFO] Loading model resnext101_32x8d\n",
      "[INFO] Loading model inception_v3\n",
      "[INFO] Loading model googlenet\n"
     ]
    }
   ],
   "source": [
    "models = {}\n",
    "\n",
    "for model_name in model_names:\n",
    "    \n",
    "    print('[INFO] Loading model {}'.format(model_name))\n",
    "    \n",
    "    # Paths\n",
    "    output_dir = os.path.join(model_root_dir,'classification/{}'.format(model_name))\n",
    "    model_file = os.path.join(output_dir, 'caltech_birds_{}_full.pth'.format(model_name))\n",
    "    \n",
    "    # Load the best model from file\n",
    "    models[model_name] = torch.load(model_file)\n",
    "    if data_parallel[model_name]:\n",
    "        models[model_name] = torch.nn.DataParallel(model[model_name])\n",
    "    models[model_name] = models[model_name].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = {}\n",
    "for model_name in model_names:\n",
    "    net[model_name] = NeuralNetClassifier(models[model_name])\n",
    "    net[model_name].initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Methods\n",
    "\n",
    "Different approaches for ensembling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking ensemble bespoke implementation\n",
    "\n",
    "Following the tutorial here: https://machinelearningmastery.com/stacking-ensemble-for-deep-learning-neural-networks/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import dstack\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "\n",
    "# TODO: refactor so that it accepts a dataloader instance and iterates over the whole dataset.\n",
    "#       Add as an outer loop and then run each model on an inner loop, so that they use the same images.\n",
    "#       Remember to also keep the labels for later use and return them\n",
    "def stacked_dataset(models, inputs):\n",
    "    stackX = None\n",
    "    for model_name, model in models.items():\n",
    "        # make prediction\n",
    "        yhat = model.predict_proba(inputs)\n",
    "        for i in np.arange(0, yhat.shape[0], 1):\n",
    "            yhat[i, ::] = softmax(yhat[i,::])\n",
    "        # stack predictions into [rows, members, probabilities]\n",
    "        if stackX is None:\n",
    "            stackX = yhat\n",
    "        else:\n",
    "            stackX = dstack((stackX, yhat))\n",
    "        print('{}...'.format(model_name), end='')\n",
    "    # flatten predictions to [rows, members x probabilities]\n",
    "    stackX = stackX.reshape((stackX.shape[0], stackX.shape[1]*stackX.shape[2]))\n",
    "    return stackX\n",
    "\n",
    "def stacked_dataset_from_dataloader(models, dataloader, device):\n",
    "    stackX = None\n",
    "    stacky = None\n",
    "    print('[INFO] Starting StackX', end='')\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, labels) in enumerate(dataloader):\n",
    "            if i < (len(dataloader)-1):\n",
    "                temp_stack = None\n",
    "                for model_name, model in models.items():\n",
    "                    # make prediction\n",
    "                    if isinstance(model, skorch.classifier.NeuralNetClassifier):\n",
    "                        yhat = model.predict_proba(inputs)\n",
    "                    else:\n",
    "                        model.eval()\n",
    "                        inputs = inputs.to(device)\n",
    "                        yhat = model(inputs)\n",
    "                        yhat = yhat.cpu().numpy()\n",
    "\n",
    "                    # Convert score to probability\n",
    "                    for ind in np.arange(0, yhat.shape[0], 1):\n",
    "                        yhat[ind, ::] = softmax(yhat[ind,::])\n",
    "\n",
    "                    # stack predictions into [rows, members, probabilities]\n",
    "                    if temp_stack is None:\n",
    "                        temp_stack = yhat\n",
    "                    else:\n",
    "                        temp_stack = dstack((temp_stack, yhat))\n",
    "\n",
    "                # flatten predictions to [rows, members x probabilities]\n",
    "                temp_stack = temp_stack.reshape((temp_stack.shape[0], temp_stack.shape[1]*temp_stack.shape[2]))\n",
    "                # stack the batch of model probabilities onto the bottom of the results table\n",
    "                if stackX is None:\n",
    "                    stackX = temp_stack\n",
    "                else:\n",
    "                    stackX = np.vstack((stackX, temp_stack))\n",
    "\n",
    "                # stack the output truth labels to bottom of truth labels table\n",
    "                if stacky is None:\n",
    "                    stacky = labels.cpu().numpy().ravel()\n",
    "                else:\n",
    "                    stacky = np.vstack((stacky, labels.cpu().numpy().ravel()))\n",
    "                    \n",
    "                if i % 5 == 0:\n",
    "                    print('..{}'.format(i), end='')\n",
    "    \n",
    "    print('..Complete')\n",
    "    return stackX, stacky\n",
    "\n",
    "# fit a model based on the outputs from the ensemble members\n",
    "def fit_stacked_model(models, dataloader=None, stackX=None, labels=None, \n",
    "                      meta_learner=LogisticRegression, meta_learner_options=None):\n",
    "    # create dataset using ensemble\n",
    "    if (stackX is None) or (labels is None):\n",
    "        print('[INFO] Creating the meta learner inputs (probabilities from individual models) as none provided.')\n",
    "        stackX, labels = stacked_dataset_from_dataloader(models, dataloader)\n",
    "    else:\n",
    "        print('[INFO] Stacked input table and labels found, using these to train meta learner.')\n",
    "    \n",
    "    # fit standalone model\n",
    "    print('[INFO] Training the meta learner...', end='')\n",
    "    if meta_learner_options is not None:\n",
    "        meta_model = meta_learner(**meta_learner_options)\n",
    "    else:\n",
    "        meta_model = meta_learner()\n",
    "    meta_model.fit(stackX, labels)\n",
    "    print('..Complete')\n",
    "    return meta_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Starting StackX..0..5..10..15..20..25..30..35..40..45..50..55..60..65..70..75..80..85..90..95..100..105..110..115..120..125..130..135..140..145..150..155..160..165..170..175..180..185..190..195..200..205..210..215..220..225..230..235..240..245..250..255..260..265..270..275..280..285..290..295..300..305..310..315..320..325..330..335..340..345..350..355..360..365..370Complete\n"
     ]
    }
   ],
   "source": [
    "run_train_data_stack = True\n",
    "if run_train_data_stack:\n",
    "    # Generate the stacked dataset from the ensemble models for the training data\n",
    "    stackX, stacky = stacked_dataset_from_dataloader(models, dataloaders['train'], device)\n",
    "    \n",
    "    # Save to pikle file\n",
    "    save_pickle(stackX, os.path.join(model_root_dir,'classification/caltech_birds_ensemble_1_stackX.pkl'))\n",
    "    save_pickle(stacky, os.path.join(model_root_dir,'classification/caltech_birds_ensemble_1_stacky.pkl'))\n",
    "else:\n",
    "    stackX = unpickle(os.path.join(model_root_dir,'classification/caltech_birds_ensemble_1_stackX.pkl'))\n",
    "    stacky = unpickle(os.path.join(model_root_dir,'classification/caltech_birds_ensemble_1_stacky.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Stacked input table and labels found, using these to train meta learner.\n",
      "[INFO] Training the meta learner...building tree 1 of 250\n",
      "building tree 2 of 250\n",
      "building tree 3 of 250\n",
      "building tree 4 of 250\n",
      "building tree 5 of 250\n",
      "building tree 6 of 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 6 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 7 of 250\n",
      "building tree 8 of 250\n",
      "building tree 9 of 250\n",
      "building tree 10 of 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:    2.2s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 11 of 250\n",
      "building tree 12 of 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   6 tasks      | elapsed:    2.7s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 13 of 250\n",
      "building tree 14 of 250\n",
      "building tree 15 of 250\n",
      "building tree 16 of 250\n",
      "building tree 17 of 250\n",
      "building tree 18 of 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  13 tasks      | elapsed:    6.7s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 19 of 250\n",
      "building tree 20 of 250\n",
      "building tree 21 of 250\n",
      "building tree 22 of 250\n",
      "building tree 23 of 250\n",
      "building tree 24 of 250\n",
      "building tree 25 of 250\n",
      "building tree 26 of 250\n",
      "building tree 27 of 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  20 tasks      | elapsed:    9.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 28 of 250\n",
      "building tree 29 of 250\n",
      "building tree 30 of 250\n",
      "building tree 31 of 250\n",
      "building tree 32 of 250\n",
      "building tree 33 of 250\n",
      "building tree 34 of 250\n",
      "building tree 35 of 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  29 tasks      | elapsed:   11.8s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 36 of 250\n",
      "building tree 37 of 250\n",
      "building tree 38 of 250\n",
      "building tree 39 of 250\n",
      "building tree 40 of 250\n",
      "building tree 41 of 250\n",
      "building tree 42 of 250\n",
      "building tree 43 of 250\n",
      "building tree 44 of 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:   15.7s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 45 of 250\n",
      "building tree 46 of 250\n",
      "building tree 47 of 250\n",
      "building tree 48 of 250\n",
      "building tree 49 of 250\n",
      "building tree 50 of 250\n",
      "building tree 51 of 250\n",
      "building tree 52 of 250\n",
      "building tree 53 of 250\n",
      "building tree 54 of 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  49 tasks      | elapsed:   20.3s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 55 of 250\n",
      "building tree 56 of 250\n",
      "building tree 57 of 250\n",
      "building tree 58 of 250\n",
      "building tree 59 of 250\n",
      "building tree 60 of 250\n",
      "building tree 61 of 250\n",
      "building tree 62 of 250\n",
      "building tree 63 of 250\n",
      "building tree 64 of 250\n",
      "building tree 65 of 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  60 tasks      | elapsed:   23.9s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 66 of 250\n",
      "building tree 67 of 250\n",
      "building tree 68 of 250\n",
      "building tree 69 of 250\n",
      "building tree 70 of 250\n",
      "building tree 71 of 250\n",
      "building tree 72 of 250\n",
      "building tree 73 of 250\n",
      "building tree 74 of 250\n",
      "building tree 75 of 250\n",
      "building tree 76 of 250\n",
      "building tree 77 of 250\n",
      "building tree 78 of 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  73 tasks      | elapsed:   29.2s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 79 of 250\n",
      "building tree 80 of 250\n",
      "building tree 81 of 250\n",
      "building tree 82 of 250\n",
      "building tree 83 of 250\n",
      "building tree 84 of 250\n",
      "building tree 85 of 250\n",
      "building tree 86 of 250\n",
      "building tree 87 of 250\n",
      "building tree 88 of 250\n",
      "building tree 89 of 250\n",
      "building tree 90 of 250\n",
      "building tree 91 of 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  86 tasks      | elapsed:   34.4s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 92 of 250\n",
      "building tree 93 of 250\n",
      "building tree 94 of 250\n",
      "building tree 95 of 250\n",
      "building tree 96 of 250\n",
      "building tree 97 of 250\n",
      "building tree 98 of 250\n",
      "building tree 99 of 250\n",
      "building tree 100 of 250\n",
      "building tree 101 of 250\n",
      "building tree 102 of 250\n",
      "building tree 103 of 250\n",
      "building tree 104 of 250\n",
      "building tree 105 of 250\n",
      "building tree 106 of 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 101 tasks      | elapsed:   39.6s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 107 of 250\n",
      "building tree 108 of 250\n",
      "building tree 109 of 250\n",
      "building tree 110 of 250\n",
      "building tree 111 of 250\n",
      "building tree 112 of 250\n",
      "building tree 113 of 250\n",
      "building tree 114 of 250\n",
      "building tree 115 of 250\n",
      "building tree 116 of 250\n",
      "building tree 117 of 250\n",
      "building tree 118 of 250\n",
      "building tree 119 of 250\n",
      "building tree 120 of 250\n",
      "building tree 121 of 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 116 tasks      | elapsed:   45.7s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 122 of 250\n",
      "building tree 123 of 250\n",
      "building tree 124 of 250\n",
      "building tree 125 of 250\n",
      "building tree 126 of 250\n",
      "building tree 127 of 250\n",
      "building tree 128 of 250\n",
      "building tree 129 of 250\n",
      "building tree 130 of 250\n",
      "building tree 131 of 250\n",
      "building tree 132 of 250\n",
      "building tree 133 of 250\n",
      "building tree 134 of 250\n",
      "building tree 135 of 250\n",
      "building tree 136 of 250\n",
      "building tree 137 of 250\n",
      "building tree 138 of 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 133 tasks      | elapsed:   52.3s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 139 of 250\n",
      "building tree 140 of 250\n",
      "building tree 141 of 250\n",
      "building tree 142 of 250\n",
      "building tree 143 of 250\n",
      "building tree 144 of 250\n",
      "building tree 145 of 250\n",
      "building tree 146 of 250\n",
      "building tree 147 of 250\n",
      "building tree 148 of 250\n",
      "building tree 149 of 250\n",
      "building tree 150 of 250\n",
      "building tree 151 of 250\n",
      "building tree 152 of 250\n",
      "building tree 153 of 250\n",
      "building tree 154 of 250\n",
      "building tree 155 of 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 150 tasks      | elapsed:   58.3s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 156 of 250\n",
      "building tree 157 of 250\n",
      "building tree 158 of 250\n",
      "building tree 159 of 250\n",
      "building tree 160 of 250\n",
      "building tree 161 of 250\n",
      "building tree 162 of 250\n",
      "building tree 163 of 250\n",
      "building tree 164 of 250\n",
      "building tree 165 of 250\n",
      "building tree 166 of 250\n",
      "building tree 167 of 250\n",
      "building tree 168 of 250\n",
      "building tree 169 of 250\n",
      "building tree 170 of 250\n",
      "building tree 171 of 250\n",
      "building tree 172 of 250\n",
      "building tree 173 of 250\n",
      "building tree 174 of 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 169 tasks      | elapsed:  1.1min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 175 of 250\n",
      "building tree 176 of 250\n",
      "building tree 177 of 250\n",
      "building tree 178 of 250\n",
      "building tree 179 of 250\n",
      "building tree 180 of 250\n",
      "building tree 181 of 250\n",
      "building tree 182 of 250\n",
      "building tree 183 of 250\n",
      "building tree 184 of 250\n",
      "building tree 185 of 250\n",
      "building tree 186 of 250\n",
      "building tree 187 of 250\n",
      "building tree 188 of 250\n",
      "building tree 189 of 250\n",
      "building tree 190 of 250\n",
      "building tree 191 of 250\n",
      "building tree 192 of 250\n",
      "building tree 193 of 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 188 tasks      | elapsed:  1.2min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 194 of 250\n",
      "building tree 195 of 250\n",
      "building tree 196 of 250\n",
      "building tree 197 of 250\n",
      "building tree 198 of 250\n",
      "building tree 199 of 250\n",
      "building tree 200 of 250\n",
      "building tree 201 of 250\n",
      "building tree 202 of 250\n",
      "building tree 203 of 250\n",
      "building tree 204 of 250\n",
      "building tree 205 of 250\n",
      "building tree 206 of 250\n",
      "building tree 207 of 250\n",
      "building tree 208 of 250\n",
      "building tree 209 of 250\n",
      "building tree 210 of 250\n",
      "building tree 211 of 250\n",
      "building tree 212 of 250\n",
      "building tree 213 of 250\n",
      "building tree 214 of 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 209 tasks      | elapsed:  1.3min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 215 of 250\n",
      "building tree 216 of 250\n",
      "building tree 217 of 250\n",
      "building tree 218 of 250\n",
      "building tree 219 of 250\n",
      "building tree 220 of 250\n",
      "building tree 221 of 250\n",
      "building tree 222 of 250\n",
      "building tree 223 of 250\n",
      "building tree 224 of 250\n",
      "building tree 225 of 250\n",
      "building tree 226 of 250\n",
      "building tree 227 of 250\n",
      "building tree 228 of 250\n",
      "building tree 229 of 250\n",
      "building tree 230 of 250\n",
      "building tree 231 of 250\n",
      "building tree 232 of 250\n",
      "building tree 233 of 250\n",
      "building tree 234 of 250\n",
      "building tree 235 of 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 230 tasks      | elapsed:  1.5min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 236 of 250\n",
      "building tree 237 of 250\n",
      "building tree 238 of 250\n",
      "building tree 239 of 250\n",
      "building tree 240 of 250\n",
      "building tree 241 of 250\n",
      "building tree 242 of 250\n",
      "building tree 243 of 250\n",
      "building tree 244 of 250\n",
      "building tree 245 of 250\n",
      "building tree 246 of 250\n",
      "building tree 247 of 250\n",
      "building tree 248 of 250\n",
      "building tree 249 of 250\n",
      "building tree 250 of 250\n",
      "..Complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 250 out of 250 | elapsed:  1.6min finished\n"
     ]
    }
   ],
   "source": [
    "meta_model = fit_stacked_model(models=models, stackX=stackX, labels=stacky.ravel(), \n",
    "                               meta_learner=RandomForestClassifier, \n",
    "                               meta_learner_options={'n_estimators' : 250, 'verbose' : 10, 'n_jobs' : -1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stacked_prediction(meta_learner, models, device=None, dataloader=None, stackX=None):\n",
    "    # create dataset using ensemble\n",
    "    return_ytrue=False\n",
    "    if (stackX is None):\n",
    "        print('[INFO] Creating the meta learner inputs (probabilities from individual models) as none provided.')\n",
    "        stackX, ytrue = stacked_dataset_from_dataloader(models, dataloader, device)\n",
    "        return_ytrue = True\n",
    "    else:\n",
    "        print('[INFO] Stacked input table and labels found, using these to train meta learner.')\n",
    "    \n",
    "    # fit standalone model\n",
    "    print('[INFO] Predicting with the meta learner...', end='')\n",
    "    yhat = meta_learner.predict(stackX)\n",
    "    print('..Complete')\n",
    "    if return_ytrue:\n",
    "        return yhat, ytrue\n",
    "    else:\n",
    "        return yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Creating the meta learner inputs (probabilities from individual models) as none provided.\n",
      "[INFO] Starting StackX..0..5..10..15..20..25..30..35..40..45..50..55..60..65..70..75..80..85..90..95..100..105..110..115..120..125..130..135..140..145..150..155..160..165..170..175..180..185..190..195..200..205..210..215..220..225..230..235..240..245..250..255..260..265..270..275..280..285..290..295..300..305..310..315..320..325..330..335..340..345..350..355..360..Complete\n",
      "[INFO] Predicting with the meta learner..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend ThreadingBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done   1 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=6)]: Done   6 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=6)]: Done  13 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=6)]: Done  20 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=6)]: Done  29 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=6)]: Done  38 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=6)]: Done  49 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=6)]: Done  60 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=6)]: Done  73 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=6)]: Done  86 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=6)]: Done 101 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=6)]: Done 116 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=6)]: Done 133 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=6)]: Done 150 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=6)]: Done 169 tasks      | elapsed:    0.4s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..Complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Done 188 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=6)]: Done 209 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=6)]: Done 230 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=6)]: Done 250 out of 250 | elapsed:    0.6s finished\n"
     ]
    }
   ],
   "source": [
    "yhat, ytrue = stacked_prediction(meta_learner=meta_model, \n",
    "                                 models=models, \n",
    "                                 dataloader=dataloaders['test'], \n",
    "                                 device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.83      0.86        30\n",
      "           1       0.96      0.86      0.91        29\n",
      "           2       0.82      1.00      0.90        28\n",
      "           3       0.91      0.97      0.94        30\n",
      "           4       0.87      0.93      0.90        14\n",
      "           5       0.83      0.91      0.87        11\n",
      "           6       1.00      0.91      0.95        23\n",
      "           7       0.80      0.89      0.84        18\n",
      "           8       0.65      0.38      0.48        29\n",
      "           9       1.00      0.97      0.98        30\n",
      "          10       0.72      0.70      0.71        30\n",
      "          11       0.93      0.96      0.94        26\n",
      "          12       0.97      0.97      0.97        30\n",
      "          13       0.93      0.93      0.93        30\n",
      "          14       0.93      0.96      0.95        28\n",
      "          15       0.90      0.96      0.93        28\n",
      "          16       0.96      1.00      0.98        27\n",
      "          17       0.94      1.00      0.97        15\n",
      "          18       0.97      1.00      0.98        29\n",
      "          19       0.77      0.93      0.84        29\n",
      "          20       0.97      0.97      0.97        30\n",
      "          21       0.87      0.77      0.82        26\n",
      "          22       0.64      0.72      0.68        29\n",
      "          23       0.73      0.86      0.79        22\n",
      "          24       0.70      0.47      0.56        30\n",
      "          25       0.88      0.97      0.92        30\n",
      "          26       0.45      0.43      0.44        30\n",
      "          27       1.00      0.90      0.95        29\n",
      "          28       0.69      0.37      0.48        30\n",
      "          29       0.38      0.47      0.42        30\n",
      "          30       0.93      0.83      0.88        30\n",
      "          31       0.90      0.83      0.86        23\n",
      "          32       0.81      0.86      0.83        29\n",
      "          33       0.93      0.93      0.93        29\n",
      "          34       0.91      1.00      0.95        30\n",
      "          35       0.94      1.00      0.97        30\n",
      "          36       0.69      0.62      0.65        29\n",
      "          37       1.00      0.87      0.93        30\n",
      "          38       0.44      0.59      0.50        29\n",
      "          39       0.70      0.70      0.70        30\n",
      "          40       0.96      0.83      0.89        30\n",
      "          41       0.91      0.97      0.94        30\n",
      "          42       0.63      0.41      0.50        29\n",
      "          43       0.97      0.93      0.95        30\n",
      "          44       0.80      0.80      0.80        30\n",
      "          45       0.90      0.93      0.92        30\n",
      "          46       0.97      0.97      0.97        30\n",
      "          47       1.00      0.97      0.98        30\n",
      "          48       0.59      0.87      0.70        30\n",
      "          49       0.93      0.83      0.88        30\n",
      "          50       0.82      0.93      0.87        30\n",
      "          51       1.00      0.97      0.98        30\n",
      "          52       1.00      1.00      1.00        30\n",
      "          53       0.90      0.87      0.88        30\n",
      "          54       0.94      0.97      0.95        30\n",
      "          55       1.00      0.90      0.95        30\n",
      "          56       1.00      1.00      1.00        30\n",
      "          57       0.93      0.93      0.93        28\n",
      "          58       0.29      0.14      0.19        29\n",
      "          59       0.56      0.62      0.59        29\n",
      "          60       0.91      1.00      0.95        30\n",
      "          61       0.60      0.70      0.65        30\n",
      "          62       0.91      1.00      0.95        30\n",
      "          63       0.75      0.60      0.67        30\n",
      "          64       0.67      0.70      0.68        20\n",
      "          65       0.62      0.83      0.71        30\n",
      "          66       0.84      0.70      0.76        30\n",
      "          67       0.76      0.87      0.81        30\n",
      "          68       0.93      0.87      0.90        30\n",
      "          69       0.94      0.97      0.95        30\n",
      "          70       0.55      0.53      0.54        30\n",
      "          71       0.62      0.50      0.56        30\n",
      "          72       1.00      0.93      0.97        30\n",
      "          73       0.94      1.00      0.97        30\n",
      "          74       1.00      1.00      1.00        27\n",
      "          75       0.91      0.97      0.94        30\n",
      "          76       0.88      0.97      0.92        30\n",
      "          77       0.78      0.97      0.86        29\n",
      "          78       0.84      0.87      0.85        30\n",
      "          79       0.96      0.87      0.91        30\n",
      "          80       0.96      0.90      0.93        30\n",
      "          81       0.77      0.90      0.83        30\n",
      "          82       1.00      0.97      0.98        30\n",
      "          83       0.84      0.91      0.87        23\n",
      "          84       0.91      1.00      0.95        30\n",
      "          85       0.97      0.97      0.97        30\n",
      "          86       0.96      0.87      0.91        30\n",
      "          87       1.00      0.97      0.98        30\n",
      "          88       1.00      0.80      0.89        30\n",
      "          89       0.81      1.00      0.90        30\n",
      "          90       0.81      0.83      0.82        30\n",
      "          91       0.82      0.90      0.86        30\n",
      "          92       1.00      1.00      1.00        30\n",
      "          93       0.88      1.00      0.94        30\n",
      "          94       0.82      0.90      0.86        30\n",
      "          95       0.95      0.67      0.78        30\n",
      "          96       0.88      0.72      0.79        29\n",
      "          97       0.90      0.87      0.88        30\n",
      "          98       0.87      0.90      0.89        30\n",
      "          99       0.97      1.00      0.98        30\n",
      "         100       0.95      1.00      0.98        20\n",
      "         101       0.93      0.47      0.62        30\n",
      "         102       0.76      0.83      0.79        30\n",
      "         103       0.85      0.77      0.81        30\n",
      "         104       0.68      0.89      0.77        19\n",
      "         105       0.97      1.00      0.98        30\n",
      "         106       0.68      0.70      0.69        30\n",
      "         107       0.96      0.83      0.89        30\n",
      "         108       0.88      0.93      0.90        30\n",
      "         109       0.94      1.00      0.97        30\n",
      "         110       0.67      0.67      0.67        30\n",
      "         111       0.73      0.73      0.73        30\n",
      "         112       0.74      0.85      0.79        20\n",
      "         113       0.97      0.93      0.95        30\n",
      "         114       0.76      0.76      0.76        29\n",
      "         115       0.86      0.80      0.83        30\n",
      "         116       0.72      0.72      0.72        29\n",
      "         117       0.62      0.67      0.65        30\n",
      "         118       0.83      0.66      0.73        29\n",
      "         119       0.81      0.83      0.82        30\n",
      "         120       0.75      0.90      0.82        30\n",
      "         121       1.00      1.00      1.00        30\n",
      "         122       0.92      0.77      0.84        30\n",
      "         123       0.70      0.97      0.81        29\n",
      "         124       0.89      0.83      0.86        29\n",
      "         125       0.83      0.63      0.72        30\n",
      "         126       0.81      0.87      0.84        30\n",
      "         127       0.81      0.83      0.82        30\n",
      "         128       0.71      0.83      0.77        30\n",
      "         129       0.68      0.50      0.58        30\n",
      "         130       0.79      0.90      0.84        30\n",
      "         131       0.88      0.97      0.92        30\n",
      "         132       0.96      0.90      0.93        30\n",
      "         133       0.90      0.93      0.92        30\n",
      "         134       0.82      0.60      0.69        30\n",
      "         135       0.77      0.57      0.65        30\n",
      "         136       0.66      0.83      0.74        30\n",
      "         137       0.90      0.90      0.90        30\n",
      "         138       0.96      0.80      0.87        30\n",
      "         139       0.94      0.97      0.95        30\n",
      "         140       0.65      0.59      0.62        29\n",
      "         141       0.83      0.83      0.83        30\n",
      "         142       0.69      0.67      0.68        30\n",
      "         143       0.33      0.33      0.33        30\n",
      "         144       0.68      0.70      0.69        30\n",
      "         145       0.55      0.60      0.57        30\n",
      "         146       0.84      0.90      0.87        30\n",
      "         147       0.97      0.97      0.97        30\n",
      "         148       0.96      0.86      0.91        29\n",
      "         149       0.83      0.83      0.83        30\n",
      "         150       0.91      1.00      0.95        21\n",
      "         151       0.89      0.83      0.86        30\n",
      "         152       0.88      0.76      0.81        29\n",
      "         153       0.96      0.83      0.89        30\n",
      "         154       0.85      0.93      0.89        30\n",
      "         155       0.88      0.93      0.90        30\n",
      "         156       0.68      0.79      0.73        29\n",
      "         157       0.97      1.00      0.98        30\n",
      "         158       1.00      1.00      1.00        30\n",
      "         159       0.90      0.97      0.93        29\n",
      "         160       0.87      0.90      0.89        30\n",
      "         161       1.00      0.87      0.93        30\n",
      "         162       0.93      0.93      0.93        30\n",
      "         163       0.94      0.97      0.95        30\n",
      "         164       0.97      0.97      0.97        30\n",
      "         165       0.90      0.90      0.90        29\n",
      "         166       0.85      0.77      0.81        30\n",
      "         167       0.82      0.97      0.89        29\n",
      "         168       0.85      1.00      0.92        29\n",
      "         169       0.96      0.90      0.93        30\n",
      "         170       0.97      0.97      0.97        30\n",
      "         171       0.71      0.90      0.79        30\n",
      "         172       0.74      0.83      0.78        30\n",
      "         173       0.96      0.83      0.89        30\n",
      "         174       0.70      0.77      0.73        30\n",
      "         175       0.94      1.00      0.97        30\n",
      "         176       0.94      0.97      0.95        30\n",
      "         177       1.00      0.88      0.94        26\n",
      "         178       0.76      0.66      0.70        29\n",
      "         179       0.84      0.90      0.87        30\n",
      "         180       0.97      0.97      0.97        29\n",
      "         181       1.00      0.93      0.97        30\n",
      "         182       0.96      0.87      0.91        30\n",
      "         183       0.80      0.93      0.86        30\n",
      "         184       1.00      0.97      0.98        30\n",
      "         185       0.97      1.00      0.98        30\n",
      "         186       0.95      1.00      0.98        20\n",
      "         187       1.00      1.00      1.00        30\n",
      "         188       0.91      1.00      0.95        30\n",
      "         189       1.00      0.93      0.96        29\n",
      "         190       1.00      0.90      0.95        30\n",
      "         191       0.94      1.00      0.97        30\n",
      "         192       0.92      0.80      0.86        30\n",
      "         193       1.00      0.90      0.95        30\n",
      "         194       0.81      0.97      0.88        30\n",
      "         195       1.00      0.73      0.85        30\n",
      "         196       0.86      0.80      0.83        30\n",
      "         197       0.79      0.90      0.84        30\n",
      "         198       0.85      0.93      0.89        30\n",
      "         199       1.00      0.93      0.97        30\n",
      "\n",
      "    accuracy                           0.85      5792\n",
      "   macro avg       0.85      0.85      0.85      5792\n",
      "weighted avg       0.85      0.85      0.85      5792\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_pred=yhat, y_true=ytrue.ravel()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, classes = next(iter(dataloaders['train']))\n",
    "\n",
    "net = {}\n",
    "output = {}\n",
    "with torch.no_grad():\n",
    "    for model_name in model_names:\n",
    "\n",
    "        net[model_name] = NeuralNetClassifier(model[model_name])\n",
    "        net[model_name].initialize()\n",
    "        \n",
    "        output[model_name] = net[model_name].predict(inputs)\n",
    "\n",
    "for i_res, truth in enumerate(classes.numpy()):\n",
    "    res_str = 'Truth: {:5}  Pred:'.format(truth)\n",
    "    for model_name in model_names:\n",
    "        res_str = res_str + ' {:5}'.format(output[model_name][i_res])\n",
    "    print(res_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from combo.models.classifier_comb import SimpleClassifierAggregator\n",
    "#from sklearn.ensemble import VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combo_model = SimpleClassifierAggregator(base_estimators=list(net.values()),\n",
    "                                         method='majority_vote',\n",
    "                                         pre_fitted=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combo_model.predict(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "py37_pytorch_003",
   "language": "python",
   "name": "conda-env-py37_pytorch_003-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
